{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2bb3fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-optimize\n",
      "  Obtaining dependency information for scikit-optimize from https://files.pythonhosted.org/packages/90/0e/15deb91b3db0003843e34e72fa865e1d92013781d986fdc65483c99a9f69/scikit_optimize-0.10.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading scikit_optimize-0.10.1-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.1.1)\n",
      "Collecting pyaml>=16.9 (from scikit-optimize)\n",
      "  Obtaining dependency information for pyaml>=16.9 from https://files.pythonhosted.org/packages/25/ec/8aaf0d127751f84d8b10395fe47def9ac3552990b70abffdd92714836d39/pyaml-23.12.0-py3-none-any.whl.metadata\n",
      "  Downloading pyaml-23.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.3.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from scikit-optimize) (23.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\casti\\anaconda3\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikit-optimize) (2.2.0)\n",
      "Downloading scikit_optimize-0.10.1-py2.py3-none-any.whl (107 kB)\n",
      "   ---------------------------------------- 0.0/107.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 107.7/107.7 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading pyaml-23.12.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: pyaml, scikit-optimize\n",
      "Successfully installed pyaml-23.12.0 scikit-optimize-0.10.1\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c666cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0133814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_vec = sp.sparse.load_npz(r'C:\\Users\\casti\\OneDrive\\Documents\\A Springboard\\Capstone 2 - Global Terrorism\\data\\interim\\X_tr_vec.npz')\n",
    "X_te_vec = sp.sparse.load_npz(r'C:\\Users\\casti\\OneDrive\\Documents\\A Springboard\\Capstone 2 - Global Terrorism\\data\\interim\\X_te_vec.npz')\n",
    "y_tr = pd.read_csv(r'C:\\Users\\casti\\OneDrive\\Documents\\A Springboard\\Capstone 2 - Global Terrorism\\data\\interim\\y_tr.csv')\n",
    "y_te = pd.read_csv(r'C:\\Users\\casti\\OneDrive\\Documents\\A Springboard\\Capstone 2 - Global Terrorism\\data\\interim\\y_te.csv')\n",
    "y_tr = y_tr.drop(columns = ['Unnamed: 0']).values.ravel()\n",
    "y_te = y_te.drop(columns = ['Unnamed: 0']).values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2614eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=15, class_weight={0:10.0, 1:1.0})\n",
    "rf.fit(X_tr_vec, y_tr)\n",
    "y_pr = rf.predict(X_te_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3e6a6751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.42      0.39       515\n",
      "           1       0.94      0.93      0.94      5426\n",
      "\n",
      "    accuracy                           0.89      5941\n",
      "   macro avg       0.66      0.68      0.67      5941\n",
      "weighted avg       0.89      0.89      0.89      5941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_te, y_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b060b923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_te, y_pr)\n",
    "\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt = 'd', cbar=True, xticklabels=True, yticklabels=True)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c5a070e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV 1/3] END criterion=entropy, max_depth=7, max_features=log2, n_estimators=85; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   4.9s\n",
      "[CV 2/3] END criterion=entropy, max_depth=7, max_features=log2, n_estimators=85; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   4.9s\n",
      "[CV 3/3] END criterion=entropy, max_depth=7, max_features=log2, n_estimators=85; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   4.8s\n",
      "[CV 1/3] END criterion=entropy, max_depth=7, max_features=sqrt, n_estimators=18; f1: (test=0.954) precision: (test=0.916) recall: (test=0.996) total time=   2.9s\n",
      "[CV 2/3] END criterion=entropy, max_depth=7, max_features=sqrt, n_estimators=18; f1: (test=0.955) precision: (test=0.916) recall: (test=0.999) total time=   2.9s\n",
      "[CV 3/3] END criterion=entropy, max_depth=7, max_features=sqrt, n_estimators=18; f1: (test=0.955) precision: (test=0.914) recall: (test=0.999) total time=   2.3s\n",
      "[CV 1/3] END criterion=entropy, max_depth=12, max_features=sqrt, n_estimators=79; f1: (test=0.956) precision: (test=0.916) recall: (test=0.999) total time=  27.2s\n",
      "[CV 2/3] END criterion=entropy, max_depth=12, max_features=sqrt, n_estimators=79; f1: (test=0.956) precision: (test=0.917) recall: (test=0.999) total time=  26.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=12, max_features=sqrt, n_estimators=79; f1: (test=0.956) precision: (test=0.915) recall: (test=0.999) total time=  22.7s\n",
      "[CV 1/3] END criterion=gini, max_depth=11, max_features=sqrt, n_estimators=97; f1: (test=0.955) precision: (test=0.915) recall: (test=0.999) total time=  31.6s\n",
      "[CV 2/3] END criterion=gini, max_depth=11, max_features=sqrt, n_estimators=97; f1: (test=0.956) precision: (test=0.916) recall: (test=1.000) total time=  34.6s\n",
      "[CV 3/3] END criterion=gini, max_depth=11, max_features=sqrt, n_estimators=97; f1: (test=0.956) precision: (test=0.916) recall: (test=0.999) total time=  35.2s\n",
      "[CV 1/3] END criterion=entropy, max_depth=11, max_features=log2, n_estimators=77; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   6.8s\n",
      "[CV 2/3] END criterion=entropy, max_depth=11, max_features=log2, n_estimators=77; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   7.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=11, max_features=log2, n_estimators=77; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   5.4s\n",
      "[CV 1/3] END criterion=entropy, max_depth=9, max_features=sqrt, n_estimators=97; f1: (test=0.955) precision: (test=0.915) recall: (test=0.999) total time=  22.7s\n",
      "[CV 2/3] END criterion=entropy, max_depth=9, max_features=sqrt, n_estimators=97; f1: (test=0.956) precision: (test=0.916) recall: (test=1.000) total time=  23.6s\n",
      "[CV 3/3] END criterion=entropy, max_depth=9, max_features=sqrt, n_estimators=97; f1: (test=0.955) precision: (test=0.915) recall: (test=0.999) total time=  19.9s\n",
      "[CV 1/3] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=17; f1: (test=0.956) precision: (test=0.916) recall: (test=0.999) total time=   3.8s\n",
      "[CV 2/3] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=17; f1: (test=0.956) precision: (test=0.917) recall: (test=0.999) total time=   2.7s\n",
      "[CV 3/3] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=17; f1: (test=0.955) precision: (test=0.916) recall: (test=0.997) total time=   3.8s\n",
      "[CV 1/3] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=42; f1: (test=0.955) precision: (test=0.916) recall: (test=0.999) total time=  16.7s\n",
      "[CV 2/3] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=42; f1: (test=0.956) precision: (test=0.916) recall: (test=1.000) total time=   9.2s\n",
      "[CV 3/3] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=42; f1: (test=0.955) precision: (test=0.915) recall: (test=0.999) total time=  12.2s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=19; f1: (test=0.955) precision: (test=0.915) recall: (test=0.999) total time=   2.2s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=19; f1: (test=0.955) precision: (test=0.916) recall: (test=0.999) total time=   1.7s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=19; f1: (test=0.955) precision: (test=0.915) recall: (test=0.999) total time=   2.4s\n",
      "[CV 1/3] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=62; f1: (test=0.956) precision: (test=0.916) recall: (test=0.999) total time=  16.8s\n",
      "[CV 2/3] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=62; f1: (test=0.956) precision: (test=0.916) recall: (test=1.000) total time=  15.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=62; f1: (test=0.955) precision: (test=0.914) recall: (test=0.999) total time=  15.8s\n",
      "[CV 1/3] END criterion=entropy, max_depth=14, max_features=log2, n_estimators=74; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   9.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=14, max_features=log2, n_estimators=74; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   7.6s\n",
      "[CV 3/3] END criterion=entropy, max_depth=14, max_features=log2, n_estimators=74; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   7.2s\n",
      "[CV 1/3] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=76; f1: (test=0.956) precision: (test=0.916) recall: (test=0.999) total time=  28.2s\n",
      "[CV 2/3] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=76; f1: (test=0.957) precision: (test=0.917) recall: (test=0.999) total time=  21.8s\n",
      "[CV 3/3] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=76; f1: (test=0.955) precision: (test=0.915) recall: (test=0.999) total time=  21.8s\n",
      "[CV 1/3] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=57; f1: (test=0.955) precision: (test=0.915) recall: (test=0.999) total time=   8.4s\n",
      "[CV 2/3] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=57; f1: (test=0.956) precision: (test=0.915) recall: (test=1.000) total time=   8.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=57; f1: (test=0.955) precision: (test=0.914) recall: (test=1.000) total time=  12.9s\n",
      "[CV 1/3] END criterion=entropy, max_depth=9, max_features=sqrt, n_estimators=49; f1: (test=0.955) precision: (test=0.915) recall: (test=1.000) total time=  12.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=9, max_features=sqrt, n_estimators=49; f1: (test=0.956) precision: (test=0.916) recall: (test=0.999) total time=   9.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=9, max_features=sqrt, n_estimators=49; f1: (test=0.955) precision: (test=0.914) recall: (test=0.999) total time=  11.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=11, max_features=sqrt, n_estimators=18; f1: (test=0.955) precision: (test=0.916) recall: (test=0.997) total time=   4.7s\n",
      "[CV 2/3] END criterion=gini, max_depth=11, max_features=sqrt, n_estimators=18; f1: (test=0.956) precision: (test=0.917) recall: (test=0.998) total time=   5.8s\n",
      "[CV 3/3] END criterion=gini, max_depth=11, max_features=sqrt, n_estimators=18; f1: (test=0.956) precision: (test=0.917) recall: (test=0.999) total time=   5.5s\n",
      "[CV 1/3] END criterion=gini, max_depth=7, max_features=sqrt, n_estimators=74; f1: (test=0.956) precision: (test=0.915) recall: (test=1.000) total time=  13.9s\n",
      "[CV 2/3] END criterion=gini, max_depth=7, max_features=sqrt, n_estimators=74; f1: (test=0.956) precision: (test=0.915) recall: (test=1.000) total time=   9.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=7, max_features=sqrt, n_estimators=74; f1: (test=0.955) precision: (test=0.914) recall: (test=0.999) total time=  11.6s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=17; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   0.6s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=17; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   0.8s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=17; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   1.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END criterion=gini, max_depth=12, max_features=log2, n_estimators=14; f1: (test=0.955) precision: (test=0.915) recall: (test=1.000) total time=   2.9s\n",
      "[CV 2/3] END criterion=gini, max_depth=12, max_features=log2, n_estimators=14; f1: (test=0.955) precision: (test=0.915) recall: (test=0.999) total time=   0.7s\n",
      "[CV 3/3] END criterion=gini, max_depth=12, max_features=log2, n_estimators=14; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   1.3s\n",
      "[CV 1/3] END criterion=entropy, max_depth=13, max_features=log2, n_estimators=70; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   6.8s\n",
      "[CV 2/3] END criterion=entropy, max_depth=13, max_features=log2, n_estimators=70; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   5.9s\n",
      "[CV 3/3] END criterion=entropy, max_depth=13, max_features=log2, n_estimators=70; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   6.4s\n",
      "[CV 1/3] END criterion=gini, max_depth=12, max_features=log2, n_estimators=49; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   7.6s\n",
      "[CV 2/3] END criterion=gini, max_depth=12, max_features=log2, n_estimators=49; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   6.3s\n",
      "[CV 3/3] END criterion=gini, max_depth=12, max_features=log2, n_estimators=49; f1: (test=0.955) precision: (test=0.913) recall: (test=1.000) total time=   5.9s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=RandomForestClassifier(class_weight={0: 8.0,\n",
       "                                                                  1: 1.0}),\n",
       "                   n_iter=20,\n",
       "                   param_distributions={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;],\n",
       "                                        &#x27;max_depth&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001D2E92CB3D0&gt;,\n",
       "                                        &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                                        &#x27;n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001D2EA12E8D0&gt;},\n",
       "                   refit=False, scoring=[&#x27;precision&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;],\n",
       "                   verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=RandomForestClassifier(class_weight={0: 8.0,\n",
       "                                                                  1: 1.0}),\n",
       "                   n_iter=20,\n",
       "                   param_distributions={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;],\n",
       "                                        &#x27;max_depth&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001D2E92CB3D0&gt;,\n",
       "                                        &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                                        &#x27;n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001D2EA12E8D0&gt;},\n",
       "                   refit=False, scoring=[&#x27;precision&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;],\n",
       "                   verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight={0: 8.0, 1: 1.0})</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight={0: 8.0, 1: 1.0})</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=RandomForestClassifier(class_weight={0: 8.0,\n",
       "                                                                  1: 1.0}),\n",
       "                   n_iter=20,\n",
       "                   param_distributions={'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001D2E92CB3D0>,\n",
       "                                        'max_features': ['sqrt', 'log2'],\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001D2EA12E8D0>},\n",
       "                   refit=False, scoring=['precision', 'recall', 'f1'],\n",
       "                   verbose=3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(class_weight={0:8.0, 1:1.0})\n",
    "\n",
    "params = {'n_estimators': randint(10,100),\n",
    "          'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': randint(5,15),\n",
    "         'max_features': ['sqrt','log2'],\n",
    "         }\n",
    "\n",
    "\n",
    "cv = RandomizedSearchCV(rf, param_distributions=params, n_iter=20, \\\n",
    "                       cv = 3, verbose=3, scoring = ['precision','recall','f1'], refit=False)\n",
    "\n",
    "cv.fit(X_tr_vec, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b802a5c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_precision</th>\n",
       "      <th>split1_test_precision</th>\n",
       "      <th>split2_test_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>rank_test_precision</th>\n",
       "      <th>split0_test_recall</th>\n",
       "      <th>split1_test_recall</th>\n",
       "      <th>split2_test_recall</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>rank_test_recall</th>\n",
       "      <th>split0_test_f1</th>\n",
       "      <th>split1_test_f1</th>\n",
       "      <th>split2_test_f1</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.470316</td>\n",
       "      <td>0.487325</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.012953</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>17</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 17}</td>\n",
       "      <td>0.916216</td>\n",
       "      <td>0.916822</td>\n",
       "      <td>0.916032</td>\n",
       "      <td>0.916357</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999447</td>\n",
       "      <td>0.999447</td>\n",
       "      <td>0.997236</td>\n",
       "      <td>0.998710</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>18</td>\n",
       "      <td>0.956024</td>\n",
       "      <td>0.956353</td>\n",
       "      <td>0.954910</td>\n",
       "      <td>0.955762</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.116938</td>\n",
       "      <td>1.916908</td>\n",
       "      <td>0.315468</td>\n",
       "      <td>0.038292</td>\n",
       "      <td>entropy</td>\n",
       "      <td>12</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>79</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 12, 'max_features': 'sqrt', 'n_estimators': 79}</td>\n",
       "      <td>0.915724</td>\n",
       "      <td>0.916935</td>\n",
       "      <td>0.915414</td>\n",
       "      <td>0.916024</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.998894</td>\n",
       "      <td>0.999263</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>17</td>\n",
       "      <td>0.955587</td>\n",
       "      <td>0.956161</td>\n",
       "      <td>0.955503</td>\n",
       "      <td>0.955750</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.819814</td>\n",
       "      <td>3.004425</td>\n",
       "      <td>0.218876</td>\n",
       "      <td>0.027535</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>76</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 76}</td>\n",
       "      <td>0.915724</td>\n",
       "      <td>0.917132</td>\n",
       "      <td>0.914782</td>\n",
       "      <td>0.915879</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.999447</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.999201</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>13</td>\n",
       "      <td>0.955587</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.955074</td>\n",
       "      <td>0.955728</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.520652</td>\n",
       "      <td>1.604319</td>\n",
       "      <td>0.363624</td>\n",
       "      <td>0.012792</td>\n",
       "      <td>gini</td>\n",
       "      <td>11</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>97</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 11, 'max_features': 'sqrt', 'n_estimators': 97}</td>\n",
       "      <td>0.915400</td>\n",
       "      <td>0.916216</td>\n",
       "      <td>0.915850</td>\n",
       "      <td>0.915822</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>5</td>\n",
       "      <td>0.998894</td>\n",
       "      <td>0.999631</td>\n",
       "      <td>0.998894</td>\n",
       "      <td>0.999140</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>14</td>\n",
       "      <td>0.955326</td>\n",
       "      <td>0.956108</td>\n",
       "      <td>0.955571</td>\n",
       "      <td>0.955669</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.619896</td>\n",
       "      <td>3.096740</td>\n",
       "      <td>0.180266</td>\n",
       "      <td>0.033734</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>42</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 42}</td>\n",
       "      <td>0.915695</td>\n",
       "      <td>0.915907</td>\n",
       "      <td>0.915386</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>6</td>\n",
       "      <td>0.998710</td>\n",
       "      <td>0.999631</td>\n",
       "      <td>0.998894</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>16</td>\n",
       "      <td>0.955403</td>\n",
       "      <td>0.955939</td>\n",
       "      <td>0.955319</td>\n",
       "      <td>0.955554</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.311588</td>\n",
       "      <td>0.464092</td>\n",
       "      <td>0.121826</td>\n",
       "      <td>0.016436</td>\n",
       "      <td>gini</td>\n",
       "      <td>11</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>18</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 11, 'max_features': 'sqrt', 'n_estimators': 18}</td>\n",
       "      <td>0.915891</td>\n",
       "      <td>0.917005</td>\n",
       "      <td>0.916751</td>\n",
       "      <td>0.916549</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997236</td>\n",
       "      <td>0.997788</td>\n",
       "      <td>0.998526</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>20</td>\n",
       "      <td>0.954834</td>\n",
       "      <td>0.955693</td>\n",
       "      <td>0.955893</td>\n",
       "      <td>0.955473</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15.747818</td>\n",
       "      <td>0.698145</td>\n",
       "      <td>0.236698</td>\n",
       "      <td>0.043496</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>62</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 62}</td>\n",
       "      <td>0.915569</td>\n",
       "      <td>0.915766</td>\n",
       "      <td>0.914334</td>\n",
       "      <td>0.915223</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.999816</td>\n",
       "      <td>0.999263</td>\n",
       "      <td>0.999386</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>11</td>\n",
       "      <td>0.955503</td>\n",
       "      <td>0.955947</td>\n",
       "      <td>0.954914</td>\n",
       "      <td>0.955455</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.610876</td>\n",
       "      <td>1.245431</td>\n",
       "      <td>0.192030</td>\n",
       "      <td>0.034696</td>\n",
       "      <td>entropy</td>\n",
       "      <td>9</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>49</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 9, 'max_features': 'sqrt', 'n_estimators': 49}</td>\n",
       "      <td>0.914840</td>\n",
       "      <td>0.916188</td>\n",
       "      <td>0.914194</td>\n",
       "      <td>0.915074</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>10</td>\n",
       "      <td>0.999631</td>\n",
       "      <td>0.999263</td>\n",
       "      <td>0.999447</td>\n",
       "      <td>0.999447</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>10</td>\n",
       "      <td>0.955358</td>\n",
       "      <td>0.955924</td>\n",
       "      <td>0.954922</td>\n",
       "      <td>0.955401</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21.843218</td>\n",
       "      <td>1.562404</td>\n",
       "      <td>0.308587</td>\n",
       "      <td>0.055773</td>\n",
       "      <td>entropy</td>\n",
       "      <td>9</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>97</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 9, 'max_features': 'sqrt', 'n_estimators': 97}</td>\n",
       "      <td>0.914951</td>\n",
       "      <td>0.916061</td>\n",
       "      <td>0.914628</td>\n",
       "      <td>0.915214</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.999631</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.999263</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>12</td>\n",
       "      <td>0.955166</td>\n",
       "      <td>0.956024</td>\n",
       "      <td>0.954990</td>\n",
       "      <td>0.955393</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.673647</td>\n",
       "      <td>2.215738</td>\n",
       "      <td>0.205325</td>\n",
       "      <td>0.035991</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>57</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 8, 'max_features': 'sqrt', 'n_estimators': 57}</td>\n",
       "      <td>0.914811</td>\n",
       "      <td>0.915148</td>\n",
       "      <td>0.914238</td>\n",
       "      <td>0.914732</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>12</td>\n",
       "      <td>0.999263</td>\n",
       "      <td>0.999816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999693</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>8</td>\n",
       "      <td>0.955174</td>\n",
       "      <td>0.955610</td>\n",
       "      <td>0.955198</td>\n",
       "      <td>0.955327</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.327011</td>\n",
       "      <td>1.907846</td>\n",
       "      <td>0.316882</td>\n",
       "      <td>0.057126</td>\n",
       "      <td>gini</td>\n",
       "      <td>7</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>74</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 74}</td>\n",
       "      <td>0.915008</td>\n",
       "      <td>0.915008</td>\n",
       "      <td>0.914180</td>\n",
       "      <td>0.914732</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>13</td>\n",
       "      <td>0.999816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999263</td>\n",
       "      <td>0.999693</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>9</td>\n",
       "      <td>0.955534</td>\n",
       "      <td>0.955618</td>\n",
       "      <td>0.954830</td>\n",
       "      <td>0.955327</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.091076</td>\n",
       "      <td>0.302740</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>19</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 19}</td>\n",
       "      <td>0.914811</td>\n",
       "      <td>0.915681</td>\n",
       "      <td>0.914657</td>\n",
       "      <td>0.915050</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>11</td>\n",
       "      <td>0.999263</td>\n",
       "      <td>0.998710</td>\n",
       "      <td>0.999447</td>\n",
       "      <td>0.999140</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>15</td>\n",
       "      <td>0.955174</td>\n",
       "      <td>0.955395</td>\n",
       "      <td>0.955174</td>\n",
       "      <td>0.955248</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.686788</td>\n",
       "      <td>0.907770</td>\n",
       "      <td>0.066641</td>\n",
       "      <td>0.007684</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>log2</td>\n",
       "      <td>14</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'max_features': 'log2', 'n_estimators': 14}</td>\n",
       "      <td>0.914546</td>\n",
       "      <td>0.914503</td>\n",
       "      <td>0.913314</td>\n",
       "      <td>0.914121</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>14</td>\n",
       "      <td>0.999816</td>\n",
       "      <td>0.999447</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999754</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>7</td>\n",
       "      <td>0.955282</td>\n",
       "      <td>0.955090</td>\n",
       "      <td>0.954693</td>\n",
       "      <td>0.955022</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.709195</td>\n",
       "      <td>0.271521</td>\n",
       "      <td>0.099551</td>\n",
       "      <td>0.022557</td>\n",
       "      <td>entropy</td>\n",
       "      <td>7</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>18</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 18}</td>\n",
       "      <td>0.916087</td>\n",
       "      <td>0.915555</td>\n",
       "      <td>0.914180</td>\n",
       "      <td>0.915274</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>7</td>\n",
       "      <td>0.995762</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.999263</td>\n",
       "      <td>0.998034</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>19</td>\n",
       "      <td>0.954265</td>\n",
       "      <td>0.955495</td>\n",
       "      <td>0.954830</td>\n",
       "      <td>0.954863</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.815564</td>\n",
       "      <td>0.219243</td>\n",
       "      <td>0.117781</td>\n",
       "      <td>0.018763</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>17</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'max_features': 'log2', 'n_estimators': 17}</td>\n",
       "      <td>0.913483</td>\n",
       "      <td>0.913314</td>\n",
       "      <td>0.913314</td>\n",
       "      <td>0.913370</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.954785</td>\n",
       "      <td>0.954693</td>\n",
       "      <td>0.954693</td>\n",
       "      <td>0.954724</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.639109</td>\n",
       "      <td>0.065299</td>\n",
       "      <td>0.324019</td>\n",
       "      <td>0.036567</td>\n",
       "      <td>entropy</td>\n",
       "      <td>7</td>\n",
       "      <td>log2</td>\n",
       "      <td>85</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7, 'max_features': 'log2', 'n_estimators': 85}</td>\n",
       "      <td>0.913483</td>\n",
       "      <td>0.913314</td>\n",
       "      <td>0.913314</td>\n",
       "      <td>0.913370</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.954785</td>\n",
       "      <td>0.954693</td>\n",
       "      <td>0.954693</td>\n",
       "      <td>0.954724</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.200885</td>\n",
       "      <td>0.407344</td>\n",
       "      <td>0.264236</td>\n",
       "      <td>0.050263</td>\n",
       "      <td>entropy</td>\n",
       "      <td>13</td>\n",
       "      <td>log2</td>\n",
       "      <td>70</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 13, 'max_features': 'log2', 'n_estimators': 70}</td>\n",
       "      <td>0.913483</td>\n",
       "      <td>0.913314</td>\n",
       "      <td>0.913314</td>\n",
       "      <td>0.913370</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.954785</td>\n",
       "      <td>0.954693</td>\n",
       "      <td>0.954693</td>\n",
       "      <td>0.954724</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.176864</td>\n",
       "      <td>0.741599</td>\n",
       "      <td>0.351642</td>\n",
       "      <td>0.035730</td>\n",
       "      <td>entropy</td>\n",
       "      <td>11</td>\n",
       "      <td>log2</td>\n",
       "      <td>77</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 11, 'max_features': 'log2', 'n_estimators': 77}</td>\n",
       "      <td>0.913483</td>\n",
       "      <td>0.913314</td>\n",
       "      <td>0.913314</td>\n",
       "      <td>0.913370</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.954785</td>\n",
       "      <td>0.954693</td>\n",
       "      <td>0.954693</td>\n",
       "      <td>0.954724</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.787173</td>\n",
       "      <td>0.743222</td>\n",
       "      <td>0.274252</td>\n",
       "      <td>0.046869</td>\n",
       "      <td>entropy</td>\n",
       "      <td>14</td>\n",
       "      <td>log2</td>\n",
       "      <td>74</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 14, 'max_features': 'log2', 'n_estimators': 74}</td>\n",
       "      <td>0.913483</td>\n",
       "      <td>0.913314</td>\n",
       "      <td>0.913314</td>\n",
       "      <td>0.913370</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.954785</td>\n",
       "      <td>0.954693</td>\n",
       "      <td>0.954693</td>\n",
       "      <td>0.954724</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.519457</td>\n",
       "      <td>0.682929</td>\n",
       "      <td>0.178606</td>\n",
       "      <td>0.021597</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>log2</td>\n",
       "      <td>49</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'max_features': 'log2', 'n_estimators': 49}</td>\n",
       "      <td>0.913483</td>\n",
       "      <td>0.913314</td>\n",
       "      <td>0.913314</td>\n",
       "      <td>0.913370</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.954785</td>\n",
       "      <td>0.954693</td>\n",
       "      <td>0.954693</td>\n",
       "      <td>0.954724</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        3.470316      0.487325         0.072464        0.012953   \n",
       "1       25.116938      1.916908         0.315468        0.038292   \n",
       "2       23.819814      3.004425         0.218876        0.027535   \n",
       "3       33.520652      1.604319         0.363624        0.012792   \n",
       "4       12.619896      3.096740         0.180266        0.033734   \n",
       "5        5.311588      0.464092         0.121826        0.016436   \n",
       "6       15.747818      0.698145         0.236698        0.043496   \n",
       "7       10.610876      1.245431         0.192030        0.034696   \n",
       "8       21.843218      1.562404         0.308587        0.055773   \n",
       "9        9.673647      2.215738         0.205325        0.035991   \n",
       "10      11.327011      1.907846         0.316882        0.057126   \n",
       "11       2.091076      0.302740         0.079208        0.010153   \n",
       "12       1.686788      0.907770         0.066641        0.007684   \n",
       "13       2.709195      0.271521         0.099551        0.022557   \n",
       "14       0.815564      0.219243         0.117781        0.018763   \n",
       "15       4.639109      0.065299         0.324019        0.036567   \n",
       "16       6.200885      0.407344         0.264236        0.050263   \n",
       "17       6.176864      0.741599         0.351642        0.035730   \n",
       "18       7.787173      0.743222         0.274252        0.046869   \n",
       "19       6.519457      0.682929         0.178606        0.021597   \n",
       "\n",
       "   param_criterion param_max_depth param_max_features param_n_estimators  \\\n",
       "0             gini              10               sqrt                 17   \n",
       "1          entropy              12               sqrt                 79   \n",
       "2             gini              10               sqrt                 76   \n",
       "3             gini              11               sqrt                 97   \n",
       "4             gini              10               sqrt                 42   \n",
       "5             gini              11               sqrt                 18   \n",
       "6             gini              10               sqrt                 62   \n",
       "7          entropy               9               sqrt                 49   \n",
       "8          entropy               9               sqrt                 97   \n",
       "9          entropy               8               sqrt                 57   \n",
       "10            gini               7               sqrt                 74   \n",
       "11         entropy               5               sqrt                 19   \n",
       "12            gini              12               log2                 14   \n",
       "13         entropy               7               sqrt                 18   \n",
       "14            gini               5               log2                 17   \n",
       "15         entropy               7               log2                 85   \n",
       "16         entropy              13               log2                 70   \n",
       "17         entropy              11               log2                 77   \n",
       "18         entropy              14               log2                 74   \n",
       "19            gini              12               log2                 49   \n",
       "\n",
       "                                                                                   params  \\\n",
       "0      {'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 17}   \n",
       "1   {'criterion': 'entropy', 'max_depth': 12, 'max_features': 'sqrt', 'n_estimators': 79}   \n",
       "2      {'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 76}   \n",
       "3      {'criterion': 'gini', 'max_depth': 11, 'max_features': 'sqrt', 'n_estimators': 97}   \n",
       "4      {'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 42}   \n",
       "5      {'criterion': 'gini', 'max_depth': 11, 'max_features': 'sqrt', 'n_estimators': 18}   \n",
       "6      {'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 62}   \n",
       "7    {'criterion': 'entropy', 'max_depth': 9, 'max_features': 'sqrt', 'n_estimators': 49}   \n",
       "8    {'criterion': 'entropy', 'max_depth': 9, 'max_features': 'sqrt', 'n_estimators': 97}   \n",
       "9    {'criterion': 'entropy', 'max_depth': 8, 'max_features': 'sqrt', 'n_estimators': 57}   \n",
       "10      {'criterion': 'gini', 'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 74}   \n",
       "11   {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 19}   \n",
       "12     {'criterion': 'gini', 'max_depth': 12, 'max_features': 'log2', 'n_estimators': 14}   \n",
       "13   {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 18}   \n",
       "14      {'criterion': 'gini', 'max_depth': 5, 'max_features': 'log2', 'n_estimators': 17}   \n",
       "15   {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'log2', 'n_estimators': 85}   \n",
       "16  {'criterion': 'entropy', 'max_depth': 13, 'max_features': 'log2', 'n_estimators': 70}   \n",
       "17  {'criterion': 'entropy', 'max_depth': 11, 'max_features': 'log2', 'n_estimators': 77}   \n",
       "18  {'criterion': 'entropy', 'max_depth': 14, 'max_features': 'log2', 'n_estimators': 74}   \n",
       "19     {'criterion': 'gini', 'max_depth': 12, 'max_features': 'log2', 'n_estimators': 49}   \n",
       "\n",
       "    split0_test_precision  split1_test_precision  split2_test_precision  \\\n",
       "0                0.916216               0.916822               0.916032   \n",
       "1                0.915724               0.916935               0.915414   \n",
       "2                0.915724               0.917132               0.914782   \n",
       "3                0.915400               0.916216               0.915850   \n",
       "4                0.915695               0.915907               0.915386   \n",
       "5                0.915891               0.917005               0.916751   \n",
       "6                0.915569               0.915766               0.914334   \n",
       "7                0.914840               0.916188               0.914194   \n",
       "8                0.914951               0.916061               0.914628   \n",
       "9                0.914811               0.915148               0.914238   \n",
       "10               0.915008               0.915008               0.914180   \n",
       "11               0.914811               0.915681               0.914657   \n",
       "12               0.914546               0.914503               0.913314   \n",
       "13               0.916087               0.915555               0.914180   \n",
       "14               0.913483               0.913314               0.913314   \n",
       "15               0.913483               0.913314               0.913314   \n",
       "16               0.913483               0.913314               0.913314   \n",
       "17               0.913483               0.913314               0.913314   \n",
       "18               0.913483               0.913314               0.913314   \n",
       "19               0.913483               0.913314               0.913314   \n",
       "\n",
       "    mean_test_precision  std_test_precision  rank_test_precision  \\\n",
       "0              0.916357            0.000337                    2   \n",
       "1              0.916024            0.000656                    3   \n",
       "2              0.915879            0.000965                    4   \n",
       "3              0.915822            0.000334                    5   \n",
       "4              0.915663            0.000214                    6   \n",
       "5              0.916549            0.000477                    1   \n",
       "6              0.915223            0.000634                    8   \n",
       "7              0.915074            0.000831                   10   \n",
       "8              0.915214            0.000614                    9   \n",
       "9              0.914732            0.000376                   12   \n",
       "10             0.914732            0.000391                   13   \n",
       "11             0.915050            0.000451                   11   \n",
       "12             0.914121            0.000571                   14   \n",
       "13             0.915274            0.000804                    7   \n",
       "14             0.913370            0.000079                   15   \n",
       "15             0.913370            0.000079                   15   \n",
       "16             0.913370            0.000079                   15   \n",
       "17             0.913370            0.000079                   15   \n",
       "18             0.913370            0.000079                   15   \n",
       "19             0.913370            0.000079                   15   \n",
       "\n",
       "    split0_test_recall  split1_test_recall  split2_test_recall  \\\n",
       "0             0.999447            0.999447            0.997236   \n",
       "1             0.999079            0.998894            0.999263   \n",
       "2             0.999079            0.999447            0.999079   \n",
       "3             0.998894            0.999631            0.998894   \n",
       "4             0.998710            0.999631            0.998894   \n",
       "5             0.997236            0.997788            0.998526   \n",
       "6             0.999079            0.999816            0.999263   \n",
       "7             0.999631            0.999263            0.999447   \n",
       "8             0.999079            0.999631            0.999079   \n",
       "9             0.999263            0.999816            1.000000   \n",
       "10            0.999816            1.000000            0.999263   \n",
       "11            0.999263            0.998710            0.999447   \n",
       "12            0.999816            0.999447            1.000000   \n",
       "13            0.995762            0.999079            0.999263   \n",
       "14            1.000000            1.000000            1.000000   \n",
       "15            1.000000            1.000000            1.000000   \n",
       "16            1.000000            1.000000            1.000000   \n",
       "17            1.000000            1.000000            1.000000   \n",
       "18            1.000000            1.000000            1.000000   \n",
       "19            1.000000            1.000000            1.000000   \n",
       "\n",
       "    mean_test_recall  std_test_recall  rank_test_recall  split0_test_f1  \\\n",
       "0           0.998710         0.001043                18        0.956024   \n",
       "1           0.999079         0.000150                17        0.955587   \n",
       "2           0.999201         0.000174                13        0.955587   \n",
       "3           0.999140         0.000347                14        0.955326   \n",
       "4           0.999079         0.000398                16        0.955403   \n",
       "5           0.997850         0.000528                20        0.954834   \n",
       "6           0.999386         0.000313                11        0.955503   \n",
       "7           0.999447         0.000151                10        0.955358   \n",
       "8           0.999263         0.000261                12        0.955166   \n",
       "9           0.999693         0.000313                 8        0.955174   \n",
       "10          0.999693         0.000313                 9        0.955534   \n",
       "11          0.999140         0.000313                15        0.955174   \n",
       "12          0.999754         0.000230                 7        0.955282   \n",
       "13          0.998034         0.001609                19        0.954265   \n",
       "14          1.000000         0.000000                 1        0.954785   \n",
       "15          1.000000         0.000000                 1        0.954785   \n",
       "16          1.000000         0.000000                 1        0.954785   \n",
       "17          1.000000         0.000000                 1        0.954785   \n",
       "18          1.000000         0.000000                 1        0.954785   \n",
       "19          1.000000         0.000000                 1        0.954785   \n",
       "\n",
       "    split1_test_f1  split2_test_f1  mean_test_f1  std_test_f1  rank_test_f1  \n",
       "0         0.956353        0.954910      0.955762     0.000617             1  \n",
       "1         0.956161        0.955503      0.955750     0.000293             2  \n",
       "2         0.956522        0.955074      0.955728     0.000599             3  \n",
       "3         0.956108        0.955571      0.955669     0.000326             4  \n",
       "4         0.955939        0.955319      0.955554     0.000275             5  \n",
       "5         0.955693        0.955893      0.955473     0.000459             6  \n",
       "6         0.955947        0.954914      0.955455     0.000423             7  \n",
       "7         0.955924        0.954922      0.955401     0.000410             8  \n",
       "8         0.956024        0.954990      0.955393     0.000452             9  \n",
       "9         0.955610        0.955198      0.955327     0.000200            10  \n",
       "10        0.955618        0.954830      0.955327     0.000354            11  \n",
       "11        0.955395        0.955174      0.955248     0.000104            12  \n",
       "12        0.955090        0.954693      0.955022     0.000245            13  \n",
       "13        0.955495        0.954830      0.954863     0.000503            14  \n",
       "14        0.954693        0.954693      0.954724     0.000043            15  \n",
       "15        0.954693        0.954693      0.954724     0.000043            15  \n",
       "16        0.954693        0.954693      0.954724     0.000043            15  \n",
       "17        0.954693        0.954693      0.954724     0.000043            15  \n",
       "18        0.954693        0.954693      0.954724     0.000043            15  \n",
       "19        0.954693        0.954693      0.954724     0.000043            15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>3.470316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.487325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.072464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.012953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_criterion</th>\n",
       "      <td>gini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_max_depth</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_max_features</th>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_n_estimators</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 17}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.916216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.916822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.916032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.916357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.000337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.999447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.999447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.997236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.99871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.001043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.956024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.956353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.95491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.955762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.000617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        0\n",
       "mean_fit_time                                                                                    3.470316\n",
       "std_fit_time                                                                                     0.487325\n",
       "mean_score_time                                                                                  0.072464\n",
       "std_score_time                                                                                   0.012953\n",
       "param_criterion                                                                                      gini\n",
       "param_max_depth                                                                                        10\n",
       "param_max_features                                                                                   sqrt\n",
       "param_n_estimators                                                                                     17\n",
       "params                 {'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 17}\n",
       "split0_test_precision                                                                            0.916216\n",
       "split1_test_precision                                                                            0.916822\n",
       "split2_test_precision                                                                            0.916032\n",
       "mean_test_precision                                                                              0.916357\n",
       "std_test_precision                                                                               0.000337\n",
       "rank_test_precision                                                                                     2\n",
       "split0_test_recall                                                                               0.999447\n",
       "split1_test_recall                                                                               0.999447\n",
       "split2_test_recall                                                                               0.997236\n",
       "mean_test_recall                                                                                  0.99871\n",
       "std_test_recall                                                                                  0.001043\n",
       "rank_test_recall                                                                                       18\n",
       "split0_test_f1                                                                                   0.956024\n",
       "split1_test_f1                                                                                   0.956353\n",
       "split2_test_f1                                                                                    0.95491\n",
       "mean_test_f1                                                                                     0.955762\n",
       "std_test_f1                                                                                      0.000617\n",
       "rank_test_f1                                                                                            1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "rf_cvdf = pd.DataFrame(cv.cv_results_).sort_values(by='rank_test_f1').reset_index(drop=True)\n",
    "rf_best = pd.DataFrame(rf_cvdf.iloc[0,:])\n",
    "display(rf_cvdf)\n",
    "display(rf_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c0c1d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.06      0.11       515\n",
      "           1       0.92      1.00      0.96      5426\n",
      "\n",
      "    accuracy                           0.92      5941\n",
      "   macro avg       0.89      0.53      0.53      5941\n",
      "weighted avg       0.91      0.92      0.88      5941\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_rf = RandomForestClassifier(class_weight = {0:8.0, 1:1.0}, criterion='gini', max_depth= 15, max_features= 'sqrt', n_estimators= 50)\n",
    "\n",
    "best_rf.fit(X_tr_vec, y_tr)\n",
    "y_pr = best_rf.predict(X_te_vec)\n",
    "\n",
    "print(classification_report(y_te, y_pr))\n",
    "sns.heatmap(confusion_matrix(y_te, y_pr), cmap='Blues', cbar=True, annot=True, xticklabels= True, yticklabels=True, fmt = 'd')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e7954c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16279, number of negative: 1544\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.328603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 40627\n",
      "[LightGBM] [Info] Number of data points in the train set: 17823, number of used features: 1999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678320 -> initscore=0.746062\n",
      "[LightGBM] [Info] Start training from score 0.746062\n"
     ]
    }
   ],
   "source": [
    "lgbm = lgb.LGBMClassifier(objective='binary', class_weight={0: 5.0, 1: 1.0}, n_estimators=500)\n",
    "\n",
    "lgbm.fit(X_tr_vec, y_tr)\n",
    "\n",
    "y_pr = lgbm.predict(X_te_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "827be78d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.60      0.58       515\n",
      "           1       0.96      0.96      0.96      5426\n",
      "\n",
      "    accuracy                           0.92      5941\n",
      "   macro avg       0.76      0.78      0.77      5941\n",
      "weighted avg       0.93      0.92      0.93      5941\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAGwCAYAAACZ7H64AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA2gElEQVR4nO3de1xUdf7H8ffIXcRJUEBKC5MM08ywcKy8a1ZGbu1qYWQrXsrSSE3XdSsrA7VWK0kj0izSrK217dcaaTfLVbwlrpplF6zcRNAQhWggPL8/3GYbUQ/UHA7S67mP83jIOd858x0ei739fL7fg8MwDEMAAAA2amL3BAAAAAgkAADAdgQSAABgOwIJAACwHYEEAADYjkACAABsRyABAAC2I5AAAADb+ds9ASuUuXnWG3Ai/n4Ou6cANDjB9fBfwpCud/rkPhVbM31yn4aICgkAALBdo6yQAADQoDj4978ZAgkAAFZz0C41QyABAMBqVEhM8R0CAAC2o0ICAIDVaNmYIpAAAGA1Wjam+A4BAADbUSEBAMBqtGxMUSEBAMBqjia+OepgxowZcjgcXkd0dLTnumEYmjFjhmJiYhQSEqLevXtr586dXvdwu90aP368WrZsqdDQUCUlJWnv3r1eY0pKSpSSkiKn0ymn06mUlBQdOnSozt8iAgkAAI3UBRdcoH379nmO7du3e67NmTNHc+fOVWZmpjZt2qTo6GgNGDBAR44c8YxJS0vTihUrtHz5cq1du1ZlZWUaPHiwqqurPWOSk5OVn5+v3Nxc5ebmKj8/XykpKXWeKy0bAACsZlPLxt/f36sq8hPDMPTYY49p+vTpuv766yVJzz33nKKiorRs2TKNHTtWpaWlWrRokXJyctS/f39J0gsvvKA2bdro7bff1pVXXqldu3YpNzdXeXl5SkxMlCRlZ2fL5XLp008/VYcOHWo9VyokAABYzUctG7fbrcOHD3sdbrf7pG/72WefKSYmRrGxsbrxxhv15ZdfSpIKCgpUWFiogQMHesYGBQWpV69eWrdunSRpy5Ytqqqq8hoTExOjTp06ecasX79eTqfTE0YkqXv37nI6nZ4xtUUgAQDgNJGRkeFZq/HTkZGRccKxiYmJev755/XWW28pOztbhYWF6tGjhw4ePKjCwkJJUlRUlNdroqKiPNcKCwsVGBioFi1anHJMZGRkjfeOjIz0jKktWjYAAFjNRy2badOmaeLEiV7ngoKCTjj2qquu8vy5c+fOcrlcOvfcc/Xcc8+pe/fu/52W97wMw6hx7njHjznR+Nrc53hUSAAAsJqPWjZBQUFq3ry513GyQHK80NBQde7cWZ999plnXcnxVYyioiJP1SQ6OlqVlZUqKSk55Zj9+/fXeK/i4uIa1RczBBIAAKzmcPjm+BXcbrd27dql1q1bKzY2VtHR0Vq9erXnemVlpdasWaMePXpIkhISEhQQEOA1Zt++fdqxY4dnjMvlUmlpqTZu3OgZs2HDBpWWlnrG1BYtGwAAGqHJkyfr2muvVdu2bVVUVKSZM2fq8OHDGjFihBwOh9LS0pSenq64uDjFxcUpPT1dTZs2VXJysiTJ6XQqNTVVkyZNUkREhMLDwzV58mR17tzZs+smPj5egwYN0ujRo5WVlSVJGjNmjAYPHlynHTYSgQQAAOvZ8Lts9u7dq5tuukkHDhxQq1at1L17d+Xl5enss8+WJE2ZMkUVFRUaN26cSkpKlJiYqFWrViksLMxzj3nz5snf319Dhw5VRUWF+vXrpyVLlsjPz88zZunSpZowYYJnN05SUpIyMzPrPF+HYRjGr/zMDU6Zu9F9JMAn/P14fDVwvOB6+Kd5SK8HfXKfijX3+eQ+DRFrSAAAgO1o2QAAYLUmVCfNEEgAALCaDWtITjd8hwAAgO2okAAAYDWbfrne6YRAAgCA1WjZmOI7BAAAbEeFBAAAq9GyMUUgAQDAarRsTBFIAACwGhUSU0Q2AABgOyokAABYjZaNKQIJAABWo2VjisgGAABsR4UEAACr0bIxRSABAMBqtGxMEdkAAIDtqJAAAGA1WjamCCQAAFiNQGKK7xAAALAdFRIAAKzGolZTBBIAAKxGy8YUgQQAAKtRITFFZAMAALajQgIAgNVo2ZgikAAAYDVaNqaIbAAAwHZUSAAAsJiDCokpAgkAABYjkJijZQMAAGxHhQQAAKtRIDFFIAEAwGK0bMzRsgEAALajQgIAgMWokJgjkAAAYDECiTkCCQAAFiOQmGMNCQAAsB0VEgAArEaBxBSBBAAAi9GyMUfLBgAA2I4KCQAAFqNCYo5AAgCAxQgk5mjZAAAA21EhAQDAYlRIzBFIAACwGnnEFC0bAABgOyokAABYjJaNOQIJAAAWI5CYI5AAAGAxAok51pAAAADbUSEBAMBqFEhMEUgAALAYLRtztGwAAIDtqJAAAGAxKiTmCCQAAFiMQGKOlg0AALAdFRIAACxGhcQcgQQAAKuRR0zRsgEAALajQgIAgMVo2ZgjkAAAYDECiTkCCQAAFiOQmGMNCQAAsB2BBAAAqzl8dPwKGRkZcjgcSktL85wzDEMzZsxQTEyMQkJC1Lt3b+3cudPrdW63W+PHj1fLli0VGhqqpKQk7d2712tMSUmJUlJS5HQ65XQ6lZKSokOHDtVpfgQSAAAs5nA4fHL8Ups2bdLTTz+tCy+80Ov8nDlzNHfuXGVmZmrTpk2Kjo7WgAEDdOTIEc+YtLQ0rVixQsuXL9fatWtVVlamwYMHq7q62jMmOTlZ+fn5ys3NVW5urvLz85WSklKnORJIAABoxMrKyjR8+HBlZ2erRYsWnvOGYeixxx7T9OnTdf3116tTp0567rnn9P3332vZsmWSpNLSUi1atEh//etf1b9/f3Xt2lUvvPCCtm/frrfffluStGvXLuXm5uqZZ56Ry+WSy+VSdna23njjDX366ae1nieBBHXyt5de1LAbktTTlaCergTdevMw/evDDzzXDcNQ1oL5urLfFepxSReNGZmiLz7/zOse33zztSal3al+vVzq6UrQ1MlpOnjwQH1/FMCnFmVnKXnoDXJd0lW9r3Apbfw47Sn40muMYRha+OR89e99uS69+EKl3pqiz4/7+ZCkbflbNeqPtyix20W6vHs3pd6aoh9++KG+Pgos4KsKidvt1uHDh70Ot9t9yve+4447dM0116h///5e5wsKClRYWKiBAwd6zgUFBalXr15at26dJGnLli2qqqryGhMTE6NOnTp5xqxfv15Op1OJiYmeMd27d5fT6fSMqQ0CCeokKipK49MmKefFV5Tz4iu65NLumnjXHZ7Q8dyzz2hpzhJNnXavnl/2N0W0bKVxY0eqvLxMklTx/fe6Y2yqHA6HnspeokXPLVNVVZXuHn+7jh49audHA36VzZs2athNw5Xz4svKyn5WP1ZX67bRqfr+++89Y55dlK2c557Vn6bfp6UvvaKIli1126g/en4+pGNhZNzYUXL1uFxLl/9NS196RTfeNFxNmvDX9enMV4EkIyPDs07jpyMjI+Ok77t8+XJ99NFHJxxTWFgo6djf6z8XFRXluVZYWKjAwECvysqJxkRGRta4f2RkpGdMbbDtF3XSs3dfr6/vmHC3Xnl5ubb/e5vandtey154XiNH36a+/Y+l6QdmztKAPpcpd+UbuuEPNyo//yPt+/Y/WvbyCjVr1kySNOOhdPW5PFGbNuYpsXuPev9MgC8sfHqR19cPzsxQnytc2vXxTiV0u0SGYWhpzvMaNeY29R9w7OdjZvps9e3ZQyv/+Yb+MPRGSdIjszN00/AUpY4e47nX2WefU2+fAw3btGnTNHHiRK9zQUFBJxz7zTff6K677tKqVasUHBx80nsevzbFMAzT9SrHjznR+Nrc5+eI3PjFqqur9dab/1RFxfe6sMtF+s9/9urggWJ1d13mGRMYGKiEhEu0LX+rJKmqslIOh0OBgYE/GxOkJk2aKP+jLfX+GQCrlP13UWBzp1OS9J+9e3XgQLFcl13uGRMYGKiEbpdo29ZjPx8HDx7U9n9vU3hEhG4ZfqP69OyhkSNu1kdbNtf/B4BP+apCEhQUpObNm3sdJwskW7ZsUVFRkRISEuTv7y9/f3+tWbNGTzzxhPz9/T2VkeOrGEVFRZ5r0dHRqqysVElJySnH7N+/v8b7FxcX16i+nIqtgWTv3r2aPn26+vTpo/j4eHXs2FF9+vTR9OnT9c0339g5NZzCZ7s/1eWJF8vV7UKlz5yhRx/LVLtz2+vggWJJUkREhNf48IgIzxqRzhdepOCQED0x71FVVFSo4vvv9fjcOTp69KgO/Pf1wOnOMAw9OidDXS9OUFzceZLk+f/38T8fEREtdeDAsZ+P/+w99vfeU09m6vrf/0ELsp5RfHxHjUm9VV99taf+PgB8z4Ztv/369dP27duVn5/vObp166bhw4crPz9f7dq1U3R0tFavXu15TWVlpdasWaMePY5VqxMSEhQQEOA1Zt++fdqxY4dnjMvlUmlpqTZu3OgZs2HDBpWWlnrG1IZtLZu1a9fqqquuUps2bTRw4EANHDhQhmGoqKhIr732mubPn68333xTl1122Snv43a7ayzoqVLgSRMjfr1zYmP14t9W6MiRw3rn7VW6/y9/UvbinP8NqFH+kxz//UlqER6u2Y8+poyZD2j5shw1adJEV151jc6P76gmTfzq82MAlsmY+aA+271bS3KW1bh24vL4sT//tI7q90OHacjvbpAkxcd31IYN6/Xa31/VXXdPsnbiaFTCwsLUqVMnr3OhoaGKiIjwnE9LS1N6erri4uIUFxen9PR0NW3aVMnJyZIkp9Op1NRUTZo0SREREQoPD9fkyZPVuXNnzyLZ+Ph4DRo0SKNHj1ZWVpYkacyYMRo8eLA6dOhQ6/naFkjuvvtujRo1SvPmzTvp9bS0NG3atOmU98nIyNADDzzgdW7a9Pv053tn+GqqOE5AQKDatD1bktTxgs76eMcOvbj0eY0YOVqSdPDAAbVq9b8FTiXfHVT4z/5V6OpxuV5fuVolJSXy9/NTWPPmGtjncp155ln1+0EAC2Q8/JDef/9dLX7uBUVFR3vOt2zZSpJ04Lifj+++O6iIiJbHxrQ6Nqbdued63TO23bkq3Pet1VOHhRrqo+OnTJmiiooKjRs3TiUlJUpMTNSqVasUFhbmGTNv3jz5+/tr6NChqqioUL9+/bRkyRL5+f3vH5FLly7VhAkTPLtxkpKSlJmZWae52BZIduzYoRdeeOGk18eOHaunnnrK9D4nWuBTpcCTjIYVDMNQZWWlzjzzLEW0bKUN69fp/PiOkqSqqkpt2bJJE9Jq/svup1XbGzfk6bvvDqpn7z71Om/AlwzDUMbDD+ndd1Zr0ZIcnXVWG6/rZ551llq2bKW8df9S/E8/H5WV2rJ5k+6aOPnYmDPPUqvISO0pKPB67Vd79ujyK3rWzweBJRpKIHn//fe9vnY4HJoxY4ZmzJhx0tcEBwdr/vz5mj9//knHhIeHn/K/6bVhWyBp3bq11q1bd9Jyzvr169W6dWvT+wQFBdVoz5S5DZ/METVlPj5Xl13eU1HR0SovL9eq3JXasnmj5i/MlsPhUPLNt2jxoiy1OftstW17thY/k6Xg4GANunqw5x6vv/aqYmPP1Rnh4dq+LV+Pzn5YySkjdE5sOxs/GfDrpD/0gN5c+YYem79AoU1DdaD42JqRZmFhCg4OlsPh0PCUW7QoO0ttzz5Hbc8+W4uePvbzcfU1x34+HA6Hbv1jqhY+OV8dOpyvDufH6/V/rNCegi/113lP2Pnx8Cs1kDzSoNkWSCZPnqzbbrtNW7Zs0YABAxQVFSWHw6HCwkKtXr1azzzzjB577DG7poeT+O67g7p3+hQdKC5Ws2Zhijuvg+YvzPbsrBnxx1Fy//CDZj38oI4cLlWnzhfqyacWKTS0mecee/bsUebj81RaWqqYM2M0cvRtGp5yq02fCPCNl196UZKUeqv347IfnJmh6353vSTpj6mj5Xa7lf7QAzp8uFSdL+yihdmLvX4+br7lVrndlXpkToZKS0vVocP5eip7sdq0bVt/HwawgcMwDNvKCS+99JLmzZunLVu2eJ6J7+fnp4SEBE2cOFFDhw79RfelQgKcmL8f/0wDjhdcD/80j7sn1yf3+eyRQT65T0NkayD5SVVVlWfbW8uWLRUQEPCr7kcgAU6MQALUVB+B5Lwpvgkku+c03kDSIJ7UGhAQUKv1IgAAoHFqEIEEAIDGrKHssmnICCQAAFiMPGKO32UDAABsR4UEAACLNWlCicQMgQQAAIvRsjFHywYAANiOCgkAABZjl405AgkAABYjj5gjkAAAYDEqJOZYQwIAAGxHhQQAAItRITFHIAEAwGLkEXO0bAAAgO2okAAAYDFaNuYIJAAAWIw8Yo6WDQAAsB0VEgAALEbLxhyBBAAAi5FHzNGyAQAAtqNCAgCAxWjZmCOQAABgMfKIOQIJAAAWo0JijjUkAADAdlRIAACwGAUScwQSAAAsRsvGHC0bAABgOyokAABYjAKJOQIJAAAWo2VjjpYNAACwHRUSAAAsRoHEHIEEAACL0bIxR8sGAADYjgoJAAAWo0JijkACAIDFyCPmCCQAAFiMCok51pAAAADbUSEBAMBiFEjMEUgAALAYLRtztGwAAIDtqJAAAGAxCiTmCCQAAFisCYnEFC0bAABgOyokAABYjAKJOQIJAAAWY5eNOQIJAAAWa0IeMcUaEgAAYDsqJAAAWIyWjTkCCQAAFiOPmKNlAwAAbEeFBAAAizlEicQMgQQAAIuxy8YcLRsAAGA7KiQAAFiMXTbmCCQAAFiMPGKOlg0AALAdFRIAACzWhBKJKQIJAAAWI4+YI5AAAGAxFrWaYw0JAACwHYEEAACLORy+Oepi4cKFuvDCC9W8eXM1b95cLpdLb775pue6YRiaMWOGYmJiFBISot69e2vnzp1e93C73Ro/frxatmyp0NBQJSUlae/evV5jSkpKlJKSIqfTKafTqZSUFB06dKjO3yMCCQAAFmvicPjkqIuzzjpLs2bN0ubNm7V582b17dtX1113nSd0zJkzR3PnzlVmZqY2bdqk6OhoDRgwQEeOHPHcIy0tTStWrNDy5cu1du1alZWVafDgwaqurvaMSU5OVn5+vnJzc5Wbm6v8/HylpKTU+XvkMAzDqPOrGrgyd6P7SIBP+PvRxwaOF1wPqymHPbfVJ/d5/saOcrvdXueCgoIUFBRUq9eHh4frkUce0ciRIxUTE6O0tDRNnTpV0rFqSFRUlGbPnq2xY8eqtLRUrVq1Uk5OjoYNGyZJ+vbbb9WmTRutXLlSV155pXbt2qWOHTsqLy9PiYmJkqS8vDy5XC598skn6tChQ60/GxUSAAAs5vDRkZGR4WmN/HRkZGSYvn91dbWWL1+u8vJyuVwuFRQUqLCwUAMHDvSMCQoKUq9evbRu3TpJ0pYtW1RVVeU1JiYmRp06dfKMWb9+vZxOpyeMSFL37t3ldDo9Y2qLXTYAAFjMV7tspk2bpokTJ3qdO1V1ZPv27XK5XPrhhx/UrFkzrVixQh07dvSEhaioKK/xUVFR+uqrryRJhYWFCgwMVIsWLWqMKSws9IyJjIys8b6RkZGeMbVFIAEA4DRRl/aMJHXo0EH5+fk6dOiQXn31VY0YMUJr1qzxXD8+KBmGYRqejh9zovG1uc/xaNkAAGCxJg7fHHUVGBio9u3bq1u3bsrIyFCXLl30+OOPKzo6WpJqVDGKioo8VZPo6GhVVlaqpKTklGP2799f432Li4trVF/MEEgAALCYw+HwyfFrGYYht9ut2NhYRUdHa/Xq1Z5rlZWVWrNmjXr06CFJSkhIUEBAgNeYffv2aceOHZ4xLpdLpaWl2rhxo2fMhg0bVFpa6hlTW7Vq2bz++uu1vmFSUlKdJgAAAHzvz3/+s6666iq1adNGR44c0fLly/X+++8rNzdXDodDaWlpSk9PV1xcnOLi4pSenq6mTZsqOTlZkuR0OpWamqpJkyYpIiJC4eHhmjx5sjp37qz+/ftLkuLj4zVo0CCNHj1aWVlZkqQxY8Zo8ODBddphI9UykAwZMqRWN3M4HF57kwEAgD2/y2b//v1KSUnRvn375HQ6deGFFyo3N1cDBgyQJE2ZMkUVFRUaN26cSkpKlJiYqFWrViksLMxzj3nz5snf319Dhw5VRUWF+vXrpyVLlsjPz88zZunSpZowYYJnN05SUpIyMzPrPF+eQwL8hvAcEqCm+ngOyS3L/u2T+zyffKFP7tMQscsGAACL/ZIFqb81vyiQlJeXa82aNfr6669VWVnpdW3ChAk+mRgAAPjtqHMg2bp1q66++mp9//33Ki8vV3h4uA4cOKCmTZsqMjKSQAIAwHF89WC0xqzO237vvvtuXXvttfruu+8UEhKivLw8ffXVV0pISNCjjz5qxRwBADit+erR8Y1ZnQNJfn6+Jk2aJD8/P/n5+cntdqtNmzaaM2eO/vznP1sxRwAA0MjVOZAEBAR4Sk9RUVH6+uuvJR3br/zTnwEAwP80cTh8cjRmdV5D0rVrV23evFnnnXee+vTpo/vuu08HDhxQTk6OOnfubMUcAQA4rTXyLOETda6QpKenq3Xr1pKkhx56SBEREbr99ttVVFSkp59+2ucTBAAAjV+dKyTdunXz/LlVq1ZauXKlTycEAEBjwy4bczwYDQAAi5FHzNU5kMTGxp4y6X355Ze/akIAAOC3p86BJC0tzevrqqoqbd26Vbm5ubrnnnt8NS8AABqNxr5DxhfqHEjuuuuuE55/8skntXnz5l89IQAAGhvyiLk677I5mauuukqvvvqqr24HAECj4XA4fHI0Zj4LJK+88orCw8N9dTsAAPAb8osejPbzlGYYhgoLC1VcXKwFCxb4dHK/lB+/5xk4oRaX3Gn3FIAGp2JrpuXv4bN//TdidQ4k1113nVcgadKkiVq1aqXevXvr/PPP9+nkAABoDBp7u8UX6hxIZsyYYcE0AADAb1mdq0h+fn4qKiqqcf7gwYPy8/PzyaQAAGhMmjh8czRmda6QGIZxwvNut1uBgYG/ekIAADQ2jT1M+EKtA8kTTzwh6Vgf7JlnnlGzZs0816qrq/XBBx+whgQAAPwitQ4k8+bNk3SsQvLUU095tWcCAwN1zjnn6KmnnvL9DAEAOM2xqNVcrQNJQUGBJKlPnz76+9//rhYtWlg2KQAAGhNaNubqvIbkvffes2IeAADgN6zOu2x+//vfa9asWTXOP/LII/rDH/7gk0kBANCYOBy+ORqzOgeSNWvW6JprrqlxftCgQfrggw98MikAABqTJg6HT47GrM4tm7KyshNu7w0ICNDhw4d9MikAABoTHh1vrs7fo06dOumll16qcX758uXq2LGjTyYFAAB+W+pcIbn33nt1ww036IsvvlDfvn0lSe+8846WLVumV155xecTBADgdNfIuy0+UedAkpSUpNdee03p6el65ZVXFBISoi5duujdd99V8+bNrZgjAACntca+/sMX6hxIJOmaa67xLGw9dOiQli5dqrS0NG3btk3V1dU+nSAAAGj8fvE6m3fffVc333yzYmJilJmZqauvvlqbN2/25dwAAGgU2PZrrk4Vkr1792rJkiVavHixysvLNXToUFVVVenVV19lQSsAACfBk1rN1bpCcvXVV6tjx476+OOPNX/+fH377beaP3++lXMDAAC/EbWukKxatUoTJkzQ7bffrri4OCvnBABAo8KiVnO1rpB8+OGHOnLkiLp166bExERlZmaquLjYyrkBANAosIbEXK0DicvlUnZ2tvbt26exY8dq+fLlOvPMM3X06FGtXr1aR44csXKeAACgEavzLpumTZtq5MiRWrt2rbZv365JkyZp1qxZioyMVFJSkhVzBADgtNbE4ZujMftVj9fv0KGD5syZo7179+rFF1/01ZwAAGhUHD76X2P2ix6Mdjw/Pz8NGTJEQ4YM8cXtAABoVBp7dcMX+AWEAADAdj6pkAAAgJOjQmKOQAIAgMUcjX3Prg/QsgEAALajQgIAgMVo2ZgjkAAAYDE6NuZo2QAAANtRIQEAwGL8cj1zBBIAACzGGhJztGwAAIDtqJAAAGAxOjbmCCQAAFisSSP/xXi+QCABAMBiVEjMsYYEAADYjgoJAAAWY5eNOQIJAAAW4zkk5mjZAAAA21EhAQDAYhRIzBFIAACwGC0bc7RsAACA7aiQAABgMQok5ggkAABYjHaEOb5HAADAdgQSAAAs5nA4fHLURUZGhi655BKFhYUpMjJSQ4YM0aeffuo1xjAMzZgxQzExMQoJCVHv3r21c+dOrzFut1vjx49Xy5YtFRoaqqSkJO3du9drTElJiVJSUuR0OuV0OpWSkqJDhw7Vab4EEgAALObw0VEXa9as0R133KG8vDytXr1aP/74owYOHKjy8nLPmDlz5mju3LnKzMzUpk2bFB0drQEDBujIkSOeMWlpaVqxYoWWL1+utWvXqqysTIMHD1Z1dbVnTHJysvLz85Wbm6vc3Fzl5+crJSWlbt8jwzCMOn7GBq+iyu4ZAA1T+KV32j0FoMGp2Jpp+Xu8sGWv+aBauDnhrF/82uLiYkVGRmrNmjXq2bOnDMNQTEyM0tLSNHXqVEnHqiFRUVGaPXu2xo4dq9LSUrVq1Uo5OTkaNmyYJOnbb79VmzZttHLlSl155ZXatWuXOnbsqLy8PCUmJkqS8vLy5HK59Mknn6hDhw61mh8VEgAAThNut1uHDx/2Otxud61eW1paKkkKDw+XJBUUFKiwsFADBw70jAkKClKvXr20bt06SdKWLVtUVVXlNSYmJkadOnXyjFm/fr2cTqcnjEhS9+7d5XQ6PWNqg0ACAIDFfNWyycjI8KzT+OnIyMgwfX/DMDRx4kRdfvnl6tSpkySpsLBQkhQVFeU1NioqynOtsLBQgYGBatGixSnHREZG1njPyMhIz5jaYNsvAAAW89VzSKZNm6aJEyd6nQsKCjJ93Z133ql///vfWrt27Qnm5j05wzBMF9AeP+ZE42tzn5+jQgIAwGkiKChIzZs39zrMAsn48eP1+uuv67333tNZZ/1vDUp0dLQk1ahiFBUVeaom0dHRqqysVElJySnH7N+/v8b7FhcX16i+nAqBBAAAi9mx7dcwDN155536+9//rnfffVexsbFe12NjYxUdHa3Vq1d7zlVWVmrNmjXq0aOHJCkhIUEBAQFeY/bt26cdO3Z4xrhcLpWWlmrjxo2eMRs2bFBpaalnTG3QsgEAwGJ2/Ov/jjvu0LJly/SPf/xDYWFhnkqI0+lUSEiIHA6H0tLSlJ6erri4OMXFxSk9PV1NmzZVcnKyZ2xqaqomTZqkiIgIhYeHa/LkyercubP69+8vSYqPj9egQYM0evRoZWVlSZLGjBmjwYMH13qHjUQgAQCgUVq4cKEkqXfv3l7nn332Wd16662SpClTpqiiokLjxo1TSUmJEhMTtWrVKoWFhXnGz5s3T/7+/ho6dKgqKirUr18/LVmyRH5+fp4xS5cu1YQJEzy7cZKSkpSZWbft1DyHBPgN4TkkQE318RySl/O/9cl9hl4U45P7NERUSAAAsBi/7Ncci1oBAIDtqJAAAGCxuu6Q+S0ikAAAYDHaEeYIJAAAWIwKiTlCGwAAsB0VEgAALEZ9xByBBAAAi9GxMUfLBgAA2I4KCQAAFmtC08YUgQQAAIvRsjFHywYAANiOCgkAABZz0LIxRSABAMBitGzM0bIBAAC2o0ICAIDF2GVjjkACAIDFaNmYI5AAAGAxAok51pAAAADbUSEBAMBibPs1RyABAMBiTcgjpmjZAAAA21EhAQDAYrRszBFIAACwGLtszNGyAQAAtqNCAgCAxWjZmCOQAABgMXbZmKNlAwAAbEeFBHWyKDtL77y9SnsKvlRQcLC6XNRVaXdP1jmx7U44/qEH7tOrf3tJk6dO080pt0qSSksPaeGT87V+3VrtLyzUGWe0UJ++/TVu/F0KCwurx08D/DLTx16tv9x2tde5wgOHFTvgz5Kk6/p2UeoNl6trfBu1bNFMicMy9O/d//EaHxURpvS036lv9/MVFhqk3XuK9Mjit7Ti7fwa7xcY4K8PciarS4ezTngvNHy0bMwRSFAnWzZv1LCbhuuCTp1V/WO1Mp+Yp9vHpOrv//inQpo29Rr77jtva/u/t6lVZKTX+eKiIhUXFWni5Klq16699u37j2Y+OEPFxUV6dN4T9fhpgF9u5+ff6prb5nu+rj5qeP7cNCRQ67d9ob+//ZEW3jf8hK9fNHOEnM2C9Ye0LB04VKZhV3VTzqyRumz4HG37dK/X2PS067SvuFRdOpxlzYeB5dhlY45AgjpZkLXI6+sHZmaob0+XPv54pxK6XeI5v3//fs1Kf1ALshZp/LixXq9pH3ee/vrY//4ib9O2re6ckKbpf7pHP/74o/z9+b8lGr4fq49q/8EjJ7z24j83SZLatg4/6esTL4zVhPTl2rzzK0nS7Gfe0vjhfXVRfBuvQDLwso7q1z1eN93zjAZdfoEPPwHqE3nEHGtI8KuUlR37C9npdHrOHT16VH+Zdo9G3Jqq9u3janefI2Vq1qwZYQSnjfZtW+nLVQ9r1xsz9PysP+qcMyPq9Pp1W7/Q7wcmqEXzpnI4HPrDlQkKCvTXB5s/84yJDA/TgntvUuq9z+v7ikpffwSgQTnt//Z3u91yu91e5442CVJQUJBNM/rtMAxDf52Toa4XJ6h93Hme888uypafn7+Sb76lVvc5dKhE2VkLdMMfhlk1VcCnNu3Yo1H35uizr4oUGRGmP40apPeWTFLC7x/Wd6XltbpHyp8WK2fWSH27Zo6qqqr1/Q+VGjYxWwV7D3jGPP3gzcp+Za0++vjrU1Zb0PA1oWdjqkFXSL755huNHDnylGMyMjLkdDq9jkdmZ9TTDH/bMh5+ULt379asOXM95z7euUPLXnheDz6cIUctfgDLyso0ftxYtTv3XI29/U4rpwv4zKp/fazX3snXzs+/1XsbPtXvxi+UJN18bWKt7zHjjmvVonlTXTX2CV128xw98cK7WvrISF3QPkaSNO6mXmoeGqxHFq+y5DOgfjl8dDRmDbpC8t133+m5557T4sWLTzpm2rRpmjhxote5o02ojlhtVvpDWvPeu1r83AuKio72nP/oo8367ruDumpAH8+56upqzX1ktpbmPK83V73rOV9eXqZxY0epadOmmvv4kwoICKjXzwD4yvc/VGrn59/q3LatajU+9qyWuv3GXrr4hpna9WWhJGn77v/osovP1dhhPTXh4eXqfcl5urRzrEo3POb12n8tnaLlb27W6PtyfP0xAFvZGkhef/31U17/8ssvTe8RFFSzPVNR9aumhVMwDEOz0h/Su++s1jPP5ujMs9p4XR987XXq3r2H17nbx6Zq8LXX6boh13vOlZWVadzYVAUEBOqx+QtpseG0Fhjgr/Njo/SvrZ/XanzT4EBJ0lHD8DpfXW14SvuT5ryiGU++4bnWupVTbyy8Uyl/elabtu/xzcRRfxp7ecMHbA0kQ4YMkcPhkHHcD+XP1absj/qTPvMBvbnyDT32xAKFhobqwIFiSVKzZmEKDg7WGWe00BlntPB6jb9/gCJatvQ8q6S8vEy3jxmpHyoq9PDjj6i8vEzl5WWSpBYtwuXn51e/Hwqoo4y7f6d/frBd3+wrUWR4M00dNUhhocFa+n8bJEktmjdVm+gWah15bLH3eedESZL2Hzys/QeP6NM9hfr86yJl/uUmTZu7QgdLy5XU50L1695B19/1lCTpm8ISr/cs+/7YWrkvvynWf4oO1dMnha/wHBJztgaS1q1b68knn9SQIUNOeD0/P18JCQn1Oymc0t9eelGSNOqPKV7nH5iZ4VUBOZWPd+7U9n9vkyRde/UAr2v/fOsdnXkmz1pAw3Zm1Bl6PuOPijgjVAdKyrRx+x71GvFXfb3vWIi4pldnZT/4v5+RnNnH1sLNfGqlHs5aqR9/PKoh4xdq5oTr9MrjY9WsaZC++KZYo+7L0VtrP7blMwF2cxinKk9YLCkpSRdddJEefPDBE17ftm2bunbtqqNHj9bpvrRsgBMLv5SFw8DxKrZmWv4eG78s9cl9Lm3nNB90mrK1QnLPPfeovPzkW+Tat2+v9957rx5nBACA79GwMWdrILniiitOeT00NFS9evWqp9kAAAC7NOhtvwAANAqUSEwRSAAAsBi7bMwRSAAAsBhPsDDXoB8dDwAAfhuokAAAYDEKJOYIJAAAWI1EYoqWDQAAsB0VEgAALMYuG3MEEgAALMYuG3O0bAAAgO2okAAAYDEKJOYIJAAAWI1EYoqWDQAAsB0VEgAALMYuG3MEEgAALMYuG3MEEgAALEYeMccaEgAAYDsqJAAAWI0SiSkCCQAAFmNRqzlaNgAAwHZUSAAAsBi7bMwRSAAAsBh5xBwtGwAAGqkPPvhA1157rWJiYuRwOPTaa695XTcMQzNmzFBMTIxCQkLUu3dv7dy502uM2+3W+PHj1bJlS4WGhiopKUl79+71GlNSUqKUlBQ5nU45nU6lpKTo0KFDdZorgQQAAKs5fHTUUXl5ubp06aLMzMwTXp8zZ47mzp2rzMxMbdq0SdHR0RowYICOHDniGZOWlqYVK1Zo+fLlWrt2rcrKyjR48GBVV1d7xiQnJys/P1+5ubnKzc1Vfn6+UlJS6jRXh2EYRt0/YsNWUWX3DICGKfzSO+2eAtDgVGw98X+sfemTfd/75D7nt276i1/rcDi0YsUKDRkyRNKx6khMTIzS0tI0depUSceqIVFRUZo9e7bGjh2r0tJStWrVSjk5ORo2bJgk6dtvv1WbNm20cuVKXXnlldq1a5c6duyovLw8JSYmSpLy8vLkcrn0ySefqEOHDrWaHxUSAABOE263W4cPH/Y63G73L7pXQUGBCgsLNXDgQM+5oKAg9erVS+vWrZMkbdmyRVVVVV5jYmJi1KlTJ8+Y9evXy+l0esKIJHXv3l1Op9MzpjYIJAAAWMzh8M2RkZHhWafx05GRkfGL5lRYWChJioqK8jofFRXluVZYWKjAwEC1aNHilGMiIyNr3D8yMtIzpjbYZQMAgMV8tctm2rRpmjhxote5oKCgX3VPx3F7kg3DqHHueMePOdH42tzn56iQAABgNR8tag0KClLz5s29jl8aSKKjoyWpRhWjqKjIUzWJjo5WZWWlSkpKTjlm//79Ne5fXFxco/pyKgQSAAB+g2JjYxUdHa3Vq1d7zlVWVmrNmjXq0aOHJCkhIUEBAQFeY/bt26cdO3Z4xrhcLpWWlmrjxo2eMRs2bFBpaalnTG3QsgEAwGJ2/S6bsrIyff75556vCwoKlJ+fr/DwcLVt21ZpaWlKT09XXFyc4uLilJ6erqZNmyo5OVmS5HQ6lZqaqkmTJikiIkLh4eGaPHmyOnfurP79+0uS4uPjNWjQII0ePVpZWVmSpDFjxmjw4MG13mEjEUgAALCcXY+O37x5s/r06eP5+qf1JyNGjNCSJUs0ZcoUVVRUaNy4cSopKVFiYqJWrVqlsLAwz2vmzZsnf39/DR06VBUVFerXr5+WLFkiPz8/z5ilS5dqwoQJnt04SUlJJ332ycnwHBLgN4TnkAA11cdzSD4vqvDJfdpHhvjkPg0RFRIAACzG77IxRyABAMBqJBJT7LIBAAC2o0ICAIDF7NplczohkAAAYDG7dtmcTmjZAAAA21EhAQDAYhRIzBFIAACwGonEFIEEAACLsajVHGtIAACA7aiQAABgMXbZmCOQAABgMfKIOVo2AADAdlRIAACwGC0bcwQSAAAsRyIxQ8sGAADYjgoJAAAWo2VjjkACAIDFyCPmaNkAAADbUSEBAMBitGzMEUgAALAYv8vGHIEEAACrkUdMsYYEAADYjgoJAAAWo0BijkACAIDFWNRqjpYNAACwHRUSAAAsxi4bcwQSAACsRh4xRcsGAADYjgoJAAAWo0BijkACAIDF2GVjjpYNAACwHRUSAAAsxi4bcwQSAAAsRsvGHC0bAABgOwIJAACwHS0bAAAsRsvGHIEEAACLsajVHC0bAABgOyokAABYjJaNOQIJAAAWI4+Yo2UDAABsR4UEAACrUSIxRSABAMBi7LIxR8sGAADYjgoJAAAWY5eNOQIJAAAWI4+YI5AAAGA1Eokp1pAAAADbUSEBAMBi7LIxRyABAMBiLGo1R8sGAADYzmEYhmH3JNA4ud1uZWRkaNq0aQoKCrJ7OkCDwc8GUBOBBJY5fPiwnE6nSktL1bx5c7unAzQY/GwANdGyAQAAtiOQAAAA2xFIAACA7QgksExQUJDuv/9+Fu0Bx+FnA6iJRa0AAMB2VEgAAIDtCCQAAMB2BBIAAGA7AgkAALAdgQSWWbBggWJjYxUcHKyEhAR9+OGHdk8JsNUHH3yga6+9VjExMXI4HHrttdfsnhLQYBBIYImXXnpJaWlpmj59urZu3aorrrhCV111lb7++mu7pwbYpry8XF26dFFmZqbdUwEaHLb9whKJiYm6+OKLtXDhQs+5+Ph4DRkyRBkZGTbODGgYHA6HVqxYoSFDhtg9FaBBoEICn6usrNSWLVs0cOBAr/MDBw7UunXrbJoVAKAhI5DA5w4cOKDq6mpFRUV5nY+KilJhYaFNswIANGQEEljG4XB4fW0YRo1zAABIBBJYoGXLlvLz86tRDSkqKqpRNQEAQCKQwAKBgYFKSEjQ6tWrvc6vXr1aPXr0sGlWAICGzN/uCaBxmjhxolJSUtStWze5XC49/fTT+vrrr3XbbbfZPTXANmVlZfr88889XxcUFCg/P1/h4eFq27atjTMD7Me2X1hmwYIFmjNnjvbt26dOnTpp3rx56tmzp93TAmzz/vvvq0+fPjXOjxgxQkuWLKn/CQENCIEEAADYjjUkAADAdgQSAABgOwIJAACwHYEEAADYjkACAABsRyABAAC2I5AAAADbEUgAAIDtCCRAIzRjxgxddNFFnq9vvfVWDRkypN7nsWfPHjkcDuXn59f7ewM4vRBIgHp06623yuFwyOFwKCAgQO3atdPkyZNVXl5u6fs+/vjjtX40OSECgB345XpAPRs0aJCeffZZVVVV6cMPP9SoUaNUXl6uhQsXeo2rqqpSQECAT97T6XT65D4AYBUqJEA9CwoKUnR0tNq0aaPk5GQNHz5cr732mqfNsnjxYrVr105BQUEyDEOlpaUaM2aMIiMj1bx5c/Xt21fbtm3zuuesWbMUFRWlsLAwpaam6ocffvC6fnzL5ujRo5o9e7bat2+voKAgtW3bVg8//LAkKTY2VpLUtWtXORwO9e7d2/O6Z599VvHx8QoODtb555+vBQsWeL3Pxo0b1bVrVwUHB6tbt27aunWrD79zABozKiSAzUJCQlRVVSVJ+vzzz/Xyyy/r1VdflZ+fnyTpmmuuUXh4uFauXCmn06msrCz169dPu3fvVnh4uF5++WXdf//9evLJJ3XFFVcoJydHTzzxhNq1a3fS95w2bZqys7M1b948XX755dq3b58++eQTScdCxaWXXqq3335bF1xwgQIDAyVJ2dnZuv/++5WZmamuXbtq69atGj16tEJDQzVixAiVl5dr8ODB6tu3r1544QUVFBTorrvusvi7B6DRMADUmxEjRhjXXXed5+sNGzYYERERxtChQ43777/fCAgIMIqKijzX33nnHaN58+bGDz/84HWfc88918jKyjIMwzBcLpdx2223eV1PTEw0unTpcsL3PXz4sBEUFGRkZ2efcI4FBQWGJGPr1q1e59u0aWMsW7bM69xDDz1kuFwuwzAMIysrywgPDzfKy8s91xcuXHjCewHA8WjZAPXsjTfeULNmzRQcHCyXy6WePXtq/vz5kqSzzz5brVq18ozdsmWLysrKFBERoWbNmnmOgoICffHFF5KkXbt2yeVyeb3H8V//3K5du+R2u9WvX79az7m4uFjffPONUlNTveYxc+ZMr3l06dJFTZs2rdU8AODnaNkA9axPnz5auHChAgICFBMT47VwNTQ01Gvs0aNH1bp1a73//vs17nPGGWf8ovcPCQmp82uOHj0q6VjbJjEx0evaT60lwzB+0XwAQCKQAPUuNDRU7du3r9XYiy++WIWFhfL399c555xzwjHx8fHKy8vTLbfc4jmXl5d30nvGxcUpJCRE77zzjkaNGlXj+k9rRqqrqz3noqKidOaZZ+rLL7/U8OHDT3jfjh07KicnRxUVFZ7Qc6p5AMDP0bIBGrD+/fvL5XJpyJAheuutt7Rnzx6tW7dOf/nLX7R582ZJ0l133aXFixdr8eLF2r17t+6//37t3LnzpPcMDg7W1KlTNWXKFD3//PP64osvlJeXp0WLFkmSIiMjFRISotzcXO3fv1+lpaWSjj1sLSMjQ48//rh2796t7du369lnn9XcuXMlScnJyWrSpIlSU1P18ccfa+XKlXr00Uct/g4BaCwIJEAD5nA4tHLlSvXs2VMjR47UeeedpxtvvFF79uxRVFSUJGnYsGG67777NHXqVCUkJOirr77S7bfffsr73nvvvZo0aZLuu+8+xcfHa9iwYSoqKpIk+fv764knnlBWVpZiYmJ03XXXSZJGjRqlZ555RkuWLFHnzp3Vq1cvLVmyxLNNuFmzZvq///s/ffzxx+rataumT5+u2bNnW/jdAdCYOAwavwAAwGZUSAAAgO0IJAAAwHYEEgAAYDsCCQAAsB2BBAAA2I5AAgAAbEcgAQAAtiOQAAAA2xFIAACA7QgkAADAdgQSAABgu/8H4PQCFUwf2qcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(classification_report(y_te, y_pr))\n",
    "sns.heatmap(confusion_matrix(y_te, y_pr), cmap='Blues', cbar=True, annot=True, xticklabels= True, yticklabels=True, fmt = 'd')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7b05a050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.137540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[CV 1/3] END class_weight=balanced, n_estimators=66; f1: (test=0.919) precision: (test=0.976) recall: (test=0.868) total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/3] END class_weight=balanced, n_estimators=66; f1: (test=0.911) precision: (test=0.979) recall: (test=0.851) total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/3] END class_weight=balanced, n_estimators=66; f1: (test=0.924) precision: (test=0.978) recall: (test=0.874) total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.156867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678165 -> initscore=0.745353\n",
      "[LightGBM] [Info] Start training from score 0.745353\n",
      "[CV 1/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=460; f1: (test=0.956) precision: (test=0.955) recall: (test=0.957) total time=  13.2s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 2/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=460; f1: (test=0.953) precision: (test=0.958) recall: (test=0.948) total time=  15.0s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.138569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 3/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=460; f1: (test=0.958) precision: (test=0.959) recall: (test=0.958) total time=  14.7s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[CV 1/3] END class_weight=balanced, n_estimators=388; f1: (test=0.946) precision: (test=0.961) recall: (test=0.932) total time=  11.5s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/3] END class_weight=balanced, n_estimators=388; f1: (test=0.944) precision: (test=0.964) recall: (test=0.924) total time=  10.8s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/3] END class_weight=balanced, n_estimators=388; f1: (test=0.950) precision: (test=0.965) recall: (test=0.935) total time=  11.5s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678165 -> initscore=0.745353\n",
      "[LightGBM] [Info] Start training from score 0.745353\n",
      "[CV 1/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=80; f1: (test=0.941) precision: (test=0.968) recall: (test=0.917) total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 2/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=80; f1: (test=0.935) precision: (test=0.973) recall: (test=0.901) total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.268323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 3/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=80; f1: (test=0.945) precision: (test=0.971) recall: (test=0.920) total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678165 -> initscore=0.745353\n",
      "[LightGBM] [Info] Start training from score 0.745353\n",
      "[CV 1/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=381; f1: (test=0.955) precision: (test=0.958) recall: (test=0.952) total time=  13.4s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 2/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=381; f1: (test=0.952) precision: (test=0.960) recall: (test=0.945) total time=  11.6s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 3/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=381; f1: (test=0.957) precision: (test=0.959) recall: (test=0.955) total time=  11.6s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.152676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678165 -> initscore=0.745353\n",
      "[LightGBM] [Info] Start training from score 0.745353\n",
      "[CV 1/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=409; f1: (test=0.956) precision: (test=0.957) recall: (test=0.955) total time=  14.2s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116176 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 2/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=409; f1: (test=0.953) precision: (test=0.959) recall: (test=0.947) total time=  14.1s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.212670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 3/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=409; f1: (test=0.958) precision: (test=0.959) recall: (test=0.957) total time=  12.1s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678165 -> initscore=0.745353\n",
      "[LightGBM] [Info] Start training from score 0.745353\n",
      "[CV 1/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=282; f1: (test=0.952) precision: (test=0.960) recall: (test=0.945) total time=   9.0s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.863381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 2/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=282; f1: (test=0.950) precision: (test=0.962) recall: (test=0.939) total time=  10.5s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 3/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=282; f1: (test=0.955) precision: (test=0.963) recall: (test=0.948) total time=  12.6s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678165 -> initscore=0.745353\n",
      "[LightGBM] [Info] Start training from score 0.745353\n",
      "[CV 1/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=145; f1: (test=0.948) precision: (test=0.964) recall: (test=0.933) total time=   9.7s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 2/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=145; f1: (test=0.945) precision: (test=0.967) recall: (test=0.924) total time=   7.8s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.171527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 3/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=145; f1: (test=0.950) precision: (test=0.967) recall: (test=0.934) total time=   7.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087676 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[CV 1/3] END class_weight=balanced, n_estimators=112; f1: (test=0.929) precision: (test=0.973) recall: (test=0.889) total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.192337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/3] END class_weight=balanced, n_estimators=112; f1: (test=0.922) precision: (test=0.976) recall: (test=0.873) total time=   6.9s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/3] END class_weight=balanced, n_estimators=112; f1: (test=0.931) precision: (test=0.974) recall: (test=0.892) total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139243 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[CV 1/3] END class_weight=balanced, n_estimators=319; f1: (test=0.945) precision: (test=0.962) recall: (test=0.929) total time=  10.0s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/3] END class_weight=balanced, n_estimators=319; f1: (test=0.942) precision: (test=0.966) recall: (test=0.919) total time=  11.0s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.173032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/3] END class_weight=balanced, n_estimators=319; f1: (test=0.948) precision: (test=0.967) recall: (test=0.931) total time=  13.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3, estimator=LGBMClassifier(objective=&#x27;binary&#x27;),\n",
       "                   param_distributions={&#x27;class_weight&#x27;: [&#x27;balanced&#x27;,\n",
       "                                                         {0: 1.0, 1: 1.0},\n",
       "                                                         {0: 5.0, 1: 1.0},\n",
       "                                                         {0: 1.0, 1: 1.0},\n",
       "                                                         {0: 10.0, 1: 1.0}],\n",
       "                                        &#x27;n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001858A71BB90&gt;},\n",
       "                   refit=False, scoring=[&#x27;precision&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;],\n",
       "                   verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3, estimator=LGBMClassifier(objective=&#x27;binary&#x27;),\n",
       "                   param_distributions={&#x27;class_weight&#x27;: [&#x27;balanced&#x27;,\n",
       "                                                         {0: 1.0, 1: 1.0},\n",
       "                                                         {0: 5.0, 1: 1.0},\n",
       "                                                         {0: 1.0, 1: 1.0},\n",
       "                                                         {0: 10.0, 1: 1.0}],\n",
       "                                        &#x27;n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001858A71BB90&gt;},\n",
       "                   refit=False, scoring=[&#x27;precision&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;],\n",
       "                   verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(objective=&#x27;binary&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(objective=&#x27;binary&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LGBMClassifier(objective='binary'),\n",
       "                   param_distributions={'class_weight': ['balanced',\n",
       "                                                         {0: 1.0, 1: 1.0},\n",
       "                                                         {0: 5.0, 1: 1.0},\n",
       "                                                         {0: 1.0, 1: 1.0},\n",
       "                                                         {0: 10.0, 1: 1.0}],\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001858A71BB90>},\n",
       "                   refit=False, scoring=['precision', 'recall', 'f1'],\n",
       "                   verbose=3)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm = lgb.LGBMClassifier(objective='binary')\n",
    "\n",
    "lgb_cv = RandomizedSearchCV(lgbm, {'class_weight':['balanced', {0: 1.0, 1: 1.0}, {0: 5.0, 1: 1.0}, {0: 1.0, 1: 1.0}, {0: 10.0, 1: 1.0}],\n",
    "                            'n_estimators': randint(50,500)},\\\n",
    "                            n_iter=10, cv = 3, verbose=3, \\\n",
    "                            scoring = ['precision','recall','f1'], refit=False)\n",
    "\n",
    "lgb_cv.fit(X_tr_vec, y_tr)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7ddc7191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_precision</th>\n",
       "      <th>split1_test_precision</th>\n",
       "      <th>split2_test_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>rank_test_precision</th>\n",
       "      <th>split0_test_recall</th>\n",
       "      <th>split1_test_recall</th>\n",
       "      <th>split2_test_recall</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>rank_test_recall</th>\n",
       "      <th>split0_test_f1</th>\n",
       "      <th>split1_test_f1</th>\n",
       "      <th>split2_test_f1</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.408356</td>\n",
       "      <td>0.773538</td>\n",
       "      <td>0.953348</td>\n",
       "      <td>0.031670</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>460</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.955298</td>\n",
       "      <td>0.958085</td>\n",
       "      <td>0.959033</td>\n",
       "      <td>0.957472</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>10</td>\n",
       "      <td>0.956882</td>\n",
       "      <td>0.947844</td>\n",
       "      <td>0.957796</td>\n",
       "      <td>0.954174</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956089</td>\n",
       "      <td>0.952937</td>\n",
       "      <td>0.958414</td>\n",
       "      <td>0.955813</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.720309</td>\n",
       "      <td>0.909609</td>\n",
       "      <td>0.827274</td>\n",
       "      <td>0.063750</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>409</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.956819</td>\n",
       "      <td>0.958925</td>\n",
       "      <td>0.959157</td>\n",
       "      <td>0.958300</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>9</td>\n",
       "      <td>0.955408</td>\n",
       "      <td>0.946554</td>\n",
       "      <td>0.956506</td>\n",
       "      <td>0.952822</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>2</td>\n",
       "      <td>0.956113</td>\n",
       "      <td>0.952699</td>\n",
       "      <td>0.957830</td>\n",
       "      <td>0.955547</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.513864</td>\n",
       "      <td>0.853734</td>\n",
       "      <td>0.747392</td>\n",
       "      <td>0.029642</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>381</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.957545</td>\n",
       "      <td>0.959753</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.958919</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>8</td>\n",
       "      <td>0.951723</td>\n",
       "      <td>0.944895</td>\n",
       "      <td>0.955216</td>\n",
       "      <td>0.950611</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>3</td>\n",
       "      <td>0.954625</td>\n",
       "      <td>0.952266</td>\n",
       "      <td>0.957333</td>\n",
       "      <td>0.954741</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.077706</td>\n",
       "      <td>1.299400</td>\n",
       "      <td>0.728233</td>\n",
       "      <td>0.207451</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>282</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.959588</td>\n",
       "      <td>0.961669</td>\n",
       "      <td>0.963463</td>\n",
       "      <td>0.961573</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>7</td>\n",
       "      <td>0.945089</td>\n",
       "      <td>0.938629</td>\n",
       "      <td>0.947659</td>\n",
       "      <td>0.943793</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>4</td>\n",
       "      <td>0.952284</td>\n",
       "      <td>0.950009</td>\n",
       "      <td>0.955496</td>\n",
       "      <td>0.952596</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.023707</td>\n",
       "      <td>1.039626</td>\n",
       "      <td>0.337143</td>\n",
       "      <td>0.005560</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>145</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.964374</td>\n",
       "      <td>0.966647</td>\n",
       "      <td>0.966997</td>\n",
       "      <td>0.966006</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>4</td>\n",
       "      <td>0.932744</td>\n",
       "      <td>0.924069</td>\n",
       "      <td>0.934206</td>\n",
       "      <td>0.930340</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>6</td>\n",
       "      <td>0.948295</td>\n",
       "      <td>0.944879</td>\n",
       "      <td>0.950319</td>\n",
       "      <td>0.947831</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.696121</td>\n",
       "      <td>0.353304</td>\n",
       "      <td>0.667800</td>\n",
       "      <td>0.014865</td>\n",
       "      <td>balanced</td>\n",
       "      <td>388</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 388}</td>\n",
       "      <td>0.960516</td>\n",
       "      <td>0.964416</td>\n",
       "      <td>0.965169</td>\n",
       "      <td>0.963367</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>6</td>\n",
       "      <td>0.932375</td>\n",
       "      <td>0.924069</td>\n",
       "      <td>0.934574</td>\n",
       "      <td>0.930340</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>5</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>0.943812</td>\n",
       "      <td>0.949625</td>\n",
       "      <td>0.946558</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.755256</td>\n",
       "      <td>1.178632</td>\n",
       "      <td>0.662893</td>\n",
       "      <td>0.068948</td>\n",
       "      <td>balanced</td>\n",
       "      <td>319</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 319}</td>\n",
       "      <td>0.961649</td>\n",
       "      <td>0.966105</td>\n",
       "      <td>0.966520</td>\n",
       "      <td>0.964758</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>5</td>\n",
       "      <td>0.928690</td>\n",
       "      <td>0.919278</td>\n",
       "      <td>0.931073</td>\n",
       "      <td>0.926347</td>\n",
       "      <td>0.005092</td>\n",
       "      <td>7</td>\n",
       "      <td>0.944882</td>\n",
       "      <td>0.942110</td>\n",
       "      <td>0.948465</td>\n",
       "      <td>0.945152</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.837220</td>\n",
       "      <td>0.732855</td>\n",
       "      <td>0.221988</td>\n",
       "      <td>0.014651</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>80</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.967516</td>\n",
       "      <td>0.972731</td>\n",
       "      <td>0.971006</td>\n",
       "      <td>0.970418</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>3</td>\n",
       "      <td>0.916528</td>\n",
       "      <td>0.900663</td>\n",
       "      <td>0.919646</td>\n",
       "      <td>0.912279</td>\n",
       "      <td>0.008312</td>\n",
       "      <td>8</td>\n",
       "      <td>0.941332</td>\n",
       "      <td>0.935311</td>\n",
       "      <td>0.944628</td>\n",
       "      <td>0.940424</td>\n",
       "      <td>0.003858</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.882831</td>\n",
       "      <td>0.674751</td>\n",
       "      <td>0.238767</td>\n",
       "      <td>0.036211</td>\n",
       "      <td>balanced</td>\n",
       "      <td>112</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 112}</td>\n",
       "      <td>0.973376</td>\n",
       "      <td>0.975896</td>\n",
       "      <td>0.973854</td>\n",
       "      <td>0.974375</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>2</td>\n",
       "      <td>0.889257</td>\n",
       "      <td>0.873019</td>\n",
       "      <td>0.892370</td>\n",
       "      <td>0.884882</td>\n",
       "      <td>0.008484</td>\n",
       "      <td>9</td>\n",
       "      <td>0.929417</td>\n",
       "      <td>0.921595</td>\n",
       "      <td>0.931333</td>\n",
       "      <td>0.927449</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.054953</td>\n",
       "      <td>0.531393</td>\n",
       "      <td>0.164447</td>\n",
       "      <td>0.035959</td>\n",
       "      <td>balanced</td>\n",
       "      <td>66</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 66}</td>\n",
       "      <td>0.976176</td>\n",
       "      <td>0.978814</td>\n",
       "      <td>0.978351</td>\n",
       "      <td>0.977780</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>1</td>\n",
       "      <td>0.868251</td>\n",
       "      <td>0.851456</td>\n",
       "      <td>0.874493</td>\n",
       "      <td>0.864733</td>\n",
       "      <td>0.009728</td>\n",
       "      <td>10</td>\n",
       "      <td>0.919056</td>\n",
       "      <td>0.910704</td>\n",
       "      <td>0.923511</td>\n",
       "      <td>0.917757</td>\n",
       "      <td>0.005309</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      13.408356      0.773538         0.953348        0.031670   \n",
       "1      12.720309      0.909609         0.827274        0.063750   \n",
       "2      11.513864      0.853734         0.747392        0.029642   \n",
       "3      10.077706      1.299400         0.728233        0.207451   \n",
       "4       8.023707      1.039626         0.337143        0.005560   \n",
       "5      10.696121      0.353304         0.667800        0.014865   \n",
       "6      10.755256      1.178632         0.662893        0.068948   \n",
       "7       5.837220      0.732855         0.221988        0.014651   \n",
       "8       5.882831      0.674751         0.238767        0.036211   \n",
       "9       5.054953      0.531393         0.164447        0.035959   \n",
       "\n",
       "  param_class_weight param_n_estimators  \\\n",
       "0   {0: 5.0, 1: 1.0}                460   \n",
       "1   {0: 5.0, 1: 1.0}                409   \n",
       "2   {0: 5.0, 1: 1.0}                381   \n",
       "3   {0: 5.0, 1: 1.0}                282   \n",
       "4   {0: 5.0, 1: 1.0}                145   \n",
       "5           balanced                388   \n",
       "6           balanced                319   \n",
       "7   {0: 5.0, 1: 1.0}                 80   \n",
       "8           balanced                112   \n",
       "9           balanced                 66   \n",
       "\n",
       "                                              params  split0_test_precision  \\\n",
       "0  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.955298   \n",
       "1  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.956819   \n",
       "2  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.957545   \n",
       "3  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.959588   \n",
       "4  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.964374   \n",
       "5  {'class_weight': 'balanced', 'n_estimators': 388}               0.960516   \n",
       "6  {'class_weight': 'balanced', 'n_estimators': 319}               0.961649   \n",
       "7  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.967516   \n",
       "8  {'class_weight': 'balanced', 'n_estimators': 112}               0.973376   \n",
       "9   {'class_weight': 'balanced', 'n_estimators': 66}               0.976176   \n",
       "\n",
       "   split1_test_precision  split2_test_precision  mean_test_precision  \\\n",
       "0               0.958085               0.959033             0.957472   \n",
       "1               0.958925               0.959157             0.958300   \n",
       "2               0.959753               0.959459             0.958919   \n",
       "3               0.961669               0.963463             0.961573   \n",
       "4               0.966647               0.966997             0.966006   \n",
       "5               0.964416               0.965169             0.963367   \n",
       "6               0.966105               0.966520             0.964758   \n",
       "7               0.972731               0.971006             0.970418   \n",
       "8               0.975896               0.973854             0.974375   \n",
       "9               0.978814               0.978351             0.977780   \n",
       "\n",
       "   std_test_precision  rank_test_precision  split0_test_recall  \\\n",
       "0            0.001585                   10            0.956882   \n",
       "1            0.001052                    9            0.955408   \n",
       "2            0.000979                    8            0.951723   \n",
       "3            0.001583                    7            0.945089   \n",
       "4            0.001163                    4            0.932744   \n",
       "5            0.002039                    6            0.932375   \n",
       "6            0.002205                    5            0.928690   \n",
       "7            0.002169                    3            0.916528   \n",
       "8            0.001093                    2            0.889257   \n",
       "9            0.001150                    1            0.868251   \n",
       "\n",
       "   split1_test_recall  split2_test_recall  mean_test_recall  std_test_recall  \\\n",
       "0            0.947844            0.957796          0.954174         0.004492   \n",
       "1            0.946554            0.956506          0.952822         0.004455   \n",
       "2            0.944895            0.955216          0.950611         0.004286   \n",
       "3            0.938629            0.947659          0.943793         0.003799   \n",
       "4            0.924069            0.934206          0.930340         0.004474   \n",
       "5            0.924069            0.934574          0.930340         0.004524   \n",
       "6            0.919278            0.931073          0.926347         0.005092   \n",
       "7            0.900663            0.919646          0.912279         0.008312   \n",
       "8            0.873019            0.892370          0.884882         0.008484   \n",
       "9            0.851456            0.874493          0.864733         0.009728   \n",
       "\n",
       "   rank_test_recall  split0_test_f1  split1_test_f1  split2_test_f1  \\\n",
       "0                 1        0.956089        0.952937        0.958414   \n",
       "1                 2        0.956113        0.952699        0.957830   \n",
       "2                 3        0.954625        0.952266        0.957333   \n",
       "3                 4        0.952284        0.950009        0.955496   \n",
       "4                 6        0.948295        0.944879        0.950319   \n",
       "5                 5        0.946237        0.943812        0.949625   \n",
       "6                 7        0.944882        0.942110        0.948465   \n",
       "7                 8        0.941332        0.935311        0.944628   \n",
       "8                 9        0.929417        0.921595        0.931333   \n",
       "9                10        0.919056        0.910704        0.923511   \n",
       "\n",
       "   mean_test_f1  std_test_f1  rank_test_f1  \n",
       "0      0.955813     0.002245             1  \n",
       "1      0.955547     0.002132             2  \n",
       "2      0.954741     0.002070             3  \n",
       "3      0.952596     0.002251             4  \n",
       "4      0.947831     0.002245             5  \n",
       "5      0.946558     0.002384             6  \n",
       "6      0.945152     0.002602             7  \n",
       "7      0.940424     0.003858             8  \n",
       "8      0.927449     0.004212             9  \n",
       "9      0.917757     0.005309            10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>13.408356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.773538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.953348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.03167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_class_weight</th>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_n_estimators</th>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.955298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.958085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.959033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.957472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.001585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.956882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.947844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.957796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.954174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.004492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.956089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.952937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.958414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.955813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.002245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       0\n",
       "mean_fit_time                                                  13.408356\n",
       "std_fit_time                                                    0.773538\n",
       "mean_score_time                                                 0.953348\n",
       "std_score_time                                                   0.03167\n",
       "param_class_weight                                      {0: 5.0, 1: 1.0}\n",
       "param_n_estimators                                                   460\n",
       "params                 {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...\n",
       "split0_test_precision                                           0.955298\n",
       "split1_test_precision                                           0.958085\n",
       "split2_test_precision                                           0.959033\n",
       "mean_test_precision                                             0.957472\n",
       "std_test_precision                                              0.001585\n",
       "rank_test_precision                                                   10\n",
       "split0_test_recall                                              0.956882\n",
       "split1_test_recall                                              0.947844\n",
       "split2_test_recall                                              0.957796\n",
       "mean_test_recall                                                0.954174\n",
       "std_test_recall                                                 0.004492\n",
       "rank_test_recall                                                       1\n",
       "split0_test_f1                                                  0.956089\n",
       "split1_test_f1                                                  0.952937\n",
       "split2_test_f1                                                  0.958414\n",
       "mean_test_f1                                                    0.955813\n",
       "std_test_f1                                                     0.002245\n",
       "rank_test_f1                                                           1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lgbm_cvdf = pd.DataFrame(lgb_cv.cv_results_).sort_values(by='rank_test_f1').reset_index(drop=True)\n",
    "lgbm_best = pd.DataFrame(lgbm_cvdf.iloc[0,:])\n",
    "display(lgbm_cvdf)\n",
    "display(lgbm_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2af18d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           0\n",
      "params               {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 460}\n",
      "mean_test_precision                                                 0.957472\n",
      "mean_test_recall                                                    0.954174\n",
      "mean_test_f1                                                        0.955813\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(lgbm_best.loc[['params', 'mean_test_precision','mean_test_recall','mean_test_f1'],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9a7a2b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(class_weight='balanced', max_iter=5000)\n",
    "\n",
    "svc.fit(X_tr_vec, y_tr)\n",
    "\n",
    "y_pr = svc.predict(X_te_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "71b862b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.56      0.50       515\n",
      "           1       0.96      0.94      0.95      5426\n",
      "\n",
      "    accuracy                           0.90      5941\n",
      "   macro avg       0.70      0.75      0.72      5941\n",
      "weighted avg       0.91      0.90      0.91      5941\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(classification_report(y_te, y_pr))\n",
    "sns.heatmap(confusion_matrix(y_te, y_pr), cmap='Blues', cbar=True, annot=True, xticklabels= True, yticklabels=True, fmt = 'd')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3a7a733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 45 candidates, totalling 135 fits\n",
      "[CV 1/3] END C=0.1, class_weight=balanced, max_iter=1000; f1: (test=0.916) precision: (test=0.973) recall: (test=0.866) total time=   2.4s\n",
      "[CV 2/3] END C=0.1, class_weight=balanced, max_iter=1000; f1: (test=0.901) precision: (test=0.977) recall: (test=0.837) total time=   1.7s\n",
      "[CV 3/3] END C=0.1, class_weight=balanced, max_iter=1000; f1: (test=0.921) precision: (test=0.975) recall: (test=0.873) total time=   1.6s\n",
      "[CV 1/3] END C=0.1, class_weight=balanced, max_iter=2500; f1: (test=0.916) precision: (test=0.973) recall: (test=0.866) total time=   1.4s\n",
      "[CV 2/3] END C=0.1, class_weight=balanced, max_iter=2500; f1: (test=0.901) precision: (test=0.977) recall: (test=0.837) total time=   1.5s\n",
      "[CV 3/3] END C=0.1, class_weight=balanced, max_iter=2500; f1: (test=0.921) precision: (test=0.975) recall: (test=0.873) total time=   1.5s\n",
      "[CV 1/3] END C=0.1, class_weight=balanced, max_iter=5000; f1: (test=0.916) precision: (test=0.973) recall: (test=0.866) total time=   1.2s\n",
      "[CV 2/3] END C=0.1, class_weight=balanced, max_iter=5000; f1: (test=0.901) precision: (test=0.977) recall: (test=0.837) total time=   1.2s\n",
      "[CV 3/3] END C=0.1, class_weight=balanced, max_iter=5000; f1: (test=0.921) precision: (test=0.975) recall: (test=0.873) total time=   0.8s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.1s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.964) precision: (test=0.932) recall: (test=0.998) total time=   0.1s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.964) precision: (test=0.933) recall: (test=0.998) total time=   0.1s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.1s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.964) precision: (test=0.932) recall: (test=0.998) total time=   0.1s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.964) precision: (test=0.933) recall: (test=0.998) total time=   0.1s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.1s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.964) precision: (test=0.932) recall: (test=0.998) total time=   0.1s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.964) precision: (test=0.933) recall: (test=0.998) total time=   0.1s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.955) precision: (test=0.953) recall: (test=0.956) total time=   0.6s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.955) precision: (test=0.958) recall: (test=0.952) total time=   1.1s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.956) precision: (test=0.957) recall: (test=0.956) total time=   1.7s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.955) precision: (test=0.953) recall: (test=0.956) total time=   0.9s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.955) precision: (test=0.958) recall: (test=0.952) total time=   1.0s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.956) precision: (test=0.957) recall: (test=0.956) total time=   0.9s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.955) precision: (test=0.953) recall: (test=0.956) total time=   0.8s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.955) precision: (test=0.958) recall: (test=0.952) total time=   1.9s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.956) precision: (test=0.957) recall: (test=0.956) total time=   1.2s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.1s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.964) precision: (test=0.932) recall: (test=0.998) total time=   0.1s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.964) precision: (test=0.933) recall: (test=0.998) total time=   0.1s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.1s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.964) precision: (test=0.932) recall: (test=0.998) total time=   0.1s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.964) precision: (test=0.933) recall: (test=0.998) total time=   0.1s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.1s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.964) precision: (test=0.932) recall: (test=0.998) total time=   0.1s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.964) precision: (test=0.933) recall: (test=0.998) total time=   0.1s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.939) precision: (test=0.963) recall: (test=0.917) total time=   0.9s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.927) precision: (test=0.968) recall: (test=0.889) total time=   1.3s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.940) precision: (test=0.967) recall: (test=0.915) total time=   1.9s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.939) precision: (test=0.963) recall: (test=0.917) total time=   1.3s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.927) precision: (test=0.968) recall: (test=0.889) total time=   1.8s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.940) precision: (test=0.967) recall: (test=0.915) total time=   2.4s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.939) precision: (test=0.963) recall: (test=0.917) total time=   2.1s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.927) precision: (test=0.968) recall: (test=0.889) total time=   3.0s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.940) precision: (test=0.967) recall: (test=0.915) total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=1, class_weight=balanced, max_iter=1000; f1: (test=0.952) precision: (test=0.949) recall: (test=0.955) total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, class_weight=balanced, max_iter=1000; f1: (test=0.952) precision: (test=0.951) recall: (test=0.953) total time=   4.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=1, class_weight=balanced, max_iter=1000; f1: (test=0.951) precision: (test=0.952) recall: (test=0.950) total time=   3.9s\n",
      "[CV 1/3] END C=1, class_weight=balanced, max_iter=2500; f1: (test=0.952) precision: (test=0.949) recall: (test=0.955) total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, class_weight=balanced, max_iter=2500; f1: (test=0.952) precision: (test=0.951) recall: (test=0.953) total time=  10.2s\n",
      "[CV 3/3] END C=1, class_weight=balanced, max_iter=2500; f1: (test=0.951) precision: (test=0.952) recall: (test=0.950) total time=   5.7s\n",
      "[CV 1/3] END C=1, class_weight=balanced, max_iter=5000; f1: (test=0.952) precision: (test=0.949) recall: (test=0.955) total time=   3.1s\n",
      "[CV 2/3] END C=1, class_weight=balanced, max_iter=5000; f1: (test=0.952) precision: (test=0.951) recall: (test=0.953) total time=  10.4s\n",
      "[CV 3/3] END C=1, class_weight=balanced, max_iter=5000; f1: (test=0.951) precision: (test=0.952) recall: (test=0.950) total time=  11.9s\n",
      "[CV 1/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   2.7s\n",
      "[CV 2/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.966) precision: (test=0.942) recall: (test=0.992) total time=   2.7s\n",
      "[CV 3/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   2.4s\n",
      "[CV 1/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   1.1s\n",
      "[CV 2/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.966) precision: (test=0.942) recall: (test=0.992) total time=   1.6s\n",
      "[CV 3/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   1.8s\n",
      "[CV 1/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   1.3s\n",
      "[CV 2/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.966) precision: (test=0.942) recall: (test=0.992) total time=   1.2s\n",
      "[CV 3/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   1.7s\n",
      "[CV 1/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.957) precision: (test=0.946) recall: (test=0.968) total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.958) precision: (test=0.948) recall: (test=0.967) total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.956) precision: (test=0.948) recall: (test=0.965) total time=   3.5s\n",
      "[CV 1/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.957) precision: (test=0.946) recall: (test=0.968) total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.958) precision: (test=0.948) recall: (test=0.968) total time=   9.1s\n",
      "[CV 3/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.956) precision: (test=0.948) recall: (test=0.965) total time=   7.9s\n",
      "[CV 1/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.957) precision: (test=0.946) recall: (test=0.968) total time=   4.1s\n",
      "[CV 2/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.958) precision: (test=0.948) recall: (test=0.968) total time=  10.1s\n",
      "[CV 3/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.956) precision: (test=0.948) recall: (test=0.965) total time=   6.9s\n",
      "[CV 1/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   0.8s\n",
      "[CV 2/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.966) precision: (test=0.942) recall: (test=0.992) total time=   1.5s\n",
      "[CV 3/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   1.5s\n",
      "[CV 1/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   1.2s\n",
      "[CV 2/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.966) precision: (test=0.942) recall: (test=0.992) total time=   1.8s\n",
      "[CV 3/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   1.5s\n",
      "[CV 1/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   1.0s\n",
      "[CV 2/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.966) precision: (test=0.942) recall: (test=0.992) total time=   1.2s\n",
      "[CV 3/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.953) precision: (test=0.948) recall: (test=0.958) total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.952) precision: (test=0.948) recall: (test=0.956) total time=   2.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.951) precision: (test=0.949) recall: (test=0.953) total time=   2.6s\n",
      "[CV 1/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.953) precision: (test=0.948) recall: (test=0.958) total time=   4.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.952) precision: (test=0.948) recall: (test=0.957) total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.951) precision: (test=0.949) recall: (test=0.953) total time=   6.2s\n",
      "[CV 1/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.953) precision: (test=0.948) recall: (test=0.958) total time=   5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.952) precision: (test=0.948) recall: (test=0.956) total time=  19.5s\n",
      "[CV 3/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.951) precision: (test=0.949) recall: (test=0.953) total time=  13.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight=balanced, max_iter=1000; f1: (test=0.949) precision: (test=0.946) recall: (test=0.953) total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight=balanced, max_iter=1000; f1: (test=0.952) precision: (test=0.947) recall: (test=0.957) total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight=balanced, max_iter=1000; f1: (test=0.952) precision: (test=0.949) recall: (test=0.956) total time=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight=balanced, max_iter=2500; f1: (test=0.948) precision: (test=0.946) recall: (test=0.950) total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight=balanced, max_iter=2500; f1: (test=0.951) precision: (test=0.946) recall: (test=0.957) total time=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight=balanced, max_iter=2500; f1: (test=0.952) precision: (test=0.949) recall: (test=0.955) total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight=balanced, max_iter=5000; f1: (test=0.948) precision: (test=0.946) recall: (test=0.951) total time=   9.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight=balanced, max_iter=5000; f1: (test=0.951) precision: (test=0.947) recall: (test=0.956) total time=  13.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight=balanced, max_iter=5000; f1: (test=0.952) precision: (test=0.949) recall: (test=0.955) total time=  14.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.962) precision: (test=0.942) recall: (test=0.983) total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.962) precision: (test=0.944) recall: (test=0.981) total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.961) precision: (test=0.944) recall: (test=0.979) total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.962) precision: (test=0.942) recall: (test=0.983) total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.962) precision: (test=0.944) recall: (test=0.981) total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.962) precision: (test=0.945) recall: (test=0.979) total time=   5.7s\n",
      "[CV 1/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.962) precision: (test=0.942) recall: (test=0.983) total time=   6.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.962) precision: (test=0.944) recall: (test=0.981) total time=  10.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.962) precision: (test=0.945) recall: (test=0.979) total time=   9.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.950) precision: (test=0.944) recall: (test=0.957) total time=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.955) precision: (test=0.947) recall: (test=0.963) total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.954) precision: (test=0.948) recall: (test=0.961) total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.949) precision: (test=0.944) recall: (test=0.954) total time=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.955) precision: (test=0.947) recall: (test=0.962) total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.954) precision: (test=0.948) recall: (test=0.960) total time=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.949) precision: (test=0.944) recall: (test=0.954) total time=  14.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.954) precision: (test=0.947) recall: (test=0.962) total time=  24.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.954) precision: (test=0.948) recall: (test=0.960) total time=  11.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.962) precision: (test=0.942) recall: (test=0.983) total time=   1.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.962) precision: (test=0.944) recall: (test=0.981) total time=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.961) precision: (test=0.944) recall: (test=0.979) total time=   2.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.962) precision: (test=0.942) recall: (test=0.983) total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.962) precision: (test=0.944) recall: (test=0.981) total time=   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.962) precision: (test=0.945) recall: (test=0.979) total time=   5.6s\n",
      "[CV 1/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.962) precision: (test=0.942) recall: (test=0.983) total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.962) precision: (test=0.944) recall: (test=0.981) total time=   9.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.962) precision: (test=0.945) recall: (test=0.979) total time=  17.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.947) precision: (test=0.945) recall: (test=0.948) total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.952) precision: (test=0.947) recall: (test=0.958) total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.951) precision: (test=0.949) recall: (test=0.953) total time=   2.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.944) precision: (test=0.945) recall: (test=0.944) total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.951) precision: (test=0.946) recall: (test=0.956) total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.951) precision: (test=0.949) recall: (test=0.952) total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.944) precision: (test=0.945) recall: (test=0.943) total time=   8.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.950) precision: (test=0.945) recall: (test=0.955) total time=   8.4s\n",
      "[CV 3/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.951) precision: (test=0.950) recall: (test=0.951) total time=  11.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=LinearSVC(dual=&#x27;auto&#x27;),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10.0],\n",
       "                         &#x27;class_weight&#x27;: [&#x27;balanced&#x27;, {0: 1.0, 1: 1.0},\n",
       "                                          {0: 5.0, 1: 1.0}, {0: 1.0, 1: 1.0},\n",
       "                                          {0: 10.0, 1: 1.0}],\n",
       "                         &#x27;max_iter&#x27;: [1000, 2500, 5000]},\n",
       "             refit=False, scoring=[&#x27;precision&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;], verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=LinearSVC(dual=&#x27;auto&#x27;),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10.0],\n",
       "                         &#x27;class_weight&#x27;: [&#x27;balanced&#x27;, {0: 1.0, 1: 1.0},\n",
       "                                          {0: 5.0, 1: 1.0}, {0: 1.0, 1: 1.0},\n",
       "                                          {0: 10.0, 1: 1.0}],\n",
       "                         &#x27;max_iter&#x27;: [1000, 2500, 5000]},\n",
       "             refit=False, scoring=[&#x27;precision&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;], verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(dual=&#x27;auto&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(dual=&#x27;auto&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LinearSVC(dual='auto'),\n",
       "             param_grid={'C': [0.1, 1, 10.0],\n",
       "                         'class_weight': ['balanced', {0: 1.0, 1: 1.0},\n",
       "                                          {0: 5.0, 1: 1.0}, {0: 1.0, 1: 1.0},\n",
       "                                          {0: 10.0, 1: 1.0}],\n",
       "                         'max_iter': [1000, 2500, 5000]},\n",
       "             refit=False, scoring=['precision', 'recall', 'f1'], verbose=3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = LinearSVC(dual='auto')\n",
    "\n",
    "svc_cv = GridSearchCV(svc, {'class_weight':['balanced', {0: 1.0, 1: 1.0}, {0: 5.0, 1: 1.0}, {0: 1.0, 1: 1.0}, {0: 10.0, 1: 1.0}],\n",
    "                           'max_iter': [1000,2500,5000],\n",
    "                           'C':[0.1, 1, 10.0]},\n",
    "                           cv = 3, verbose=3, \\\n",
    "                            scoring = ['precision','recall','f1'], refit=False)\n",
    "\n",
    "svc_cv.fit(X_tr_vec, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e767173b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_precision</th>\n",
       "      <th>split1_test_precision</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_recall</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>rank_test_recall</th>\n",
       "      <th>split0_test_f1</th>\n",
       "      <th>split1_test_f1</th>\n",
       "      <th>split2_test_f1</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.649263</td>\n",
       "      <td>0.144584</td>\n",
       "      <td>0.027297</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.941043</td>\n",
       "      <td>0.941557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990601</td>\n",
       "      <td>0.991154</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>7</td>\n",
       "      <td>0.965449</td>\n",
       "      <td>0.965982</td>\n",
       "      <td>0.964904</td>\n",
       "      <td>0.965445</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.335717</td>\n",
       "      <td>0.287607</td>\n",
       "      <td>0.025812</td>\n",
       "      <td>0.015654</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.941043</td>\n",
       "      <td>0.941557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990601</td>\n",
       "      <td>0.991154</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>7</td>\n",
       "      <td>0.965449</td>\n",
       "      <td>0.965982</td>\n",
       "      <td>0.964904</td>\n",
       "      <td>0.965445</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.592869</td>\n",
       "      <td>0.239610</td>\n",
       "      <td>0.025776</td>\n",
       "      <td>0.007437</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.941043</td>\n",
       "      <td>0.941557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990601</td>\n",
       "      <td>0.991154</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>7</td>\n",
       "      <td>0.965449</td>\n",
       "      <td>0.965982</td>\n",
       "      <td>0.964904</td>\n",
       "      <td>0.965445</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.477117</td>\n",
       "      <td>0.176543</td>\n",
       "      <td>0.027305</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.941043</td>\n",
       "      <td>0.941557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990601</td>\n",
       "      <td>0.991154</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>7</td>\n",
       "      <td>0.965449</td>\n",
       "      <td>0.965982</td>\n",
       "      <td>0.964904</td>\n",
       "      <td>0.965445</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.403068</td>\n",
       "      <td>0.331300</td>\n",
       "      <td>0.027555</td>\n",
       "      <td>0.004213</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.941043</td>\n",
       "      <td>0.941557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990601</td>\n",
       "      <td>0.991154</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>7</td>\n",
       "      <td>0.965449</td>\n",
       "      <td>0.965982</td>\n",
       "      <td>0.964904</td>\n",
       "      <td>0.965445</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.585148</td>\n",
       "      <td>0.289189</td>\n",
       "      <td>0.024945</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.941043</td>\n",
       "      <td>0.941557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990601</td>\n",
       "      <td>0.991154</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>7</td>\n",
       "      <td>0.965449</td>\n",
       "      <td>0.965982</td>\n",
       "      <td>0.964904</td>\n",
       "      <td>0.965445</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.178408</td>\n",
       "      <td>0.010019</td>\n",
       "      <td>0.017858</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.935211</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997788</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965314</td>\n",
       "      <td>0.964222</td>\n",
       "      <td>0.964461</td>\n",
       "      <td>0.964666</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.189866</td>\n",
       "      <td>0.011016</td>\n",
       "      <td>0.018092</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.935211</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997788</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965314</td>\n",
       "      <td>0.964222</td>\n",
       "      <td>0.964461</td>\n",
       "      <td>0.964666</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.175018</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.935211</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997788</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965314</td>\n",
       "      <td>0.964222</td>\n",
       "      <td>0.964461</td>\n",
       "      <td>0.964666</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.192559</td>\n",
       "      <td>0.016034</td>\n",
       "      <td>0.023056</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.935211</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997788</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965314</td>\n",
       "      <td>0.964222</td>\n",
       "      <td>0.964461</td>\n",
       "      <td>0.964666</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.174609</td>\n",
       "      <td>0.009646</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.935211</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997788</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965314</td>\n",
       "      <td>0.964222</td>\n",
       "      <td>0.964461</td>\n",
       "      <td>0.964666</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.203134</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.019834</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.935211</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997788</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965314</td>\n",
       "      <td>0.964222</td>\n",
       "      <td>0.964461</td>\n",
       "      <td>0.964666</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.435739</td>\n",
       "      <td>0.425723</td>\n",
       "      <td>0.031630</td>\n",
       "      <td>0.013650</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.943814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979174</td>\n",
       "      <td>0.981141</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>14</td>\n",
       "      <td>0.961774</td>\n",
       "      <td>0.962233</td>\n",
       "      <td>0.961542</td>\n",
       "      <td>0.961850</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.457349</td>\n",
       "      <td>0.253198</td>\n",
       "      <td>0.017482</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.943814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979174</td>\n",
       "      <td>0.981141</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>14</td>\n",
       "      <td>0.961774</td>\n",
       "      <td>0.962233</td>\n",
       "      <td>0.961542</td>\n",
       "      <td>0.961850</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11.245902</td>\n",
       "      <td>4.986192</td>\n",
       "      <td>0.026040</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.943814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979174</td>\n",
       "      <td>0.981141</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>14</td>\n",
       "      <td>0.961774</td>\n",
       "      <td>0.962233</td>\n",
       "      <td>0.961542</td>\n",
       "      <td>0.961850</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8.944186</td>\n",
       "      <td>1.418137</td>\n",
       "      <td>0.026307</td>\n",
       "      <td>0.010862</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.943647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979174</td>\n",
       "      <td>0.981141</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>14</td>\n",
       "      <td>0.961774</td>\n",
       "      <td>0.962147</td>\n",
       "      <td>0.961542</td>\n",
       "      <td>0.961821</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.883238</td>\n",
       "      <td>0.012786</td>\n",
       "      <td>0.011210</td>\n",
       "      <td>0.007227</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.943647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979359</td>\n",
       "      <td>0.981203</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>13</td>\n",
       "      <td>0.961774</td>\n",
       "      <td>0.962147</td>\n",
       "      <td>0.961462</td>\n",
       "      <td>0.961794</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.086944</td>\n",
       "      <td>0.114233</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.943814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978990</td>\n",
       "      <td>0.981080</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>18</td>\n",
       "      <td>0.961774</td>\n",
       "      <td>0.962233</td>\n",
       "      <td>0.961361</td>\n",
       "      <td>0.961790</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.083195</td>\n",
       "      <td>2.422923</td>\n",
       "      <td>0.029459</td>\n",
       "      <td>0.014442</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.945664</td>\n",
       "      <td>0.948167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965352</td>\n",
       "      <td>0.967135</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>19</td>\n",
       "      <td>0.956941</td>\n",
       "      <td>0.957767</td>\n",
       "      <td>0.956450</td>\n",
       "      <td>0.957053</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.627306</td>\n",
       "      <td>2.773763</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.945664</td>\n",
       "      <td>0.948167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965352</td>\n",
       "      <td>0.967135</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>19</td>\n",
       "      <td>0.956941</td>\n",
       "      <td>0.957767</td>\n",
       "      <td>0.956450</td>\n",
       "      <td>0.957053</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.501866</td>\n",
       "      <td>0.414782</td>\n",
       "      <td>0.034463</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.945664</td>\n",
       "      <td>0.948158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965352</td>\n",
       "      <td>0.967074</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>21</td>\n",
       "      <td>0.956941</td>\n",
       "      <td>0.957672</td>\n",
       "      <td>0.956450</td>\n",
       "      <td>0.957021</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.234542</td>\n",
       "      <td>0.458075</td>\n",
       "      <td>0.028797</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.953493</td>\n",
       "      <td>0.958418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955953</td>\n",
       "      <td>0.954420</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>29</td>\n",
       "      <td>0.954633</td>\n",
       "      <td>0.954962</td>\n",
       "      <td>0.956393</td>\n",
       "      <td>0.955329</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.988320</td>\n",
       "      <td>0.051435</td>\n",
       "      <td>0.033565</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.953493</td>\n",
       "      <td>0.958418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955953</td>\n",
       "      <td>0.954420</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>29</td>\n",
       "      <td>0.954633</td>\n",
       "      <td>0.954962</td>\n",
       "      <td>0.956393</td>\n",
       "      <td>0.955329</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.371338</td>\n",
       "      <td>0.432298</td>\n",
       "      <td>0.026989</td>\n",
       "      <td>0.007369</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.953493</td>\n",
       "      <td>0.958418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955953</td>\n",
       "      <td>0.954420</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>29</td>\n",
       "      <td>0.954633</td>\n",
       "      <td>0.954962</td>\n",
       "      <td>0.956393</td>\n",
       "      <td>0.955329</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.896143</td>\n",
       "      <td>0.443305</td>\n",
       "      <td>0.021384</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.944172</td>\n",
       "      <td>0.947244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960560</td>\n",
       "      <td>0.960071</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>22</td>\n",
       "      <td>0.950394</td>\n",
       "      <td>0.955036</td>\n",
       "      <td>0.954229</td>\n",
       "      <td>0.953219</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.079093</td>\n",
       "      <td>0.765587</td>\n",
       "      <td>0.022514</td>\n",
       "      <td>0.014438</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.944384</td>\n",
       "      <td>0.947206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960192</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.003358</td>\n",
       "      <td>23</td>\n",
       "      <td>0.949317</td>\n",
       "      <td>0.954654</td>\n",
       "      <td>0.954038</td>\n",
       "      <td>0.952669</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16.953667</td>\n",
       "      <td>5.515041</td>\n",
       "      <td>0.037513</td>\n",
       "      <td>0.012595</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.944222</td>\n",
       "      <td>0.946843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960007</td>\n",
       "      <td>0.958782</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>24</td>\n",
       "      <td>0.949326</td>\n",
       "      <td>0.954288</td>\n",
       "      <td>0.953942</td>\n",
       "      <td>0.952519</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.036972</td>\n",
       "      <td>0.927205</td>\n",
       "      <td>0.023312</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.947867</td>\n",
       "      <td>0.948118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.952820</td>\n",
       "      <td>0.955833</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>25</td>\n",
       "      <td>0.952992</td>\n",
       "      <td>0.952294</td>\n",
       "      <td>0.951067</td>\n",
       "      <td>0.952117</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13.001462</td>\n",
       "      <td>5.754233</td>\n",
       "      <td>0.040797</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.947867</td>\n",
       "      <td>0.948099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.952820</td>\n",
       "      <td>0.955710</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>26</td>\n",
       "      <td>0.952992</td>\n",
       "      <td>0.952101</td>\n",
       "      <td>0.951067</td>\n",
       "      <td>0.952053</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.529574</td>\n",
       "      <td>0.165996</td>\n",
       "      <td>0.021928</td>\n",
       "      <td>0.004746</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.947858</td>\n",
       "      <td>0.948109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.952635</td>\n",
       "      <td>0.955648</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>27</td>\n",
       "      <td>0.952896</td>\n",
       "      <td>0.952197</td>\n",
       "      <td>0.950970</td>\n",
       "      <td>0.952021</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6.467467</td>\n",
       "      <td>2.893459</td>\n",
       "      <td>0.018796</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 1, 'class_weight': 'balanced', 'max_iter': 2500}</td>\n",
       "      <td>0.949432</td>\n",
       "      <td>0.951085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950055</td>\n",
       "      <td>0.952700</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>35</td>\n",
       "      <td>0.952136</td>\n",
       "      <td>0.952135</td>\n",
       "      <td>0.951019</td>\n",
       "      <td>0.951764</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8.526383</td>\n",
       "      <td>3.837907</td>\n",
       "      <td>0.043070</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 1, 'class_weight': 'balanced', 'max_iter': 5000}</td>\n",
       "      <td>0.949432</td>\n",
       "      <td>0.951085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950055</td>\n",
       "      <td>0.952700</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>35</td>\n",
       "      <td>0.952136</td>\n",
       "      <td>0.952135</td>\n",
       "      <td>0.951019</td>\n",
       "      <td>0.951764</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.372046</td>\n",
       "      <td>0.487149</td>\n",
       "      <td>0.036348</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 1, 'class_weight': 'balanced', 'max_iter': 1000}</td>\n",
       "      <td>0.949432</td>\n",
       "      <td>0.951085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.949871</td>\n",
       "      <td>0.952638</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>37</td>\n",
       "      <td>0.952136</td>\n",
       "      <td>0.952135</td>\n",
       "      <td>0.950923</td>\n",
       "      <td>0.951731</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.812723</td>\n",
       "      <td>0.476115</td>\n",
       "      <td>0.025161</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': 'balanced', 'max_iter': 1000}</td>\n",
       "      <td>0.945531</td>\n",
       "      <td>0.947119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955769</td>\n",
       "      <td>0.955403</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>28</td>\n",
       "      <td>0.949349</td>\n",
       "      <td>0.952154</td>\n",
       "      <td>0.952433</td>\n",
       "      <td>0.951312</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>12.553261</td>\n",
       "      <td>2.280884</td>\n",
       "      <td>0.019783</td>\n",
       "      <td>0.020131</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': 'balanced', 'max_iter': 5000}</td>\n",
       "      <td>0.945738</td>\n",
       "      <td>0.946543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955031</td>\n",
       "      <td>0.953929</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>32</td>\n",
       "      <td>0.948171</td>\n",
       "      <td>0.951316</td>\n",
       "      <td>0.952136</td>\n",
       "      <td>0.950541</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.746870</td>\n",
       "      <td>0.197350</td>\n",
       "      <td>0.024984</td>\n",
       "      <td>0.008743</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 10.0, 'class_weight': 'balanced', 'max_iter': 2500}</td>\n",
       "      <td>0.945555</td>\n",
       "      <td>0.946217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954663</td>\n",
       "      <td>0.953867</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>33</td>\n",
       "      <td>0.947988</td>\n",
       "      <td>0.951334</td>\n",
       "      <td>0.951681</td>\n",
       "      <td>0.950334</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2.991368</td>\n",
       "      <td>0.498975</td>\n",
       "      <td>0.028655</td>\n",
       "      <td>0.012355</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.945414</td>\n",
       "      <td>0.947138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953373</td>\n",
       "      <td>0.952946</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>34</td>\n",
       "      <td>0.946632</td>\n",
       "      <td>0.952346</td>\n",
       "      <td>0.951356</td>\n",
       "      <td>0.950112</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5.062548</td>\n",
       "      <td>0.608579</td>\n",
       "      <td>0.026913</td>\n",
       "      <td>0.008940</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.944854</td>\n",
       "      <td>0.946005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.952267</td>\n",
       "      <td>0.950673</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>38</td>\n",
       "      <td>0.944419</td>\n",
       "      <td>0.950862</td>\n",
       "      <td>0.950777</td>\n",
       "      <td>0.948686</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>9.597168</td>\n",
       "      <td>1.279823</td>\n",
       "      <td>0.014749</td>\n",
       "      <td>0.010238</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.945142</td>\n",
       "      <td>0.945285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951345</td>\n",
       "      <td>0.949813</td>\n",
       "      <td>0.005152</td>\n",
       "      <td>39</td>\n",
       "      <td>0.944009</td>\n",
       "      <td>0.950225</td>\n",
       "      <td>0.950645</td>\n",
       "      <td>0.948293</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.929830</td>\n",
       "      <td>0.449947</td>\n",
       "      <td>0.029018</td>\n",
       "      <td>0.016920</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.962841</td>\n",
       "      <td>0.967904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914854</td>\n",
       "      <td>0.906935</td>\n",
       "      <td>0.012537</td>\n",
       "      <td>40</td>\n",
       "      <td>0.939211</td>\n",
       "      <td>0.926904</td>\n",
       "      <td>0.939973</td>\n",
       "      <td>0.935363</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.445045</td>\n",
       "      <td>0.399272</td>\n",
       "      <td>0.023070</td>\n",
       "      <td>0.004549</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.962841</td>\n",
       "      <td>0.967904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914854</td>\n",
       "      <td>0.906935</td>\n",
       "      <td>0.012537</td>\n",
       "      <td>40</td>\n",
       "      <td>0.939211</td>\n",
       "      <td>0.926904</td>\n",
       "      <td>0.939973</td>\n",
       "      <td>0.935363</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2.762519</td>\n",
       "      <td>0.437830</td>\n",
       "      <td>0.054862</td>\n",
       "      <td>0.019328</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.962841</td>\n",
       "      <td>0.967904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914854</td>\n",
       "      <td>0.906935</td>\n",
       "      <td>0.012537</td>\n",
       "      <td>40</td>\n",
       "      <td>0.939211</td>\n",
       "      <td>0.926904</td>\n",
       "      <td>0.939973</td>\n",
       "      <td>0.935363</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.139109</td>\n",
       "      <td>0.154177</td>\n",
       "      <td>0.017208</td>\n",
       "      <td>0.012301</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': 'balanced', 'max_iter': 5000}</td>\n",
       "      <td>0.973281</td>\n",
       "      <td>0.976564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872650</td>\n",
       "      <td>0.858529</td>\n",
       "      <td>0.015418</td>\n",
       "      <td>43</td>\n",
       "      <td>0.916431</td>\n",
       "      <td>0.901459</td>\n",
       "      <td>0.920937</td>\n",
       "      <td>0.912942</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.554295</td>\n",
       "      <td>0.034467</td>\n",
       "      <td>0.032060</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 0.1, 'class_weight': 'balanced', 'max_iter': 2500}</td>\n",
       "      <td>0.973281</td>\n",
       "      <td>0.976564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872650</td>\n",
       "      <td>0.858529</td>\n",
       "      <td>0.015418</td>\n",
       "      <td>43</td>\n",
       "      <td>0.916431</td>\n",
       "      <td>0.901459</td>\n",
       "      <td>0.920937</td>\n",
       "      <td>0.912942</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.951452</td>\n",
       "      <td>0.316191</td>\n",
       "      <td>0.042286</td>\n",
       "      <td>0.019735</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': 'balanced', 'max_iter': 1000}</td>\n",
       "      <td>0.973281</td>\n",
       "      <td>0.976564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872650</td>\n",
       "      <td>0.858529</td>\n",
       "      <td>0.015418</td>\n",
       "      <td>43</td>\n",
       "      <td>0.916431</td>\n",
       "      <td>0.901459</td>\n",
       "      <td>0.920937</td>\n",
       "      <td>0.912942</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0        2.649263      0.144584         0.027297        0.006587       1   \n",
       "1        1.335717      0.287607         0.025812        0.015654       1   \n",
       "2        1.592869      0.239610         0.025776        0.007437       1   \n",
       "3        1.477117      0.176543         0.027305        0.009494       1   \n",
       "4        1.403068      0.331300         0.027555        0.004213       1   \n",
       "5        1.585148      0.289189         0.024945        0.005682       1   \n",
       "6        0.178408      0.010019         0.017858        0.000091     0.1   \n",
       "7        0.189866      0.011016         0.018092        0.002041     0.1   \n",
       "8        0.175018      0.011330         0.022321        0.001986     0.1   \n",
       "9        0.192559      0.016034         0.023056        0.001317     0.1   \n",
       "10       0.174609      0.009646         0.019300        0.002111     0.1   \n",
       "11       0.203134      0.019000         0.019834        0.001567     0.1   \n",
       "12       5.435739      0.425723         0.031630        0.013650    10.0   \n",
       "13       5.457349      0.253198         0.017482        0.001994    10.0   \n",
       "14      11.245902      4.986192         0.026040        0.004448    10.0   \n",
       "15       8.944186      1.418137         0.026307        0.010862    10.0   \n",
       "16       1.883238      0.012786         0.011210        0.007227    10.0   \n",
       "17       2.086944      0.114233         0.021110        0.004952    10.0   \n",
       "18       7.083195      2.422923         0.029459        0.014442       1   \n",
       "19       6.627306      2.773763         0.027115        0.001797       1   \n",
       "20       3.501866      0.414782         0.034463        0.002958       1   \n",
       "21       1.234542      0.458075         0.028797        0.002333     0.1   \n",
       "22       0.988320      0.051435         0.033565        0.002275     0.1   \n",
       "23       1.371338      0.432298         0.026989        0.007369     0.1   \n",
       "24       2.896143      0.443305         0.021384        0.004823    10.0   \n",
       "25       5.079093      0.765587         0.022514        0.014438    10.0   \n",
       "26      16.953667      5.515041         0.037513        0.012595    10.0   \n",
       "27       5.036972      0.927205         0.023312        0.004546       1   \n",
       "28      13.001462      5.754233         0.040797        0.010135       1   \n",
       "29       2.529574      0.165996         0.021928        0.004746       1   \n",
       "30       6.467467      2.893459         0.018796        0.003155       1   \n",
       "31       8.526383      3.837907         0.043070        0.001132       1   \n",
       "32       4.372046      0.487149         0.036348        0.012647       1   \n",
       "33       2.812723      0.476115         0.025161        0.002889    10.0   \n",
       "34      12.553261      2.280884         0.019783        0.020131    10.0   \n",
       "35       4.746870      0.197350         0.024984        0.008743    10.0   \n",
       "36       2.991368      0.498975         0.028655        0.012355    10.0   \n",
       "37       5.062548      0.608579         0.026913        0.008940    10.0   \n",
       "38       9.597168      1.279823         0.014749        0.010238    10.0   \n",
       "39       1.929830      0.449947         0.029018        0.016920     0.1   \n",
       "40       1.445045      0.399272         0.023070        0.004549     0.1   \n",
       "41       2.762519      0.437830         0.054862        0.019328     0.1   \n",
       "42       1.139109      0.154177         0.017208        0.012301     0.1   \n",
       "43       1.554295      0.034467         0.032060        0.003697     0.1   \n",
       "44       1.951452      0.316191         0.042286        0.019735     0.1   \n",
       "\n",
       "   param_class_weight param_max_iter  \\\n",
       "0    {0: 1.0, 1: 1.0}           1000   \n",
       "1    {0: 1.0, 1: 1.0}           1000   \n",
       "2    {0: 1.0, 1: 1.0}           2500   \n",
       "3    {0: 1.0, 1: 1.0}           5000   \n",
       "4    {0: 1.0, 1: 1.0}           5000   \n",
       "5    {0: 1.0, 1: 1.0}           2500   \n",
       "6    {0: 1.0, 1: 1.0}           2500   \n",
       "7    {0: 1.0, 1: 1.0}           1000   \n",
       "8    {0: 1.0, 1: 1.0}           2500   \n",
       "9    {0: 1.0, 1: 1.0}           5000   \n",
       "10   {0: 1.0, 1: 1.0}           5000   \n",
       "11   {0: 1.0, 1: 1.0}           1000   \n",
       "12   {0: 1.0, 1: 1.0}           2500   \n",
       "13   {0: 1.0, 1: 1.0}           2500   \n",
       "14   {0: 1.0, 1: 1.0}           5000   \n",
       "15   {0: 1.0, 1: 1.0}           5000   \n",
       "16   {0: 1.0, 1: 1.0}           1000   \n",
       "17   {0: 1.0, 1: 1.0}           1000   \n",
       "18   {0: 5.0, 1: 1.0}           5000   \n",
       "19   {0: 5.0, 1: 1.0}           2500   \n",
       "20   {0: 5.0, 1: 1.0}           1000   \n",
       "21   {0: 5.0, 1: 1.0}           1000   \n",
       "22   {0: 5.0, 1: 1.0}           2500   \n",
       "23   {0: 5.0, 1: 1.0}           5000   \n",
       "24   {0: 5.0, 1: 1.0}           1000   \n",
       "25   {0: 5.0, 1: 1.0}           2500   \n",
       "26   {0: 5.0, 1: 1.0}           5000   \n",
       "27  {0: 10.0, 1: 1.0}           2500   \n",
       "28  {0: 10.0, 1: 1.0}           5000   \n",
       "29  {0: 10.0, 1: 1.0}           1000   \n",
       "30           balanced           2500   \n",
       "31           balanced           5000   \n",
       "32           balanced           1000   \n",
       "33           balanced           1000   \n",
       "34           balanced           5000   \n",
       "35           balanced           2500   \n",
       "36  {0: 10.0, 1: 1.0}           1000   \n",
       "37  {0: 10.0, 1: 1.0}           2500   \n",
       "38  {0: 10.0, 1: 1.0}           5000   \n",
       "39  {0: 10.0, 1: 1.0}           2500   \n",
       "40  {0: 10.0, 1: 1.0}           1000   \n",
       "41  {0: 10.0, 1: 1.0}           5000   \n",
       "42           balanced           5000   \n",
       "43           balanced           2500   \n",
       "44           balanced           1000   \n",
       "\n",
       "                                                              params  \\\n",
       "0       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "1       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "2       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "3       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "4       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "5       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "6     {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "7     {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "8     {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "9     {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "10    {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "11    {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "12   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "13   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "14   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "15   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "16   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "17   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "18      {'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "19      {'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "20      {'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "21    {'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "22    {'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "23    {'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "24   {'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "25   {'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "26   {'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "27     {'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "28     {'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "29     {'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "30            {'C': 1, 'class_weight': 'balanced', 'max_iter': 2500}   \n",
       "31            {'C': 1, 'class_weight': 'balanced', 'max_iter': 5000}   \n",
       "32            {'C': 1, 'class_weight': 'balanced', 'max_iter': 1000}   \n",
       "33         {'C': 10.0, 'class_weight': 'balanced', 'max_iter': 1000}   \n",
       "34         {'C': 10.0, 'class_weight': 'balanced', 'max_iter': 5000}   \n",
       "35         {'C': 10.0, 'class_weight': 'balanced', 'max_iter': 2500}   \n",
       "36  {'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "37  {'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "38  {'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "39   {'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "40   {'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "41   {'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "42          {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 5000}   \n",
       "43          {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 2500}   \n",
       "44          {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 1000}   \n",
       "\n",
       "    split0_test_precision  split1_test_precision  ...  split2_test_recall  \\\n",
       "0                0.941043               0.941557  ...            0.990601   \n",
       "1                0.941043               0.941557  ...            0.990601   \n",
       "2                0.941043               0.941557  ...            0.990601   \n",
       "3                0.941043               0.941557  ...            0.990601   \n",
       "4                0.941043               0.941557  ...            0.990601   \n",
       "5                0.941043               0.941557  ...            0.990601   \n",
       "6                0.935211               0.932358  ...            0.997788   \n",
       "7                0.935211               0.932358  ...            0.997788   \n",
       "8                0.935211               0.932358  ...            0.997788   \n",
       "9                0.935211               0.932358  ...            0.997788   \n",
       "10               0.935211               0.932358  ...            0.997788   \n",
       "11               0.935211               0.932358  ...            0.997788   \n",
       "12               0.941571               0.943814  ...            0.979174   \n",
       "13               0.941571               0.943814  ...            0.979174   \n",
       "14               0.941571               0.943814  ...            0.979174   \n",
       "15               0.941571               0.943647  ...            0.979174   \n",
       "16               0.941571               0.943647  ...            0.979359   \n",
       "17               0.941571               0.943814  ...            0.978990   \n",
       "18               0.945664               0.948167  ...            0.965352   \n",
       "19               0.945664               0.948167  ...            0.965352   \n",
       "20               0.945664               0.948158  ...            0.965352   \n",
       "21               0.953493               0.958418  ...            0.955953   \n",
       "22               0.953493               0.958418  ...            0.955953   \n",
       "23               0.953493               0.958418  ...            0.955953   \n",
       "24               0.944172               0.947244  ...            0.960560   \n",
       "25               0.944384               0.947206  ...            0.960192   \n",
       "26               0.944222               0.946843  ...            0.960007   \n",
       "27               0.947867               0.948118  ...            0.952820   \n",
       "28               0.947867               0.948099  ...            0.952820   \n",
       "29               0.947858               0.948109  ...            0.952635   \n",
       "30               0.949432               0.951085  ...            0.950055   \n",
       "31               0.949432               0.951085  ...            0.950055   \n",
       "32               0.949432               0.951085  ...            0.949871   \n",
       "33               0.945531               0.947119  ...            0.955769   \n",
       "34               0.945738               0.946543  ...            0.955031   \n",
       "35               0.945555               0.946217  ...            0.954663   \n",
       "36               0.945414               0.947138  ...            0.953373   \n",
       "37               0.944854               0.946005  ...            0.952267   \n",
       "38               0.945142               0.945285  ...            0.951345   \n",
       "39               0.962841               0.967904  ...            0.914854   \n",
       "40               0.962841               0.967904  ...            0.914854   \n",
       "41               0.962841               0.967904  ...            0.914854   \n",
       "42               0.973281               0.976564  ...            0.872650   \n",
       "43               0.973281               0.976564  ...            0.872650   \n",
       "44               0.973281               0.976564  ...            0.872650   \n",
       "\n",
       "    mean_test_recall  std_test_recall  rank_test_recall  split0_test_f1  \\\n",
       "0           0.991154         0.000451                 7        0.965449   \n",
       "1           0.991154         0.000451                 7        0.965449   \n",
       "2           0.991154         0.000451                 7        0.965449   \n",
       "3           0.991154         0.000451                 7        0.965449   \n",
       "4           0.991154         0.000451                 7        0.965449   \n",
       "5           0.991154         0.000451                 7        0.965449   \n",
       "6           0.997850         0.000379                 1        0.965314   \n",
       "7           0.997850         0.000379                 1        0.965314   \n",
       "8           0.997850         0.000379                 1        0.965314   \n",
       "9           0.997850         0.000379                 1        0.965314   \n",
       "10          0.997850         0.000379                 1        0.965314   \n",
       "11          0.997850         0.000379                 1        0.965314   \n",
       "12          0.981141         0.001516                14        0.961774   \n",
       "13          0.981141         0.001516                14        0.961774   \n",
       "14          0.981141         0.001516                14        0.961774   \n",
       "15          0.981141         0.001516                14        0.961774   \n",
       "16          0.981203         0.001437                13        0.961774   \n",
       "17          0.981080         0.001596                18        0.961774   \n",
       "18          0.967135         0.001317                19        0.956941   \n",
       "19          0.967135         0.001317                19        0.956941   \n",
       "20          0.967074         0.001299                21        0.956941   \n",
       "21          0.954420         0.002045                29        0.954633   \n",
       "22          0.954420         0.002045                29        0.954633   \n",
       "23          0.954420         0.002045                29        0.954633   \n",
       "24          0.960071         0.002578                22        0.950394   \n",
       "25          0.958904         0.003358                23        0.949317   \n",
       "26          0.958782         0.003129                24        0.949326   \n",
       "27          0.955833         0.002236                25        0.952992   \n",
       "28          0.955710         0.002206                26        0.952992   \n",
       "29          0.955648         0.002236                27        0.952896   \n",
       "30          0.952700         0.001990                35        0.952136   \n",
       "31          0.952700         0.001990                35        0.952136   \n",
       "32          0.952638         0.002072                37        0.952136   \n",
       "33          0.955403         0.001672                28        0.949349   \n",
       "34          0.953929         0.002385                32        0.948171   \n",
       "35          0.953867         0.002542                33        0.947988   \n",
       "36          0.952946         0.003995                34        0.946632   \n",
       "37          0.950673         0.004941                38        0.944419   \n",
       "38          0.949813         0.005152                39        0.944009   \n",
       "39          0.906935         0.012537                40        0.939211   \n",
       "40          0.906935         0.012537                40        0.939211   \n",
       "41          0.906935         0.012537                40        0.939211   \n",
       "42          0.858529         0.015418                43        0.916431   \n",
       "43          0.858529         0.015418                43        0.916431   \n",
       "44          0.858529         0.015418                43        0.916431   \n",
       "\n",
       "    split1_test_f1  split2_test_f1  mean_test_f1  std_test_f1  rank_test_f1  \n",
       "0         0.965982        0.964904      0.965445     0.000440             1  \n",
       "1         0.965982        0.964904      0.965445     0.000440             1  \n",
       "2         0.965982        0.964904      0.965445     0.000440             1  \n",
       "3         0.965982        0.964904      0.965445     0.000440             1  \n",
       "4         0.965982        0.964904      0.965445     0.000440             1  \n",
       "5         0.965982        0.964904      0.965445     0.000440             1  \n",
       "6         0.964222        0.964461      0.964666     0.000469             7  \n",
       "7         0.964222        0.964461      0.964666     0.000469             7  \n",
       "8         0.964222        0.964461      0.964666     0.000469             7  \n",
       "9         0.964222        0.964461      0.964666     0.000469             7  \n",
       "10        0.964222        0.964461      0.964666     0.000469             7  \n",
       "11        0.964222        0.964461      0.964666     0.000469             7  \n",
       "12        0.962233        0.961542      0.961850     0.000287            13  \n",
       "13        0.962233        0.961542      0.961850     0.000287            13  \n",
       "14        0.962233        0.961542      0.961850     0.000287            13  \n",
       "15        0.962147        0.961542      0.961821     0.000249            16  \n",
       "16        0.962147        0.961462      0.961794     0.000280            17  \n",
       "17        0.962233        0.961361      0.961790     0.000356            18  \n",
       "18        0.957767        0.956450      0.957053     0.000543            19  \n",
       "19        0.957767        0.956450      0.957053     0.000543            19  \n",
       "20        0.957672        0.956450      0.957021     0.000502            21  \n",
       "21        0.954962        0.956393      0.955329     0.000764            22  \n",
       "22        0.954962        0.956393      0.955329     0.000764            22  \n",
       "23        0.954962        0.956393      0.955329     0.000764            22  \n",
       "24        0.955036        0.954229      0.953219     0.002025            25  \n",
       "25        0.954654        0.954038      0.952669     0.002384            26  \n",
       "26        0.954288        0.953942      0.952519     0.002262            27  \n",
       "27        0.952294        0.951067      0.952117     0.000796            28  \n",
       "28        0.952101        0.951067      0.952053     0.000787            29  \n",
       "29        0.952197        0.950970      0.952021     0.000796            30  \n",
       "30        0.952135        0.951019      0.951764     0.000526            31  \n",
       "31        0.952135        0.951019      0.951764     0.000526            31  \n",
       "32        0.952135        0.950923      0.951731     0.000572            33  \n",
       "33        0.952154        0.952433      0.951312     0.001393            34  \n",
       "34        0.951316        0.952136      0.950541     0.001709            35  \n",
       "35        0.951334        0.951681      0.950334     0.001665            36  \n",
       "36        0.952346        0.951356      0.950112     0.002493            37  \n",
       "37        0.950862        0.950777      0.948686     0.003018            38  \n",
       "38        0.950225        0.950645      0.948293     0.003034            39  \n",
       "39        0.926904        0.939973      0.935363     0.005989            40  \n",
       "40        0.926904        0.939973      0.935363     0.005989            40  \n",
       "41        0.926904        0.939973      0.935363     0.005989            40  \n",
       "42        0.901459        0.920937      0.912942     0.008326            43  \n",
       "43        0.901459        0.920937      0.912942     0.008326            43  \n",
       "44        0.901459        0.920937      0.912942     0.008326            43  \n",
       "\n",
       "[45 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>2.649263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.144584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.027297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.006587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_C</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_class_weight</th>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_max_iter</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.941043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.941557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.940507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.941036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.000429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.991155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.991707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.990601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.991154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.000451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.965449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.965982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.964904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.965445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.00044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  0\n",
       "mean_fit_time                                                              2.649263\n",
       "std_fit_time                                                               0.144584\n",
       "mean_score_time                                                            0.027297\n",
       "std_score_time                                                             0.006587\n",
       "param_C                                                                           1\n",
       "param_class_weight                                                 {0: 1.0, 1: 1.0}\n",
       "param_max_iter                                                                 1000\n",
       "params                 {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}\n",
       "split0_test_precision                                                      0.941043\n",
       "split1_test_precision                                                      0.941557\n",
       "split2_test_precision                                                      0.940507\n",
       "mean_test_precision                                                        0.941036\n",
       "std_test_precision                                                         0.000429\n",
       "rank_test_precision                                                              34\n",
       "split0_test_recall                                                         0.991155\n",
       "split1_test_recall                                                         0.991707\n",
       "split2_test_recall                                                         0.990601\n",
       "mean_test_recall                                                           0.991154\n",
       "std_test_recall                                                            0.000451\n",
       "rank_test_recall                                                                  7\n",
       "split0_test_f1                                                             0.965449\n",
       "split1_test_f1                                                             0.965982\n",
       "split2_test_f1                                                             0.964904\n",
       "mean_test_f1                                                               0.965445\n",
       "std_test_f1                                                                 0.00044\n",
       "rank_test_f1                                                                      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svc_cvdf = pd.DataFrame(svc_cv.cv_results_).sort_values(by='rank_test_f1').reset_index(drop=True)\n",
    "svc_best = pd.DataFrame(svc_cvdf.iloc[0,:])\n",
    "display(svc_cvdf)\n",
    "display(svc_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "826753ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                0\n",
      "params               {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}\n",
      "mean_test_precision                                                      0.941036\n",
      "mean_test_recall                                                         0.991154\n",
      "mean_test_f1                                                             0.965445\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(svc_best.loc[['params', 'mean_test_precision','mean_test_recall','mean_test_f1'],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32d13b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Classifcation Report - Random Forest\n",
      "**************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.46      0.43       515\n",
      "           1       0.95      0.93      0.94      5426\n",
      "\n",
      "    accuracy                           0.89      5941\n",
      "   macro avg       0.67      0.70      0.68      5941\n",
      "weighted avg       0.90      0.89      0.90      5941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_rf = RandomForestClassifier(class_weight={0:10.0, 1:1.0}, criterion= 'entropy', max_depth= 5, \\\n",
    "                                 max_features= 'log2', n_estimators= 57)\n",
    "maybe_rf = RandomForestClassifier(n_estimators=100, max_depth=15, class_weight={0:10.0, 1:1.0})\n",
    "\n",
    "maybe_rf.fit(X_tr_vec, y_tr)\n",
    "y_pr = maybe_rf.predict(X_te_vec)\n",
    "y_prob = maybe_rf.predict_proba(X_te_vec)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_te, y_prob[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "precision, recall, pthresholds = precision_recall_curve(y_te, y_prob[:, 1])\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "# Plot ROC curve\n",
    "\n",
    "ax[0].plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "ax[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax[0].set_xlabel('False Positive Rate')\n",
    "ax[0].set_ylabel('True Positive Rate')\n",
    "ax[0].set_title('Receiver Operating Characteristic (ROC) Curve - Random Forest')\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "ax[1].plot(recall, precision, color='b', alpha=0.8, label='Precision-Recall curve (area = %0.2f)' % pr_auc)\n",
    "ax[1].set_xlabel('Recall')\n",
    "ax[1].set_ylabel('Precision')\n",
    "ax[1].set_title('Precision-Recall Curve - Random Forest')\n",
    "ax[1].legend(loc='lower left')\n",
    "\n",
    "plt.show();\n",
    "# Classification report\n",
    "\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_te, y_pr), cmap='Blues', cbar=True, annot=True, xticklabels= True, yticklabels=True, fmt = 'd')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "\n",
    "plt.show();\n",
    "\n",
    "print('*'*50)\n",
    "print('Classifcation Report - Random Forest')\n",
    "print('*'*50)\n",
    "print(classification_report(y_te, y_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8906915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16279, number of negative: 1544\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 40627\n",
      "[LightGBM] [Info] Number of data points in the train set: 17823, number of used features: 1999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678320 -> initscore=0.746062\n",
      "[LightGBM] [Info] Start training from score 0.746062\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABlUAAANVCAYAAADhqHiEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3gU1dvG8e+mNxJCC71KR3rvNXQQRECKNBXFju0HAooiCPYCIkoR6SDSe1F6U4pSBKSX0CG09Hn/mDebLEkggSSTcn+ua6/dOXNm5tlhsszMM+ccm2EYBiIiIiIiIiIiIiIiInJfTlYHICIiIiIiIiIiIiIikh4oqSIiIiIiIiIiIiIiIpIISqqIiIiIiIiIiIiIiIgkgpIqIiIiIiIiIiIiIiIiiaCkioiIiIiIiIiIiIiISCIoqSIiIiIiIiIiIiIiIpIISqqIiIiIiIiIiIiIiIgkgpIqIiIiIiIiIiIiIiIiiaCkioiIiIiIiIiIiIiISCIoqSLpxpQpU7DZbPaXi4sLefLkoWvXrhw5csTq8AAoXLgwvXv3tjqMOG7fvs0nn3xCpUqV8PHxwdvbm4oVKzJy5Ehu375tdXiJNnLkSBYsWBCn/Pfff8dms/H777+nekzRjh07xssvv0yJEiXw9PTEy8uLsmXLMmTIEM6ePWuv17BhQ8qVK2dZnI9ixowZfPXVVym2/of5+9myZQsffPAB169fjzOvYcOGNGzYMFlii9akSRNeeOEF+3T0sRf9cnZ2JmfOnLRt25Zdu3bFuw7DMJgxYwaNGzfG398fd3d3ihYtyksvvcTp06cT3PbixYtp27YtAQEBuLm5kS1bNpo0acL06dMJDw8H4Nq1a2TNmjXev5P7Sezxm1F98MEH2Gw2Ll++nGCdR/mdOXHiBDabjc8+++yBdZctW8YHH3yQ4PzQ0FDGjh1LgwYNyJ49O66urmTPnp2GDRvyww8/cPPmTYf6sY9Pm82Gt7c3pUuXZvjw4XF+/3v37o3NZiNLlizcunUrzrZPnjyJk5MTNpvtvjGKiIhIXPFdz+bPn58+ffpYcr7Vu3dvChcunKRlos9ppkyZkiIxPUj0uUr0y83NjWLFivHWW28RHBxsSUyxxbd/ov/dT5w4kah17Nu3jz59+lCkSBE8PDzw8fGhcuXKjBkzhqtXr6ZM4GlI79698fHxuW+dpO7T2KLP6efNm/fAug+6/g0ODuaTTz6hRo0aZM2aFVdXVwICAmjRogUzZswgNDTUXjf62Ij98vX1pUKFCnz11VdERkY6rLthw4bYbDaKFi2KYRhxtr1hwwb7eqz6exTJ7FysDkAkqSZPnkypUqUICQlh8+bNfPzxx6xfv55Dhw7h7+9vaWy//fYbvr6+lsZwrwsXLtC0aVP+++8/Xn31VcaMGQPAunXrGDFiBDNnzmTNmjUEBARYHOmDjRw5kk6dOvHEE084lFeuXJmtW7dSpkwZS+JasmQJXbt2JUeOHLz88stUqlQJm83G33//zaRJk1i6dCm7d++2JLbkNGPGDP755x9ef/31FFn/w/z9bNmyheHDh9O7d2+yZs3qMG/cuHHJGB0sXLiQzZs3M3Xq1DjzRo4cSaNGjQgPD2f37t0MHz6cBg0asGfPHooXL26vFxUVRbdu3Zg9ezZPP/00U6ZMwc/Pj3379vHpp58yY8YMlixZQp06dezLGIZB3759mTJlCq1ateKLL76gQIEC3Lhxg/Xr1zNgwAAuX77Ma6+9hr+/P2+88QZvv/02rVq1ws3N7YHfK7Mcv48qtX5nli1bxtixY+NNWly6dIkWLVrwzz//0KtXL1599VVy5crFlStXWLduHe+88w6bNm3il19+cViuU6dOvPnmmwDcunWLP/74gw8//JB9+/bx66+/OtR1dXUlIiKC2bNn069fP4d5kydPJkuWLGnipoWIiEh6FX09e/fuXTZs2MCoUaP4448/+Pvvv/H29k61OIYOHcprr72WpGXy5MnD1q1bKVasWApF9WCenp6sW7cOgOvXrzNv3jw+//xz9u3bx6pVqyyLKzn8+OOPDBgwgJIlS/L2229TpkwZwsPD2bVrF+PHj2fr1q389ttvVodpudatW7N161by5MmTotu53/XvkSNHaNGiBRcvXuT555/nvffew9/fn/Pnz7Ny5Ur69u3LwYMH+eijjxyWe+WVV+jWrRtgHr+LFi3ijTfe4PTp03z++ecOdbNkycLx48dZt24dTZo0cZg3adIkfH19dV4uYiVDJJ2YPHmyARg7d+50KB8+fLgBGJMmTbIoMmtFREQYISEhCc4PDAw0XFxcjI0bN8aZt3HjRsPFxcVo3rx5SoYYrwfFHR9vb2+jV69eKRPQQzp27Jjh7e1tVKpUybh+/Xqc+VFRUcavv/5qn27QoIFRtmzZFI0pKirKuHPnTrKvt3Xr1kahQoWSfb2PEuunn35qAMbx48eTL6AEVK9e3ejatatD2fr16w3AmDt3rkP5zz//bADGsGHDHMpHjhxpAMYnn3wSZ/1BQUFGoUKFjICAAOPatWv28tGjRxuAMXz48HjjOn/+vMPfd1BQkOHi4mJMnz79gd8pqcfvowgLCzPCw8OTZV3J7f333zcA49KlSymy/uPHjxuA8emnnz6w7ksvvWQkdHoWGBhouLq6Gn/88Ue88y9fvmz88ssvDmWA8dJLL8Wp27NnT8PJycm4e/euvaxXr16Gt7e30bVrV6N27doO9aOiooxChQoZzz33nAEY77///gO/i4iIiMRI6Hp26NChBmBMmzYtwWVv376d0uGlC9HnKvdq1KiRARjHjh2zIKoY0ed8kydPtpdF/7s/6Hply5YthrOzs9GiRYt4r5NDQ0ONhQsXJkucd+7cMaKiopJlXcktoX/j5JLQ9Vt8Err+DQ8PN8qUKWNkzZrVOHDgQLzLnjhxwvjtt9/s0/e7HqhXr56RJ08eh7Lo+wY1a9Y0unXr5jAvODjY8PLysp+Xxz7eRCT1qPsvSfeqVq0KmC0yYtu1axft2rUjW7ZseHh4UKlSJebMmRNn+bNnz/L8889ToEAB3NzcyJs3L506dXJYX3BwMG+99RZFihTBzc2NfPny8frrr8fpOiV290WXLl3Czc2NoUOHxtnmoUOHsNlsfPPNN/ayoKAg+vfvT/78+XFzc6NIkSIMHz6ciIgIe53oJqNjxoxhxIgRFClSBHd3d9avXx/vvtm1axerVq2iX79+1K1bN878unXr0rdvX1auXMmff/5pL7fZbLz88sv88MMPlChRAnd3d8qUKcOsWbPirONR4w4JCeHNN9+kYsWK+Pn5kS1bNmrVqsXChQsdtmOz2bh9+zY///yzvZlrdNdO8XXLE91s+OjRo7Rq1QofHx8KFCjAm2++6dAMF+DMmTN06tSJLFmykDVrVrp3787OnTsT1ZT2iy++4Pbt24wbNw4/P7848202Gx07doxTvnPnTurVq4eXlxdFixblk08+ISoqyj4/sfslehsvv/wy48ePp3Tp0ri7u/Pzzz8DMHz4cGrUqEG2bNnw9fWlcuXKTJw4Md4mxDNmzKBWrVr4+Pjg4+NDxYoVmThxImA2P166dCknT550aLIcLSwsjBEjRlCqVCnc3d3JmTMnffr04dKlSw7bKFy4MG3atGH+/PlUqlQJDw8Phg8fbp8Xu/uvqKgoRowYQcmSJfH09CRr1qyUL1+er7/+GjC7bHr77bcBKFKkiD2m6OMgvu6/QkND+fDDDyldujQeHh5kz56dRo0asWXLljj7I7bdu3ezY8cOevbsed960eL7XQoLC+PTTz+ldOnSvPPOO3GWCQgIYNSoUVy4cMG+38PDwxk9ejSlSpWK97cEIHfu3A5/3wEBATRr1ozx48c/MM6kHr8JddF2776O/pv85ZdfePPNN8mXLx/u7u7s378fm81m/36xLV++HJvNxqJFi+xlR44coVu3buTKlQt3d3dKly7N2LFjH/i9UkJC3X/9+OOPDr+TM2bMuG93Gl988QVFihTBx8eHWrVqsW3bNvu83r17279f7L+zEydOsHPnTlatWsXzzz9P/fr141139uzZ6dGjR6K+j5+fn73Lunv17duXLVu28O+//9rL1qxZw8mTJ+nTp0+i1i8iIiKJU7NmTcDsZhNirmP+/vtvAgMDyZIli/0p9cSec8P9z+2jt3Pv+crcuXOpUaMGfn5+9uuUvn372ucn1P3Xpk2baNKkCVmyZMHLy4vatWuzdOlShzrRXTatX7+eF198kRw5cpA9e3Y6duzIuXPnHnr/QcL3BGbPnk2tWrXw9vbGx8eH5s2bx9sCe/v27bRt25bs2bPj4eFBsWLFHFonHD16lD59+lC8eHG8vLzIly8fbdu25e+//36kuGMbOXIkNpuNCRMm4O7uHme+m5sb7dq1s08n1B3rvefr0ft91apV9O3bl5w5c+Ll5cXs2bOx2WysXbs2zjq+//57bDYb+/bts5cl9v5Kaoiv+y/DMBg5ciSFChXCw8ODqlWrsnr16gS7hA4PD+e9994jb968+Pr60rRpU4dz3/td//72228cOHCA9957j9KlS8cbY6FCheL0sJEQPz8/XF1d453Xt29f5s+f79DddfR9ma5duyZq/SKSMtT9l6R7x48fB6BEiRL2svXr19OiRQtq1KjB+PHj8fPzY9asWXTp0oU7d+7YTzLOnj1LtWrVCA8PZ/DgwZQvX54rV66wcuVKrl27RkBAAHfu3KFBgwacOXPGXmf//v0MGzaMv//+mzVr1jjcXI6WM2dO2rRpw88//8zw4cNxcorJYU6ePBk3Nze6d+8OmImJ6tWr4+TkxLBhwyhWrBhbt25lxIgRnDhxgsmTJzus+5tvvqFEiRJ89tln+Pr6OnQvFNvq1asB7vuf+RNPPMGECRNYvXo1VapUsZcvWrSI9evX8+GHH+Lt7c24ceN4+umncXFxoVOnTskWd2hoKFevXuWtt94iX758hIWFsWbNGjp27MjkyZN55plnANi6dSuNGzemUaNG9pvLD+oqKjw8nHbt2tGvXz/efPNNNmzYwEcffYSfnx/Dhg0DzPFmGjVqxNWrVxk9ejSPPfYYK1asoEuXLvddd7RVq1YREBBgvxhKjKCgILp3786bb77J+++/z2+//cagQYPImzev/fsmdr9EW7BgARs3bmTYsGHkzp2bXLlyAeaFT//+/SlYsCAA27Zt45VXXuHs2bP2fQAwbNgwPvroIzp27Mibb76Jn58f//zzj/3ibty4cTz//PP8999/cZqcR0VF0b59ezZu3Mg777xD7dq1OXnyJO+//z4NGzZk165deHp62uv/9ddfHDx4kCFDhlCkSJEEuzkYM2YMH3zwAUOGDKF+/fqEh4dz6NAh+wnls88+y9WrV/n222+ZP3++vfl3Qt0zRURE0LJlSzZu3Mjrr79O48aNiYiIYNu2bZw6dYratWsn+G+2ZMkSnJ2dE7yZfa/4fpf+/PNPrl27xvPPPx/vbwZA27ZtcXJyYvXq1bz55pvs2rWLq1ev8txzzyW4THwaNmzIoEGDuH79epxu0WJ7mOM3KQYNGkStWrUYP348Tk5OFChQgEqVKjF58uQ4XUtNmTKFXLly0apVKwAOHDhA7dq1KViwIJ9//jm5c+dm5cqVvPrqq1y+fJn3338/RWJOigkTJtC/f3+efPJJvvzyS27cuMHw4cPjJG6jjR07llKlStn7Zh46dCitWrXi+PHj+Pn5MXToUG7fvs28efPYunWrfbk8efIwY8YMAIeL6cQyDMOe6I7u/uvnn3+ma9eu8V7ANW3alEKFCjFp0iRGjx4NwMSJE6lfv36C/9+IiIjIwzl69ChgXj9GCwsLo127dvTv35///e9/REREJOmc+0Hn9vHZunUrXbp0oUuXLnzwwQd4eHhw8uRJe1dbCfnjjz9o1qwZ5cuXZ+LEibi7uzNu3Djatm3LzJkz41xXPfvss7Ru3ZoZM2Zw+vRp3n77bXr06PHA7dzP8ePHcXFxoWjRovaykSNHMmTIEPr06cOQIUPsDzjVq1ePHTt22K8ZVq5cSdu2bSldujRffPEFBQsW5MSJEw5diZ07d47s2bPzySefkDNnTq5evcrPP/9MjRo12L17NyVLlnzo2AEiIyNZt24dVapUoUCBAo+0roT07duX1q1b88svv3D79m3atGlDrly5mDx5cpyupaZMmULlypUpX748kPj7K1Z67733GDVqFM8//zwdO3bk9OnTPPvss4SHhztck0UbPHgwderU4aeffiI4OJh3332Xtm3bcvDgQZydne97/Rt9n+VhzsujoqLs5+U3btxg4cKFrFixgnfffTfe+l27duWNN95g5syZvPjii4B5Xt6pU6c01/W8SKZjcUsZkUSLbja7bds2Izw83Lh586axYsUKI3fu3Eb9+vUdupUpVaqUUalSpThdzbRp08bIkyePERkZaRiGYfTt29dwdXVNsMmmYRjGqFGjDCcnpzjNtOfNm2cAxrJly+xlhQoVcuieatGiRQZgrFq1yl4WERFh5M2b13jyySftZf379zd8fHyMkydPOmzjs88+MwBj//79hmHENBktVqyYERYW9qBdZrzwwgsGYBw6dCjBOgcPHjQA48UXX7SXAYanp6cRFBTkEHepUqWMxx57LEXjjoiIMMLDw41+/foZlSpVcpiXUPdf0U14169fby/r1auXARhz5sxxqNuqVSujZMmS9umxY8cagLF8+XKHev37909UU1oPDw+jZs2a960TW4MGDQzA2L59u0N5mTJl7tsN2/32C2D4+fkZV69eve+2IyMjjfDwcOPDDz80smfPbm/yfezYMcPZ2dno3r37fZdPqPnzzJkzDSBON1E7d+40AGPcuHH2skKFChnOzs7Gv//+G2c99/79tGnTxqhYseJ9Y7pf918NGjQwGjRoYJ+eOnWqARg//vjjfdcZn5YtWxqlSpWKUx597M2ePdsIDw837ty5Y2zevNkoWbKkUaZMGYduvGbNmmUAxvjx4++7rYCAAKN06dJJWuZeq1evjve4vldSj997/42i3buvo/dL/fr149T95ptvDMDhGLh69arh7u5uvPnmm/ay5s2bG/nz5zdu3LjhsPzLL79seHh4PPB4T4rEdP917+9MZGSkkTt3bqNGjRoO9U6ePGm4uro6/K1E/wY+/vjjRkREhL18x44dBmDMnDnTXpZQ918J/Z5HRUUZ4eHh9lfs9RuG+fsQ36tly5bGrVu3HOrG7m7h/fffN3Lnzm2Eh4cbV65cMdzd3Y0pU6YYly5dUvdfIiIiDyG+69klS5YYOXPmNLJkyWK/9oq+jrm3e+vEnnMn9ty+V69eDucr0ddw8XUJGy2+7q1q1qxp5MqVy7h586a9LCIiwihXrpyRP39++zVH9PcfMGCAwzrHjBljAMb58+fvG290zN7e3vbznsuXLxvff/+94eTkZAwePNhe79SpU4aLi4vxyiuvOCx/8+ZNI3fu3Ebnzp3tZcWKFTOKFSvm0CXqg0RERBhhYWFG8eLFjTfeeMNe/rDdfwUFBRlAnK6G7yeh87F7z9ejt//MM8/EqTtw4EDD09PT4d/8wIEDBmB8++239rLE3l9JDonp/uvefRp9LdGlSxeHelu3bjWAeK9TWrVq5VB3zpw5BmBs3brVXpbQ9W+LFi0MIE43bfc7L48+NuJ79e7dO845fOxuw3v16mVUrVrVMAzD2L9/vwEYv//+u/1vX91/iVhD3X9JulOzZk1cXV3JkiULLVq0wN/fn4ULF+LiYja8Onr0KIcOHbK3AomIiLC/WrVqxfnz5+3NOpcvX06jRo0SbLIJ5hPq5cqVo2LFig7rat68ebxdwcTWsmVLcufO7dBiY+XKlZw7d86hGfWSJUto1KgRefPmddhGy5YtAfPpn9jatWuXYPPQpDL+vxuoe5+Cb9KkicPg9c7OznTp0oWjR49y5syZZI177ty51KlTBx8fH1xcXHB1dWXixIkcPHjwkb6bzWajbdu2DmXly5d3eELrjz/+sB9LsT399NOPtO37yZ07N9WrV79vXJC0/dK4cWP8/f3jlK9bt46mTZvi5+eHs7Mzrq6uDBs2jCtXrnDx4kXAfNImMjKSl1566aG+z5IlS8iaNStt27Z1OA4qVqxI7ty54/yNlC9fPt6nhe5VvXp19u7dy4ABA1i5cuUjD8K3fPlyPDw8HP72EuvcuXP21j/x6dKlC66urnh5eVGnTh2Cg4NZunTpfVuJJMQwjCS1SolPdKxnz559pPU8qieffDJOWffu3XF3d3foNmLmzJmEhobau5YKCQlh7dq1dOjQAS8vrzi/4yEhIQ7dZt0r+gmw6FdkZGSyf7d///2XoKAgOnfu7FBesGBB6tSpE+8yrVu3duhuK/rpv/s9NfogCxcuxNXV1f6Krxu3zp07s3PnTnbu3MmGDRv45ptv2LVrFy1atEiwVU2fPn24cOECy5cvZ/r06bi5ufHUU089dJwiIiJiin0926ZNG3Lnzs3y5csdrr0g7nlUYs+5H/bcvlq1aoB53jBnzpxEnUfevn2b7du306lTJ3x8fOzlzs7O9OzZkzNnzjh0qQRxn+6/93zoQedxt2/ftp/35MiRgxdffJEuXbrw8ccf2+usXLmSiIgInnnmGYd1eXh40KBBA/u+Onz4MP/99x/9+vXDw8Mjwe8ZERHByJEjKVOmDG5ubri4uODm5saRI0ce+Zo1tcR3Xt63b1/u3r3L7Nmz7WWTJ0/G3d3dPph6Uu6vxCcyMtJhmdhdXieXbdu2ERoaGue8vGbNmgl2yfug4/BhfP311w7n5RUqVIhT57XXXrOfl69fv56RI0cyZ86c+95/6Nu3L7t27eLvv/9m4sSJFCtWLNE9KIhIylFSRdKdqVOnsnPnTtatW0f//v05ePCgw39A0f2ovvXWWw7/obm6ujJgwAAALl++DJjjnuTPn/++27tw4QL79u2Ls64sWbJgGIZ9XfFxcXGhZ8+e/Pbbb/Yui6ZMmUKePHlo3ry5wzYWL14cZxtly5Z1iDdadDdHDxLd5VN0V0Txie6H9N5mxrlz545TN7rsypUryRb3/Pnz6dy5M/ny5WPatGls3bqVnTt30rdvX0JCQhL1PRPi5eUV5+TY3d3dYb1XrlyJcwEDxFsWn4IFC953/8Yne/bsccrc3d25e/eufTqp+yW+fbtjxw4CAwMBc9yHzZs3s3PnTt577z0A+/ai+2B+0N9CQi5cuMD169dxc3OLcywEBQU99PE7aNAgPvvsM7Zt20bLli3Jnj07TZo0YdeuXQ8V56VLl8ibN69DV3yJdffu3fteaI0ePZqdO3fyxx9/8N5773HhwgWeeOIJhxvWifl7vH37NpcvX7b/PSZmmfhExxr7mIrPwxy/SRHfv3W2bNlo164dU6dOtV8kT5kyherVq9t/O65cuUJERATffvttnGMqunuw+/329u3b12GZe7s0SA7Rv4NJ+f24928/ur/sB/07QcyxcO+FXsOGDe0XZm3atIl32Zw5c1K1alWqVq1KvXr1eOWVV/jmm2/YtGlTguNGFSpUiCZNmjBp0iQmTZpE165d8fLyemCcIiIicn/R17O7d+/m3Llz7Nu3L84DGV5eXnG69knsOffDntvXr1+fBQsW2JMR+fPnp1y5csycOTPBZa5du4ZhGPGe8+XNmxeIOWeK9qDzoQ8//NDhuxUrVsyhvqenp/3cZ/HixTRs2JCZM2fyySef2OtE3xOoVq1anH01e/bsJO+rgQMHMnToUJ544gkWL17M9u3b2blzJxUqVEjUedyD5MiRAy8vr1Q/Ly9btizVqlWzPwQaGRnJtGnTaN++PdmyZQOSdn8lPk2aNHFY5mEecHuQtHJe3q1bN/uxWbly5XiXzZ8/v/28PLrL5qFDhzJ37lxWrlwZ7zLRXfD+8MMP/PLLL/Tt2/eRH8ITkUenMVUk3SldurR9ILpGjRoRGRnJTz/9xLx58+jUqRM5cuQAzBuy8Q0QDtj7PM2ZM6e91UVCcuTIgaenJ5MmTUpw/v306dOHTz/91N7n6KJFi3j99dcdnlbOkSMH5cuXd3i6JrboE9Joif0PtFmzZgwePJgFCxbEaYkRbcGCBfa6sQUFBcWpG10WfQKSHHFPmzaNIkWK2AfKi5bQ09PJLXv27OzYsSNOeXzfPz7Nmzfn22+/Zdu2bck6LkVS90t8+3bWrFm4urqyZMkSh4RA9L95tOj+m8+cOfNQffhGDzK5YsWKeOdnyZLlgbHGx8XFhYEDBzJw4ECuX7/OmjVrGDx4MM2bN+f06dNJvsGbM2dONm3aRFRUVJITKzly5ODq1asJzi9atKj9d6l+/fp4enoyZMgQvv32W9566y0AqlSpgr+/P4sWLWLUqFHx7odFixYRFRVl/3usWrUq2bJlY+HChQkuE5/oWB/0+5TU49fDwyPeY/Dy5cvxbiuhePv06cPcuXNZvXo1BQsWZOfOnXz//ff2+f7+/vYnHBN6yrJIkSIJxvnBBx/w8ssv26fvPQaTQ/Tv4L0DokLifz+SIvr3fNGiRfZkKUDWrFntx158CduERD+Nt3fv3gTr9O3blx49ehAVFeXw7yMiIiIPL/b1bELiO4dK7Dn3o5zbt2/fnvbt2xMaGsq2bdsYNWoU3bp1o3DhwtSqVStOfX9/f5ycnDh//nycedGDzz/ofPRezz//vMODIvcO2u7k5OSw/5o1a0aVKlUYPnw43bt3p0CBAvZtzps3j0KFCiW4rdj76n6mTZvGM888w8iRIx3KL1++/FAt0+/l7OxMkyZNWL58OWfOnElUQszd3T3e8/J7k1jR7ndePmDAAA4ePMixY8c4f/68vfU4kKT7K/H54YcfuHnzZpz1JacHnZcn1FrlYTVr1owJEyawaNEi+7UemL0FRPcYkCVLlkTf04h9Xh774dvYoscGstls9OrV6xG/gYgkB7VUkXRvzJgx+Pv7M2zYMKKioihZsiTFixdn79699icA7n1Fn3C2bNmS9evX37e5aps2bfjvv//Inj17vOt60H/QpUuXpkaNGkyePJkZM2Y4dHETexv//PMPxYoVi3cb9yYnEqtq1aoEBgYyceJENm/eHGf+pk2bmDRpEi1atHAYpB5g7dq1DiclkZGRzJ49m2LFitlP8pIjbpvNhpubm8NJXlBQEAsXLoxT997WHMmhQYMG3Lx5k+XLlzuUz5o1K1HLv/HGG3h7ezNgwABu3LgRZ75hGHEGtkuMpOyX+63DxcXFIYF39+5dfvnlF4d6gYGBODs7P/CmaUL7v02bNly5coXIyMh4j4NHHbgRzBvHnTp14qWXXuLq1av2FlZJeaKoZcuWhISEJPhk/v2UKlWKY8eOJbr+O++8w2OPPcYnn3xiv4hwc3Pj7bff5uDBg3z66adxlrl48SKDBg0iICCAZ599FgBXV1feffddDh06xEcffRTvti5evBjn7zs61ugBOBOS1OO3cOHC7Nu3z6HO4cOH7/sbGp/AwEDy5cvH5MmTmTx5Mh4eHg4tDr28vGjUqBG7d++mfPny8R5X90sgFC5cONmPwXuVLFmS3LlzM2fOHIfyU6dOsWXLlodeb0LHdPTv+Y8//sjGjRsfev3R9uzZA3Dfbu06dOhAhw4d6Nu3b7ImjUVERCTpEnvOndhz+/txd3enQYMGjB49GoDdu3fHW8/b25saNWowf/58h3OXqKgopk2bRv78+RPV7W9sefPmdfhejz/++ANjHTt2LCEhIYwYMQIwHxxycXHhv//+S/CeAECJEiUoVqwYkyZNuu8NcJvNFie5s3Tp0mTtanfQoEEYhsFzzz1HWFhYnPnh4eEsXrzYPh3fefm6deu4detWkrb79NNP4+HhwZQpU5gyZQr58uVzeIAnKfdX4lOyZMkk3T95GDVq1MDd3d2hGzMwuwV7lO68Err+7dChA2XKlGHkyJEcOnToodcfLTHn5b169aJt27a8/fbb5MuX75G3KSKPTi1VJN3z9/dn0KBBvPPOO8yYMYMePXrwww8/0LJlS5o3b07v3r3Jly8fV69e5eDBg/z111/MnTsXMJsWL1++nPr16zN48GAef/xxrl+/zooVKxg4cCClSpXi9ddf59dff6V+/fq88cYblC9fnqioKE6dOsWqVat48803qVGjxn1j7Nu3L/379+fcuXPUrl07zg2+Dz/8kNWrV1O7dm1effVVSpYsSUhICCdOnGDZsmWMHz/+obtmmjp1Kk2bNiUwMJBXX33V3g3OunXr+PrrrylVqlS8N5lz5MhB48aNGTp0KN7e3owbN45Dhw45JBuSI+42bdowf/58BgwYQKdOnTh9+jQfffQRefLk4ciRIw51H3/8cX7//XcWL15Mnjx5yJIlyyPfLO3VqxdffvklPXr0YMSIETz22GMsX77c3vT2QS0aihQpYm+FVLFiRV5++WUqVaoEwIEDB5g0aRKGYdChQ4ckxZWU/ZKQ1q1b88UXX9CtWzeef/55rly5wmeffRbngqBw4cIMHjyYjz76iLt37/L000/j5+fHgQMHuHz5MsOHDwfM/T9//ny+//57qlSpYn9KrGvXrkyfPp1WrVrx2muvUb16dVxdXTlz5gzr16+nffv2Sf7+AG3btqVcuXJUrVqVnDlzcvLkSb766isKFSpE8eLF7TGB2X9tr169cHV1pWTJkvGe2D/99NNMnjyZF154gX///ZdGjRoRFRXF9u3bKV26NF27dk0wloYNGzJp0iQOHz6cqAtDV1dXRo4cSefOnfn6668ZMmQIAO+++y579+61v3fp0gU/Pz/27dvHp59+ys2bN1myZInDuBjRiZj333+fHTt20K1bNwoUKMCNGzfYsGEDEyZMYPjw4Q7dRmzbto3s2bM/8CI0qcdvz5496dGjBwMGDODJJ5/k5MmTjBkzxv6UX2I5OzvzzDPP8MUXX+Dr60vHjh3jjAXy9ddfU7duXerVq8eLL75I4cKFuXnzJkePHmXx4sWsW7cuSdtMjMWLF8d77HTq1ClOmZOTE8OHD6d///506tSJvn37cv36dYYPH06ePHkeqps5iDmmR48eTcuWLXF2dqZ8+fK4ubkxbdo0mjdvTtOmTenduzfNmzcnV65cBAcHs2/fPtasWROnqxAwn9qLHoMmJCSEPXv2MGLECLJmzRonyR+bh4cH8+bNe6jvISIiIskrsefciT23v9ewYcM4c+YMTZo0IX/+/Fy/ft0+RkSDBg0SjGvUqFE0a9aMRo0a8dZbb+Hm5sa4ceP4559/mDlzZqp0U9SgQQNatWrF5MmT+d///keRIkX48MMPee+99zh27Jh9LNYLFy6wY8cOvL297fth7NixtG3blpo1a/LGG29QsGBBTp06xcqVK5k+fTpgXptNmTKFUqVKUb58ef78808+/fTTh75Gj0+tWrX4/vvvGTBgAFWqVOHFF1+kbNmyhIeHs3v3biZMmEC5cuXsY4b27NmToUOHMmzYMBo0aMCBAwf47rvv4h1f736yZs1Khw4dmDJlCtevX+ett96Kcx6b2PsrySUyMjLec1Bvb2/7+K2xZcuWjYEDBzJq1Cj8/f3p0KEDZ86cSZbz8viuf52dnVmwYAHNmzenevXqPPfcczRs2BB/f3+uX7/O9u3b2bt3b7xj9546dcp+Xn779m22bt3KqFGjKFSoUIItgcBMNt7b44SIWCyZB74XSTGTJ082AGPnzp1x5t29e9coWLCgUbx4cSMiIsIwDMPYu3ev0blzZyNXrlyGq6urkTt3bqNx48bG+PHjHZY9ffq00bdvXyN37tyGq6urkTdvXqNz587GhQsX7HVu3bplDBkyxChZsqTh5uZm+Pn5GY8//rjxxhtvGEFBQfZ6hQoVMnr16hUnvhs3bhienp4GYPz444/xfr9Lly4Zr776qlGkSBHD1dXVyJYtm1GlShXjvffeM27dumUYhmEcP37cAIxPP/00Sfvu1q1bxsiRI42KFSsaXl5ehpeXl1G+fHljxIgR9nXHBhgvvfSSMW7cOKNYsWKGq6urUapUKWP69OkpEvcnn3xiFC5c2HB3dzdKly5t/Pjjj8b7779v3PsTtWfPHqNOnTqGl5eXARgNGjQwDMMw1q9fbwDG+vXr7XV79epleHt7x9lWfOs9deqU0bFjR8PHx8fIkiWL8eSTTxrLli0zAGPhwoX33bfR/vvvP2PAgAHGY489Zri7uxuenp5GmTJljIEDBxrHjx+312vQoIFRtmzZOMv36tXLKFSo0EPtl+h/r/hMmjTJKFmypOHu7m4ULVrUGDVqlDFx4kQDcIjLMAxj6tSpRrVq1QwPDw/Dx8fHqFSpkjF58mT7/KtXrxqdOnUysmbNathsNoc4wsPDjc8++8yoUKGCfflSpUoZ/fv3N44cOWKvV6hQIaN169bxxnrv38/nn39u1K5d28iRI4fh5uZmFCxY0OjXr59x4sQJh+UGDRpk5M2b13BycnI4Dho0aGA/RqLdvXvXGDZsmFG8eHHDzc3NyJ49u9G4cWNjy5Yt8cYU7caNG4aPj48xZswYh/LoY2/u3LnxLlejRg3D39/fuH79ur0sKirKmD59utGwYUMja9ashpubm1GkSBHjxRdfNE6ePJlgDAsXLjRat25t5MyZ03BxcTH8/f2NRo0aGePHjzdCQ0Md1l+oUCHjlVdeue93ii2xx29UVJQxZswYo2jRooaHh4dRtWpVY926dXH29YP2i2EYxuHDhw3AAIzVq1fHW+f48eNG3759jXz58hmurq5Gzpw5jdq1axsjRoxI9HdLjOi/q4Resb9T7N8ZwzCMCRMmGI899pjh5uZmlChRwpg0aZLRvn17o1KlSg7fI6HfQMB4//337dOhoaHGs88+a+TMmdP+dxb73yAkJMT49ttvjbp16xpZs2Y1XFxcjGzZshn16tUzRo8ebVy5ciXO+mO/XF1djaJFixp9+vQxjh496lA3od/N2C5duhQnZhEREXmw+13Pxna//48Te85tGA8+t7/3+mPJkiVGy5YtjXz58hlubm5Grly5jFatWhkbN26014k+p4m9HsMwjI0bNxqNGzc2vL29DU9PT6NmzZrG4sWLE/X9EzrHSuq++fvvvw0nJyejT58+9rIFCxYYjRo1Mnx9fQ13d3ejUKFCRqdOnYw1a9Y4LLt161ajZcuWhp+fn+Hu7m4UK1bMeOONN+zzr127ZvTr18/IlSuX4eXlZdStW9fYuHFjnHPg+PZP9Pe+99orIXv27DF69eplFCxY0HBzczO8vb2NSpUqGcOGDTMuXrxorxcaGmq88847RoECBQxPT0+jQYMGxp49e+JcUyXmuFu1apX9XPHw4cPx1kns/ZVH1atXrwTPyaOP1/j2aVRUlDFixAgjf/78hpubm1G+fHljyZIlRoUKFYwOHTrY6yV0nRLfv939rn8Nw7xGHDlypFGtWjXD19fXcHFxMXLlymU0a9bMGDt2rHH79u0464/98vDwMEqUKGG8/vrrxvnz5x3WndB9g9h27twZ79+jiKQOm2EYRjLlZ0Qkg7DZbLz00kt89913VodimZEjRzJkyBBOnTqVrE8gSfr1yiuvsHbtWvbv35+mBwZcu3YtgYGB7N+/n1KlSlkdTqZz/fp1SpQowRNPPMGECROsDkdEREREJFM6fvw4pUqV4v3332fw4MFWhyMiGYy6/xKRTC86eVSqVCnCw8NZt24d33zzDT169FBCReyGDBnC1KlT+fXXX+PtDiqtGDFiBH379lVCJRUEBQXx8ccf06hRI7Jnz87Jkyf58ssvuXnzJq+99prV4YmIiIiIZAp79+5l5syZ1K5dG19fX/7991/GjBmDr68v/fr1szo8EcmAlFQRkUzPy8uLL7/8khMnThAaGkrBggV599137eNgiAAEBAQwffp0rl27ZnUoCbp27RoNGjRgwIABVoeSKbi7u3PixAkGDBjA1atX8fLyombNmowfP56yZctaHZ6IiIiISKbg7e3Nrl27mDhxItevX8fPz4+GDRvy8ccfExAQYHV4IpIBqfsvERERERERERERERGRRHCyOgAREREREREREREREZH0QEkVERERERERERERERGRRFBSRUREREREREREREREJBEy3UD1UVFRnDt3jixZsmCz2awOR0REREQkxRmGwc2bN8mbNy9OTnquSh5M100iIiIikpkk5Zop0yVVzp07R4ECBawOQ0REREQk1Z0+fZr8+fNbHYakA7puEhEREZHMKDHXTJkuqZIlSxbA3Dm+vr4WRyMiIiIikvKCg4MpUKCA/VxY5EF03SQiIiIimUlSrpkyXVIluum6r6+vLg5EREREJFNRN06SWLpuEhEREZHMKDHXTOpQWUREREREREREREREJBGUVBEREREREREREREREUkEJVVEREREREREREREREQSQUkVERERERERERERERGRRFBSRUREREREREREREREJBGUVBEREREREREREREREUkEJVVEREREREREREREREQSQUkVERERERERERERERGRRFBSRUREREREREREREREJBGUVBEREREREREREREREUkEJVVEREREREREREREREQSQUkVERERERERERERERGRRFBSRUREREREREREREREJBGUVBEREREREREREREREUkEJVVEREREREREREREREQSQUkVERERERERERERERGRRFBSRUREREREREREREREJBGUVBEREREREREREREREUkEJVVEREREREREREREREQSQUkVERERERERERERERGRRFBSRUREREREREREREREJBGUVBEREREREREREREREUkEJVVEREREREREREREREQSQUkVERERERERERERERGRRFBSRUREREREREREREREJBGUVBEREREREREREREREUkEJVVEREREREREREREREQSQUkVERERERERERERERGRRLA0qbJhwwbatm1L3rx5sdlsLFiw4IHL/PHHH1SpUgUPDw+KFi3K+PHjUz5QERERERGRFJBS10S//vorZcqUwd3dnTJlyvDbb7+lQPQiIiIiIpmPpUmV27dvU6FCBb777rtE1T9+/DitWrWiXr167N69m8GDB/Pqq6/y66+/pnCkIiIiIiIiyS8lrom2bt1Kly5d6NmzJ3v37qVnz5507tyZ7du3p9TXEBERERHJNGyGYRhWBwFgs9n47bffeOKJJxKs8+6777Jo0SIOHjxoL3vhhRfYu3cvW7duTdR2goOD8fPz48aNG/j6+j5q2CIiIumDEQVREQ+uF3oDQq4mbp13LkD4nfjnXTsCVw+Bq3fiY4yXAUcXQM4KYLM94rpErHEn1Ia7i4GzM1D1bchTPdVj0Dlw+pBc10RdunQhODiY5cuX2+u0aNECf39/Zs6cmahYrDxmbt8GK/I/Li5QtSp4eaX+tkVERETEWkk5/3VJpZiSxdatWwkMDHQoa968ORMnTiQ8PBxXV9c4y4SGhhIaGmqfDg4OTvE4RURELBMZBv/OMZMaACeWQ9BOa2NKDtePWh2ByEMJjXCm3cTuZPe6w9Snf8O9VHerQ5J0LjHXRFu3buWNN96IU+err75KcL1p6brp4kV45x1rtt22Lbz/vjXbFhEREZH0IV0lVYKCgggICHAoCwgIICIigsuXL5MnT544y4waNYrhw4enVogiIiKJF3oDIkMTnh8RArfOmp+v/gsuHubn4FMQFgxR4fDvbMhaHE6tSfl4RSRJIqNs9JjRkbVHitrLZne0MCDJEBJzTZRQnaCgoATXm5aum9zdoUKF1N3mtWtw6pSZ0BERERERuZ90lVQBs0l8bNG9l91bHm3QoEEMHDjQPh0cHEyBAgVSLkAREZH4XP8PLu4BmzOcXg+7v0m+dQefTHzdfPXuPz8qHC7tgxKdErEyA64cgCKt4++aK/w25K0DntkTH19C3HyTZz0iqcQwDF4euIV5+w4B4OnpzOuf/Q8Kl7E4MskIEnNNFF+dhK6ZIG1dN+XNCxMnpu42ly+HoUNTd5siIiIikj6lq6RK7ty54zxddfHiRVxcXMiePf4bLe7u7ri7u6dGeCIikhndvgA7RsW0KLnX4XmpG0+0/PWhxnvmZxdPyFsbnJytiUUkE/rg/fWMn2gmVFxcnPj11y7Ualrc4qgkI0jMNVFCde5tvRKbrptERERERBInXSVVatWqxeLFix3KVq1aRdWqVeMdT0VERCRJoiIh+ATcOBGTgDi1Hv6dabaUCD4Fdy+BT15z3q1zybPdYu0Snnf5H8hfD5zczIHmc/1/fygh1yBbKTNhkrMCeOYAZ3dw1v+HIlZbuvQwH364wT49ZUp7WrZUQkWSR2KuiWrVqsXq1asdxlVZtWoVtWvXTtVYRUREREQyIkuTKrdu3eLo0ZiBZ48fP86ePXvIli0bBQsWZNCgQZw9e5apU6cC8MILL/Ddd98xcOBAnnvuObZu3crEiROZOXOmVV9BRETSu9AbsPUj+PPzxC/zsMkUrwDIU8NsNRIVAY89ATnKPty6RCTNatHiMZ57rjI//vgXX33VnO7dy1sdkqRhKXFN9Nprr1G/fn1Gjx5N+/btWbhwIWvWrGHTpk2p/v1ERERERDIaS5Mqu3btolGjRvbp6D58e/XqxZQpUzh//jynTp2yzy9SpAjLli3jjTfeYOzYseTNm5dvvvmGJ598MtVjFxGRdCwqAla/AOe3mmOCPAyf/OY4IiHXIPwW1BgMFV6Mv66HP7h6P3y8IpKuODs78cMPbXjqqTI0a1bM6nAkjUuJa6LatWsza9YshgwZwtChQylWrBizZ8+mRo0aqffFREREREQyKJsRPaphJhEcHIyfnx83btzA19fX6nBERCQ1XP3XHBz+5Go4Mv/B9bOVglyVIEtBczriLpTtDTn//2lzjU0iIveIijJwckp4EHCr6RxYkiqzHTPRA9XXqAFjx1odjYiIiIiktqSc/6arMVVERESS7M5lmFoeIsPuU8lmjlvSbj54Zk+10EQkYzh48BKdO89j2rQOVKiQ2+pwREREREREJAUpqSIiIunXnYtwah3cuQDHlkLQDgioGjP/7iW4tC/h5Su9CnU+AveM/wSuiKSM06dv0Lz5NE6fDqZ+/SmsXfsMVavmtTosERERERERSSFKqoiISPpgREHYLdj3A2wabI6LEp9TaxNeR9bHoM4IKNJSiRQReWRXrtyxJ1QAihXzp0QJtXYTERERERHJyJRUERER6xgGRIXHTIdeNwd+PzLfTJwAuHhBVFjCSZTEyl0N2swBv8KPth4REeDWrTBat57BwYOXAXjssWwsX94dX193iyMTERERERGRlKSkioiIpJ7L/5gJkzuXzBYlVw8+eJmIO/ef7+wGFV8G7zyQoxzkrg4u99zUdPV++JhFRO4RFhZJp05z2L79LAC5c/uwalUPAgJ8LI5MREREREREUpqSKiIikjoiw2BuE3MclKTKVRmc3c1kScg1yFbaHAvF/7Hkj1NE5D6iogx6917AypX/AeDn587KlT0oUsTf4shEREREREQkNSipIiIiKScqEk6sgIMzzG6+Ekqo+BYyX5HhcPlvKPGk2e1XlYFKnIhImmEYBq+/voKZM/8BwMPDhcWLn6Z8+QCLIxORjMIw4Px52LMHdu8233Plgm+/BScnq6MTEREREVBSRUREHkXINdg3Ae5egWv/wt3L4J3b7OLrfly9oe088MkL/iXAxSN14hUReQSbNp3i2293AODsbGPOnE7Uq1fI4qhEJL27eBG2boXt280kysV7nkE5fhzOnIGCBS0JT0RERETuoaSKiIg8WNAu2PEJHPnVTJrYnODWuYdcmQ2ajIUiLZI1RBGRlFavXiHGjWvFyy8vZ+LEdrRtW9LqkEQkHYqIMFuhbN0KmzfDf/85znd2hjJloFIlmDULwsKsiVNERERE4qekioiImIwoOLsFrh+FG8ch+Lj5fnaTY73bQUlfd47HofLrkPNx8MoNvgWSJWQRkdT24ovVaNq0KMWLZ7c6FBFJRyIiYMcOWLMGfv8dgoNj5tlsUK4c1KoFVapA2bLg8f+NeOfPV1JFREREJK1RUkVEJDO7/A+cWg9nN8DheUlbNksBwAa3zkDVt6BIS7MFi29h890rFzi7pUTUIiKp5tatMHx8HH/LlFARkcQwDDh4EBYsMJMpsRMp/v5Qpw7Urg01aoCfn2VhioiIiEgSKakiIpIZ3TgB616GY0sfXNfDHyLDICoCGn0NZXqYY6KIiGRwf/55jhYtpvP9963p1KmM1eGISDpx8yYsX24mUw4fjinPlg0aN4amTaFyZQ08LyIiIpJeKakiIpJRRYbDhV1waKbZjRfAsSUJ13fzhexlIGd5KNoG/IqYrU7cfFIlXBGRtOTIkSu0bDmdy5fv0LnzXJYu7UbLlsWtDktE0rCgIJgxw0ym3Lljlrm5mYmUdu2galUlUkREREQyAiVVRETSu6gI2DwMdowyu+S6eTppyzf4DAo0NpMpTs4pE6OISDpy7txNAgOncemSeVe0Tp2CNGxY2NqgRCTNOn0afvwRVqyAqCizrGhRePJJaNkSfH2tjU9EREREkpeSKiIi6ZERBVcOwMXdsO4VCL1hlicloZKtNAT+BPlqp0yMIiLp0LVrd2nefBonTlwHoHz5ABYvfhpPT1drAxORNOfSJfjpJ7NlSmSkWVa9OvToYQ46b7NZGp6IiIiIpBAlVURE0hMjCv78Ev546/713LNC6HXzc54aULSt2aWXqzf4FgJn3RwUEbnXnTvhtG07k3/+uQhAkSJZWbGiO1mzelgcmYikJWFhMHUqTJ4MoaFmWd260L8/lC5tbWwiIiIikvKUVBERSesiQiBoJ0SFw55xcOTX+Ot5ZIdn9kKWfKkbn4hIBhAeHknnznPZvNls8ZcrlzerVvUkT54sFkcmImnJrl0wahScPGlOly8Pr7wClSpZG5eIiIiIpB4lVURE0rKgnbDwCbh1Lv75voWh9nAo0MAcT8Wm0U9FRJIqKsrg2WcXs3TpEQCyZHFjxYruPPZYNosjE5G0IjQUvvoK5s41p7Nlg7fegmbNrOnmyzDg8GFYvhxWrgQPD5gzB1zVGFlEREQkxSmpIiKSFkSGmeOh3DoH57fD+a1wZP79l+m0Ggo1TZ34REQysP37LzJ79j8AuLs7s2jR01SqlMfiqEQkrThyBN57D44dM6c7dYKXXoIsFjRku3ABli0zkynR8US7cgVy5079mEREREQyGyVVRESsdmE3zG8Bdy4mXMczBzz+LERFmIPSl38OcldLvRhFRDKwxx8PYOXKHnToMJuJE9vRsGFhq0MSkTRixQr48ENzHJVs2czPNWumfhzbtpktZTZtgqgos8zNDerXh3XrYspEREREJOUpqSIiYqXrx2Ba5fvX8c4NXTaAf/HUiUlEJBNq0KAwx469pkHpRQQwkxQ//AATJ5rTtWvDBx+YiRUrjBkT87lyZWjTBho3Bh8fM7awMGviEhEREcmMlFQREUlNURFw+FdY2jX++XlqQolO4JYFspUC/xLgmROcnFM3ThGRDO748WsUKeLvUKaEiogARESYCZQVK8zpZ56Bl18GJwuGrvPxgVu3wNfXTKQ8+SQUKpT6cYiIiIhIDCVVRERS079zYFn3+OdlLwNd/gBnt9SNSUQkk9mw4SSBgb/w9tu1+fDDRtisGGVaRNKk8HAYNAjWrwdnZxg61ExmWOWzz+DMGahXD9zdrYtDRERERGIoqSIikpKiIuH6UVj7MpzdCJGhcesEVIVSXaHKG2Cz4BFIEZFMZO/eINq2nUloaCQjRmzkscey0atXRavDEpE04q+/zHdXV7PLrXr1rI2nVCnzJSIiIiJph5IqIiLJKfw2BO2EyDD4bxHsGZtw3dazoFSX1ItNRCSTO3bsGs2bTyM42Exwt2z5GN26PW5xVCKSFhhGzGc3N/jiC2sGpE9p587Brl0x47GIiIiISNIpqSIi8iiCT8Guz+DAVAi9kbhl/EtA4RZQsnPKxiYiInZBQbdo1uwXLly4DUCtWvmZO/cpXF01ZpWImC1Ton36acZKqEREwIYNMH8+bNtmlp07By+8YG1cIiIiIumVkioiIg/r74mw6tnE1fXJB5Vfh8efBY+sKRmViIjc48aNEFq0mMaxY9cAKFs2J0uWdMPbW2NYiYipVi144glo1Ajq1LE6muRx8SLMmwcLF8KVK47zbt2yJiYRERGRjEBJFRGRhxF8GlY951jmmRPuXgJXbyjZBdyygM0ZyveHbCWsiVNEJJMLCYmgXbtZ7N17AYCCBf1YubIH2bJ5WhyZiKQlPj4wZIjVUSSPw4dh2jRYuRIiI82ybNmgXTu4dAmWLrU2PhEREZH0TkkVEZGkiIqEowtgcSfH8vpjoNrbloQkIiLxi4iI4umnf2XDhpMA5MjhxapVPciXz9fiyEREkt/WrbB6NezYEVNWuTJ06QL165tdnI0bZ118IiIiIhmFkioiIkmx4xPYfM9jjBVeUEJFRCQNOnfuJjt2nAXAx8eN5cu7U7JkDoujEhFJXmFh5vvHH5vvTk7QtCn06AFlylgXl4iIiEhGpaSKiEhiXTkUN6ECUF6jfIqIpEUFC/qxeXNf2radyZdfNqdq1bxWhyQikux8fGLGSHn6afOVVz93IiIiIilGSRURkQcJvwtrX4L9kx3Ln1oLBRqBzWZNXCIi8kCFC2dlz57+ODs7WR2KiEiKGDwYzp6FJ54Af/9HW1dUFGzZAnfvQrNmyRKeiIiISIajpIqISDTDgHNb4eohuLQXrh+F0OtwbkvcuvnqKqEiIpIGbd58iho18uPiEpNEUUJFRDKywMBHX8edO7BoEcyaBWfOmGVlykC+fI++bhEREZGMRkkVERGAS3/DjJoQcefBdau+DfU/UUJFRCSNWbr0MO3bz6J16xLMmvUknp6uVockIpKmnT9vJlIWLIDbtx3n3TstIiIiIiY9ticimVf4bbiwG4J2wqy6D06oVHoFXroGDcaATT+fIiJpyebNp3jqqblERhosWvQvEyb8aXVIIiJp1vnzMHy42WXY9OlmAqVQIRg0CLJmtTo6ERERkbRNLVVEJHMJuwlHF5hdfG0fGX+dHI9DpZfN7sDy1gZXL8hSEJz1xLOISFr0998XaNNmJnfvRgDQpUtZXnmlhsVRiYikXX/8EfO5enXo3h1q1QInJ/jxR+viEhEREUkPlFQRkYwp7BYcngv/zoZT6yAqHLwC4M6F+y/n4Q/dtoOrZ+rEKSIij+TEies0bz6N69dDAGjWrChTp3bAyUldNIqI3Msz1ilu/frQrx+ULWtdPCIiIiLpkZIqIpLxGAbMqgOX9jmWJ5RQyVMDshQCz2xQ+Q0lVERE0omLF28TGPgL58/fAqB69XzMn98FNzdniyMTEUmbnnwSXFygRg0oUcLqaERERETSJyVVRCRjiQg1x0e5N6ESzdkNXDwhX10o2RUCKkP2Mqkbo4iIPLLg4FBatpzOkSNXAShZMjtLl3bDx8fN4shERNIuX1/o2dPqKERERETSNyVVRCRjOb0eLuyKmfYrAp3WgG8hc3B5m7qDERFJ70JDI+jQYTZ//XUegHz5srBqVU9y5PCyODIRERERERHJ6JysDkBEJNkE7YT5LR3L2i+ErEXByVkJFRGRDOLOnXBu3w4DwN/fg1WrelKwoJ/FUYmIZCxr1sCpU1ZHISIiIpL2qKWKiKRfQTvh2hHY8A7cOht3ftPvIefjqR+XiIikKH9/T9aseYZevRbw1lu1KFMmp9UhiYhkGDdvmu+TJsGuXea7iIiIiMRQUkVE0p+bZ2BOQ7j+X8J1claAEk+lWkgiIpK6fHzc+PXXzlaHISKS4QQExLRQuXgx6cvfuQNOTuDhkbxxiYiIiKQV6v5LRNIXw4D5rRJOqHhkh9Yz4Zk94Jk9VUMTEZGUM2fOfq5evWt1GCIiGd6IEdC+vfnZ0zPxywUFwZgx0LQpPPNMysQmIiIikhaopYqIpE1hNyFoF2BA+B34ZxK4eMKhGY71CjaBYu3BOwCKtQMXPRInIpLRzJjxN927z6dMmZysXNmD/Pl9rQ5JRCTDKlMGQkJg4cLE1T91CqZMgaVLITLSLDt2LMXCExEREbGckioikrbcuQiLO8OZPx5cN0tBeGpNysckIiKWWbHiKL16LQDgwIFLzJz5N2+/XcfaoEREhJMnYcIEWL0aoqLMsnLl4J9/rI1LREREJKUpqSIi1jEMOLnaHGjeKwBOr4eo8MQt6+wG7ealbHwiImKp7dvP8OSTc4iIMO/WPf98Zd56q7bFUYmIZG4XLpjJlMWLY5Ip9epB376QPz80a2ZtfCIiIiIpTUkVEbHOiZUwv+X965TuDr6FwIiCrI9BgUbgnhU8s6VKiCIiYo0DBy7RqtUM7twxk+0dO5Zm3LjW2Gw2iyMTEcmcrl+HyZNh7lwICzPL6teHF16AEiXM6WvXLAtPREREJNUoqSIi1ji37f4JlSoDocFnoJtnIiKZzqlTN2jefJp9YPpGjQozfXpHnJ2dLI5MRCTzCQuDWbPgp5/gzh2zrHJlePllKF/e2thERERErKCkioikrgu7YV4TCLnnMbaaw6Dya+DkAu4agFhEJLO6fPkOzZtP48yZYAAqV87DggVd8fDQaauISGq7cgU6d4YzZ8zpkiXhpZegVi09+yQiIiKZl65ORSR1XNwDv1SKf16JTlBrqJlQERGRTOv27TBat57BoUOXAXjssWwsX94dX193iyMTEcmcgoPNV/bs8Mor0KoVOKnRoIiIiGRyuoMpIskvKgL2T4UrB+D4Urh6KOG6LaZA2V6pFpqIiKRd7u4ulCmTkx07zpInjw+rVvUgVy5vq8MSEcl0/P3Nd1dX6N7dHITey8vamERERETSCiVVROTRXdwLf/8E14/Cmd8hIuTBy7SeBSWfApsedRMREZOLixOTJrWjYEFfOnUqQ5Ei/laHJCKSKRUpAlOmQI4ckDu31dGIiIiIpC1KqojIo7m0D36pmPj6raZBqW7qhFlEROJls9kYPryR1WGIiGR65cpZHYGIiIhI2qSkiogkXegNOLUerh6ETYMTrlewsTkAvWd2yF5WiRQREYnj66+30bRpUcqWzWV1KCIiIiIiIiIPpKSKiCRNVKQ54PyN43Hn5a4GdUZAtpKQpaCSKCIicl/jx+/i9ddX4u/vwdKl3ahVq4DVIYmIiIiIiIjclwYzEJGkuXUu/oRK3trQaQ0UDgTfQkqoiIjIfc2bd4ABA5YCcO1aCNu2nbE4IhERSSlRUWAYVkchIiIikjyUVBGRxDEMOLESfiwYU5azPDT8Avr8C09vBndf6+ITEZF0Y+3aY3TvPt9+g+2dd2rzxhu1rA1KRESSXVgYTJoEDRvCxx+nzjbPnoUDB1JnWyIiIpI5qfsvEUnYwRmweUj8LVPAbJ1S5Y3UjUlERNK1XbvO8cQTswkLiwSgT5+KfPJJU4ujEhGR5PbHH/D553DunDm9Z0/ilvvzT5g6FapVgx49Er+9w4dh8mRYs8Z8Hmz+fChY8MHLiYiIiCSVkioiEr8Lf8Ky7gnP98kLVd5MvXhERCTdO3z4Ci1bTufWrTAA2rUryYQJbbGpy0gRkQznzf+/VHBzM1usPMjhw/Ddd7Blizl97Fjikir79pmtYTZtciy/fl1JFREREUkZSqqISPwu7Utghg26bjRbqegmmIiIJNLZs8EEBv7C5ct3AKhXryCzZj2Ji4t6oxURyShiXx64ukLPnlChArz2WsLLnDsH48fD8uWO465ERSW8jGHAzp3w00/w119mmZMTNG1qll+79mjfQ0REROR+lFQREVNUBFz/D/5bBBvecZxX/nlz7BRXb2tiExGRdC0kJILmzadx8uQNAMqXD2DRoqfx9HS1ODIREUlOfn7Qrh2Eh8MLL0C+fDFJj3vdvm0mRWbOhIgIsyww0Bx/ZfDghLexdy+MHRuzXhcXaNMGnnnGbJnSoYOSKiIiIpKylFQRyezC78Cuz2DL+wnXKdhUCRUREXloHh4uPPdcZV5/fSVFimRlxYruZM3qYXVYIiKSzGw2GDbs/nWiomDZMvjmG7h61SyrXh1eeQVKl054kPlDh2DcuJjuwdzcoGNHM5mSK1f8y0REgLOzGtiLiIhI8lJSRSSz+2fy/RMqdUfBY+1TLx4REcmQXnutJrlz+1ClSl7y5MlidTgiImKBAwfg00/h77/N6YIFYeBAqFs34WVOnDCTKevWmdNOTvDEE9CvHwQEJLzcjz+ag963bAlDhz44ths3YNEiyJPH7EZMREREJCFKqohkdjeOOU4Xbg5FWkKJp8zB6EVERJJJly7lrA5BREQscuIE9Opljofi5QXPPgtdu5otTuITGgqffALz55utW2w2aNECnn8eChR48Pa2bjXf//nn/vWCgmD6dPjtNwgJMWNTUkVERETuR0kVkczq8K+wbwJcidW+vsNSKNrKuphERCRDMAyDQYPWUqtWftq3L2V1OCIikkYYBrRqZXb1lTPn/etevw7z5pmf69eHl16CYsUevI1cueD0aXM8l7NnE6537BhMnQrLl0NkZEx5aOiDtyEiIiKZm5IqIplRZDis7AthwY7lXgl0RiwiIpIEo0dvZvTozTg52fjpp7b06VPJ6pBERMQihQqBj4+Z7Bg0CCo94L8Ej1hDbpUuDa+/DlWqJH57n30Gly7BlSvw4otx5x88CD/9BH/8EVNWrZo52P379+kVWURERCSakioimVFUeNyESqFmEFDZmnhERCTD+Omnvxg0aC0AUVEGhmFxQCIiYqns2WHVKnB1TdyA8UWKwNtvQ7Zs0KSJOYZKUmTJYr6uXHEsP3wYxo+HDRtiyho1MrskK1fOTMQ8CsOA48fNJJKz86OtS0RERNI2JVVEMrv8DeDJleDibnUkIiKSzi1YcIj+/ZfYpz/5pAl9+6qViohIZpfQuCnxsdmgS5fk2/bNm/DOO44D3bdoAX36mAmcexkGfPMN7N5tDnBftOj91x8WBitWwLRpZpdi3brBwIHJF7+IiIikPUqqiGQ2hgHbP46ZdnJWQkVERB7ZH3+coGvXeURFmU1TBg6syTvv1LE4KhERyewuXjQTKjYbBAbCc89B4cIJ14+KMsdaAdi0KeGkSnCwOebL7NmOrWLOn0+20EVERCSNUlJFJDMwDLiyH0JvwB9vwvntMfOckvDYmIiISDz27AmiXbtZhIaaI/327FmeTz8NxJaYfl5ERERSgK9vzOdmzcxkyv1anXh5md12RUaaXZWFh8df7+xZmDEDFi6EkBCzLFcuM1GzY0eyhS8iIiJpmJIqIhlZaDCcXAWLn0q4Tvn+qRePiIhkOP/9d5UWLaYRHBwKQKtWxZk4sR1OTkqoiIiIdUqUgG+/hYCAB3fhBeDtDT/+aD6P9ttvsGSJ4/zjx2HSJFi50mzNEr2NHj3MpM3ixTFJFcOAnTvNddSvD02bJu93ExEREWspqSKSUV34C6ZVuX+dbtsgT43UiUdERDKciIgo2radyYULtwGoXbsAc+c+haurRugVERFr2WxQq1bSlilf3nxfsCCm7MgRmDgR1q41kyUAtWubyZRq1cztxHbsGHTvDocPm9PHjyupIiIiktEoqSKSEZ3dDLPqxj+vwgvgnQcqvQIe/qkbl4iIZCguLk58801LnnhiFoULZ2Xx4qfx8nK1OiwREZFkMWuWOWh9tMaNoV8/KFky4WVOnnScjoxMmdhERETEOkqqiGREO0Y7TrtnhRZToGgbc2B6ERGRZNK0aVF+/703efL4kC2bp9XhiIiIJJuLF82WKM2aQd++8NhjCdfNm9d8z5YNunaF/Plh8ODUiVNERERSl5IqIhlR+M2YzyWegjazwOZkXTwiIpJhGIYRZwD6qlXzWhSNiIhI8qtUyRw7pWlTM5lSuPCDl6lZExYtghw5wM0Ntm9P8TBFRETEIkqqiGQ0Idfh9O8x0y2nKqEiIiLJwjAMnn12EYULZ2XIkPpxkisiIiIZQbt20LZt3PFSHiRvPM8YHD4Mly5BzpwJLxcSAqdPmy1h9F+rpKSwMHB11XEmIvKolFQRyUjC78JYjZMiIiIpY9CgtUyatAeAGzdC+eyzQGsDEhERSSGPetPZzS3m88aN0LFj3Do3bsDs2ebrxg0YM8Yct0UkuYSFwd9/w86dsGsX/PMPFCxojhfkpGcvRUQempIqIhlF+F2YWMyxLEc5cHa3Jh4REclQPv98C6NHbwbMG001a+a3OCIREZG06/HHYz6HhTnOO3cOpk+HhQvNVirRgoJSJzbJuMLD4cCBmCTKvn1xj79jx+DWLfD1tSZGEZGMQEkVkYzi4m64fd6xrPMfatcrIiKP7Oef9/DWW6vt0+PGtaZTpzIWRiQiIpK2ubhAq1awbJl5oxvMrsCmToVVqyAqyiwrWRIiI+HoUetiTWsMw9xXa9aYN/579rQ6orQrMhIOHjQTKLt2wZ49jok6MMf5qVoVKleGkSPNsuhjUkREHo6SKiIZhuE4+UIQeGazJhQREckwliw5TL9+i+zTH37YkBdeqGphRCIiIumDq6v5fugQvP46bNoUM696dejdG6pVg6FDk55UOX0aVqyA2rWhbNmE60VFmdvdsAE6d4YSJZL6LR7NnTuwdi2sXg3FisFrryVc99Qp8zutXAknT8aUP/EEZMmSsnFGRZkJiY0bze6xOnRInnVGj6eTXF1tGQacOQPbtpmvXbvg9m3HOv7+UKWKmUipVs38PjabGU90UmXDhuT5jiIimZWSKiIZwZ1L8FvrmOkqb4J3gHXxiIhIhrBp0ymeemoukZFm4v7ll6sxZEh9i6MSERFJH6KTKitXmu9OTtC0KTzzDJQqFVPP+P/n4yZPhqefvn9nA/v2wS+/wO+/m8v9+SeMHx+3XnAwLFoEc+aY3Y2BeVN92DDHeoZhvhJz0z842GyB4+V1/3pRUebN/iVLYN26mJYTO3bETapcvGi23Fmxwkw+RXNzi+m2KiLC/Lxtm7mONm0c99/DCguD7dth/XozyXD9ulnu4uKYcDAMOH/ebPERe6yc+Ny5Y65z40YzmXX1qtnS5n7JpAe5edPcn1u3mvsg+t8zmq+v2QolOolSpEj8/56xy86fN4+l0qVjjlMREUk8JVVE0jPDgDN/wJxGjuVuPtbEIyIiGca+fRdo02YGISERADz9dDm+/rolNnUrKSIikijRY1Y4O0Pr1mbLlIIF49Zzdjbfr10zx1XJk8dxflSUmUT55Rdz0PHYQkMdp//7zxz4ftmyuN1ARUTEfA4OhsWLYe5c8/OcOWbS4F5RUeaN/F9/NRMFhQqZy8TnxAlYutR8XbwYUx4QABcuxCSPgoPNrr1WrIDdu2PKnZygZk1o3hwaNoQGDczy0aPNGG7dMqevXIFRo+KP4UFu3YLNm81EypYtZhIkmpeXOR0RYXartWcP/PGHue/PnYMWLWDEiLjrPHfOTKBs3GgmP+7tWuu//8z3qKjEJa8iI2H//pjWKP/8E9NdHJhJnwoVzH1Vs6bZhVxiW8IEBppJrEmTzNfbb0OXLolbVkREYiipIpJehVw3B6YPuRp3XtneqR2NiIhkIIZh0Lv3Am7cMO/UBAYWY8qUJ3ByUkJFREQksXr0MLt+qlMH8uZNuF6HDmYSBBxvyN+9ayY+pk+Hs2fNMldXc6yWAgXgu+/Msqgos6XFrFnmTf1oxYtD165mF1TRrVn+/ddMoKxY4ZiQOXXKMaly6ZLZ0mXBArNVQ7TY3XKBmaRYudKsu39/THmWLOYN/DZtzKRKq1ZmnO+8YyYfYn/PSpXMREqTJmbXVfdas8Z8d3ExEx5JHQ/k+nUzibJ+vdnSJXZyKVcuM4HTqJHZwqNFC7O8WTMz+RPb6dPme1SUmejYsMH8LtFJk2h580L9+ma9OXPMgeGfe85cpmdPGDAgboyXL5tJnk2bzBijE0jRCheOSaJUrvzg1kIJufc4PHvW/DfNlQs8PR9unSIimZGSKiLpUfgdmFQ8bkLFIxs8ewzc/ayJS0REMgSbzcbcuU8RGDiNHDm8+PXXzri5OVsdloiISLri6wtPPfXgepUqgY9PzI30mzfN1iAzZsR0SRW9rs6dIXt2swUFmDfFn3wy5oa/k5OZJOja1VyvzQbTppnz1qyJSd6AmXQJCjK3BzGtUubPNxMG0a0jfH3NViOLF5vThmG2mPntN3OslOgWMU5OZgKpTRuoVy+mq6zoViuGYXYHBubYLi1amImX3Lnj3y9VqsDx49C4sVnv+HGzhcrhw9CvH1SsCK+8Ev+yN2+aSZTVq83uuGK39Chc2EyiNGpkdn8V3Qg3ej+AmVDx9TWTI76+5r/FlSswfLiZSIn+d4n+3hUqmN+5Xj1z/Tabua/nzDH3cVCQWXf7djOpEhUFBw6YSZRNmxy7Pove59WrxyRSEtpHSfXcc+a/5ZIlZuujGTPMV+XKMGFC8mxDRCQzUFJFJD06NBPuXnYsazYByvQEFw9rYhIRkQylWLFsbN7cFxcXJ3x8HtCBuIiIiCSLKVPMgd2jBx/Pl89s8dKmTfwtCa5eNV++vmaLl6eeinsDPrprqLAws6uxJk3M5EyFCub7zZtmi5QPPnAcr6NiRejY0ax/65aZVImKMruLOnYspl6RIuZg8i1bQrZscWPMls3s9uzOHbNOq1ZmQudBfvjBcfrECfP93Dnzdfy4Y1Llzh0z2bRqlTn+SOwWKSVLmt8jukVKfLJkMdd37ZqZTKlQwdxfGzeaiYegoJjEUpYsULu2mUSpXTumq7fYatSAcuXMedmymYmMy5fNcW22bjW3E1uZMlC3rpmYKl06+Qa3j83dHR5/HPbudSyP3rciIpI4SqqIpDdnNsGqZ2Ombc4w4BJ4xNNOWkREJJFCQyNwcXHC2TnmCj53bo3RJSIikpoWLTLfixaFPn3MFhrO8TQWLVTIbAmSMyd0724mXRLqEqpBA9i507xp36FD/GOnRLdgyZLFHP+lY0czhmixu6M6dsy8OR8YaK7v8cdjWnvEx8XFbBXxqMOyPf64mZzIlg2OHjXLQkLMpMfq1WaLj+jB7cGMv3lzsyuv+MayiU+vXnHLihYFb28zORI91kvFiub3up/s2c0kGZgtf5YsMceWid7X3t5mK5ToREp8CamU0rGjGV94OHz4oZmYa9rU/Fy7durFISKSXimpIpKehFyDxZ0cywJ/UkJFREQeSWRkFN26zcfJyca0aR1wd9cpooiISGry9zcTF6VLQ9++5o37+7VUKFzYbNHi7v7gFg358sGXX8Y/LyDAbPFRsqTZAqV5c3Od8cVXrZrZgqZdO7PrLp8kPHvxqAkVMFu3rFpltqro1MnsoqtZM3PsmWgFC5rJnsBAx6TQo8iXz+xKzGZ7+O9Rrpy5Hjc3M4lSt67ZEuZBiZmU4uVlthq6cCGm7Pp1swVNzZpmF2vbt5uvU6dg8GAlW0REYtMVs0h6su0juBPrrOfx56DsM9bFIyIi6Z5hGAwYsJT58w8CEBYWycKFXS2OSkREJHMZO9bsGupBrT5iS46BxUePNsc8iR4HJCFOTvD994++veQQu+XO3bvm4OvNmpkJoeLFkyeBc69H7YorWzZYuDB5YklOAQHw+efwySdw6ZKZqFu2DG7ccKy3YYOZbDlyxEy07NhhDnD/7rtmgkhEJLNRUkUkPbh5Fn5/Aw7PdSyv+zHYUqCjVRERyTSGDVvPhAl/AeDi4sSAAVUtjkhERCTzyZvXfKU2b++ExxhJq/Llg27dzPFdmjc3W4GkRCIls2jQAP791xyo/uJFs8zLC6pWNVsm/fmnmVRZs8ZszRLb779DrVpw4ICZaNm+3Ryj56uvzISNiEhGpaSKSFoWEQob3oHd38Sd1/8ceOVM/ZhERCTD+Oab7YwYsdE+PXXqEzRv/piFEYmIiIjcn5MTDBxodRQZS5s2cPasmdirUcNMVLm4wKRJZlIldrKlShWzhdCuXTHJlthj7gBs22YmvHbvNpMt+/dD27bmS0QkI1BSRSQt+29h/AmV2sPBJ0/qxyMiIhnGjBl/89prK+zTX3/dgqefftzCiERERETECnnzwvDhcctbt4YzZ8xWJ7GTLb/8YiZVrl416/n6mi1bjh41x2D58UezS7Hw8Jh13bplJlUuXoSdO81WLfv3Q/v28Ix6NReRdEZJFZG0KuwmbB7mWFZ3JFR/V11+iYjII1mx4ii9ei2wTw8ZUo9XX61hXUAiIiIikuYEBMCwYXHLW7eG8+chZ04z2VKqlNmC6H//M5MqQUExy+fPb7Z2OXMGnnzSHIsltkWLlFQRkfRHSRWRtOjWefjhng51W/ysQelFROSRbdt2hiefnENERBQA/ftX4cMPG1kclYiIiIikF9mywTvvxC3v3RuyZoXixaFaNShQwOwC7Pnn4c4dM6Hi5GQmYfLkgbVrUztyEZHkoaSKSFq0pHPcsoJNUj8OERHJcN5//3fu3DH7YnjyydKMHdsKm0Z3FREREZFHVKqU2VoltgoVoPP/3+KoVs0ck8XXF/76S0kVEUm/lFQRSWvCbsLZTY5lA66AZzZr4hERkQxl7tyn6NBhNgDTp3fE2VldSoqIiIhIynB2jr9Vi4hIeqakikhaEhkOk8s4lg2M1BgqIiKSbHx93Vm2rBthYZG4u+tUUERERERERCQpdCUtkpZc/w9unYmZLtZOCRUREXkkt26FERERRdasHvYyd3cXJVREREREJE2IijLHWhERSS/0kyWSlgVOtDoCERFJx0JDI+jYcTYNGkzh/PmbVocjIiIiIuLg5EmoXRuWLLE6EhGRxFNSRSStKtsbvHJYHYWIiKRTkZFR9Oq1gNWrj7Fv3wXatp2JYRhWhyUiIiIigq+v+W4YEBEBGzdaG4+ISFIoqSIiIiKSwRiGwWuvrWD27P0AeHq68PXXLbDZbBZHJiIiIiICjz0Go0ZBp07m9Nq1cesYBhw+DFOmwJgxcO1aqoYoIpIgdaYtIiIiksF89NEGxo7dCYCzs425c5+iTp2CFkclIiIiIhKjWTMoVgzmzTOng4PNRMr27bBlC2zdCleuxNQvWBC6drUmVhGR2JRUEUlToqwOQERE0rnvv9/J++//bp+eNKk9rVuXsC4gEREREZEEFC0a8/mFF+DIETOxEs3TE9zd4fp1+Owzc/yVgnpWSEQspu6/RNKCoF0wNgdMKWt1JCIiko7NmbOfl15aZp/+/PNAnnmmgoURiYiIiIjcn7Oz+X74sJlQKV4cnnkGvv/e7BasYcOYulOnWhKiiIgDtVQRsdrNMzC9Wtxy79ypH4uIiKRba9Yco0eP+fYn+/73vzoMHFjL2qBERERERB5g4ED45x+oVg1q1oRcuRznd+wICxaYnxcsMJMsdeumcpAiIrGopYqI1Vb0jltWrh9Ufi3VQxERkfRrzpz9hIeb3Uj27VuRkSObWByRiIiIiMiDdekCH30E7drFTagAlCkDb70VM/3RR6kXm4hIfNRSRSS1XfgTlnSF4JPmdFR4zLxspaD3frAp3ykiIkkzfnwbPDxcOHXqBj/80BabzWZ1SCIiIiIiyaJRI5gzB06dglu3rI5GRDI7JVVEUtve8XD9aPzzeu5RQkVERB6Kk5ONr79uQUREFC4u+r9ERERERDKOgAD4+mvo0AFcYt3NDAmBP/+E3bvNQewrV7YuRhHJPJRUEUktm4fCthGOZS6ekK00uGWBGoPBxd2a2EREJN25evUuly/foUSJ7PYym82Gq6uzhVGJiIiIiKSsyEiz1cqmTbBrF4SFmeWbN8PMmdbGJiKZg5IqIqnh7lXY9rFjmc0Jnj0O3gHWxCQiIunW7dthtGkzgyNHrrJsWTeqVctndUgiIiIiIqkiJATGjImZ9vODGzfg5Eno3Bk8POCnn8DNzZwfHg579phJl/374amnIDDQktBFJINQUkUkNUTcBQzHsmYTlFAREZEkCw+P5Kmn5rJ16xkAunWbz8GDL6nLLxERERHJ0HLmBH9/M4FSsSLUrQt16oCnpznIfVgYHDtm1t26Fa5dMxMp27fDnTsx67HZoGZNyJLF/CwiklRKqoiktuJPQrt5VkchIiLpUFSUQd++i1i+3Byby9fXnblzn1JCRUREREQyPA8PWLrUTJ74+DjOe/ZZuHgR1q0zB7J/803H+dmyQb588Pff5vgrjRtDly7w9tupF7+IZBxKqoiIiIikA4Zh8OabK5k2bR8A7u7OLFrUlYoVc1scmYiIiIhI6nBzi+nWK7YXXjDfjx2Df/4xW6CUK2e2ZKlTB0qWhL/+MusZ/9+RyKFDqRe3iGQsSqqIiIiIpAOffLKJr77aDoCTk41ZszrRoEFha4MSEREREUlDRo6EI0egQgXImtVxXpUq8OGHcPBg/APaR0WBkxqAi0gi6KdCREREJI376ae/GDx4nX16woQ2PPFEKQsjEhERERFJe/LmhQYN4iZUwGy90qoV1KplTt++bXYH9v330K2bOUbL8uWpGq6IpFNqqSIiIiKShv3220H6919in/7kkyb061fZwohERERERNKvnDnN96NHoU8fx3l//gktW8a/XGgo7NoFV6+adVx0V1Uk09Kfv4iIiEgadvz4daKizI6fBw6syTvv1LE4IhERERGR9CtfPvD2NluqeHubLVfu3oXNm2HBAhgyJKZuUBBs2mS+du40EysAXl7QpIkl4YtIGqCkioiIiEgaNnBgLfz9Pdi48RSffhqIzWazOiQRERERkXTLywt+/hmuXzcHs3dxgdmzzaSKzQZ798LGjWYi5ejR+Nfx7rvw5ZdQr16qhi4iaYSSKiIiIiJpXJ8+lejTp5LVYYiIiIiIZAiFCztO16sHn34KhgH9+sWUOznB44+b463Uqwc//QRr1pjzJkyAkiXNpEy2bKkWuoikAUqqiIiIiKQh58/f5J9/LtKsWTGrQxERERERyRR8fcHDA0JCzM+1aplJlFq1wM8vpl7XrrB9O9y8CYcOmQPfe3jA0qWO9UQkY1NSRURERCSNuH49hBYtpnPgwCWmTGlP9+7lrQ5JRERERCTD8/GBX34xkyVly4Kzc/z1KlaEceOgZ0+zVQuYiZgLF5RUEclMnKwOQERERETg7t1w2radyb59F4iIiGLo0PXcvRtudVgiIiIiIplCkSJQvnzCCZVopUrBRx+ZL29vsyy6S7D7iYqCv/82kzIvvghr1z56zCJiDbVUEREREbFYREQUXbrMY9OmUwDkzOnFypU98PR0tTgyERERERGJzWaDli3Nz0OHmu/LlsETT0DevI51b92CbdvMge+3bIFr12LmXb8OOXJAsWJmSxkRST/UUkVERETEQoZh8Nxzi1m8+DAAWbK4sWJFD4oXz25xZCIiIiIicj/du5vvQUHw4Yfm51OnYMYMszVKkybwv/+ZY65cu2YmT6pXN+sdOQL9+sGYMdbELiIPTy1VRFLSjRPw19fw11dWRyIiImnUu++uYcqUPQC4uTmzYEFXKlfOY21QIiIiIiLyQDVqwPTp5uf9+6FjRzOpElvhwuag93XrQoUKEBYGbdvCjRvm/GXLoGZNc9B7EUkflFQRSQlREbCoE/y3MO68rI+lfjwiIpImffrpZj79dAsATk42ZszoSOPGRSyOSkREREREEqN2bfjpJ3j2Wbh710youLhA5coxiZQCBRyXcXGB2bNh1Sr44guz7IMPzGX27wd/f/OziKRdSqqIJDfDgJl1IGhH3Hnl+kLt4akfk4iIpDk//7yHd96JGdHy++9b8+STZSyMSEREREREkurxx6FdO/Nz3bpmqxMvr/svkyOHOQbLrl2wYYM5iH3btuYtJRcXWLkSrlyBTZvM8Vj+/Rdef91sCWMYZtdhHh5QsGBKfzsRiY+SKiLJ7e6VuAmVyq9DjUHglcuSkEREJO3JlcsbT08X7t6NYMSIRjz/fBWrQxIRERERkSRydoZhw5K+nJeX2VLlySfh5EkzWQIQEQFPPQVXrzrWnzcPDhyAzZvh0iVwc4Ply8HP79G/g4gkjZIqIsnOcJzsdxSyFrMmFBERSbNatizO2rXPsHTpEQYPrmd1OCIiIiIiYoFRo+DQIahSBZ55xhxr5epVM2lStarZcmXDBjh82HxFCwszW7MoqSKS+pysDmDcuHEUKVIEDw8PqlSpwsaNG+9bf/r06VSoUAEvLy/y5MlDnz59uHLlSipFK/IARhRsei9mumhbJVRERCRBtWoVYMSIxthsNqtDERERERERC5QoYXYfli8fDBoEXbuaLVjWroVvvoEXXzRbteTPb8777jvIksXqqEUyN0uTKrNnz+b111/nvffeY/fu3dSrV4+WLVty6tSpeOtv2rSJZ555hn79+rF//37mzp3Lzp07efbZZ1M5cpF4REXA+tfh7x9jylzcLQtHRETSluPHr/Hdd/GMtyUiIiIiIgI0bQpvvQX164Onp1lWvDj88Qf89ps5r2ZNcPr/O7qDB0OfPnD7tnUxi2RGliZVvvjiC/r168ezzz5L6dKl+eqrryhQoADff/99vPW3bdtG4cKFefXVVylSpAh169alf//+7Nq1K5UjF4nHP5Nh97eOZY8/Z00sIiKSply4cIvAwGm88spy3n57FYZhPHghERERERERwGYzX9Gcnc33o0fh779h3Dj4+mtzbBYRSXmWJVXCwsL4888/CQwMdCgPDAxky5Yt8S5Tu3Ztzpw5w7JlyzAMgwsXLjBv3jxat26d4HZCQ0MJDg52eImkiEv7HKef3gqFA+OvKyIimUZwcCgtW07n6FFzpMmlS49w82aYxVGJiIiIiEh6NWAANG4M2bKZ07Nnwy+/wOTJEBVlbWwimYFlSZXLly8TGRlJQECAQ3lAQABBQUHxLlO7dm2mT59Oly5dcHNzI3fu3GTNmpVvv/023voAo0aNws/Pz/4qUKBAsn4PEQAOTIM938VMt5oOeWtaF4+IiKQJISERtG8/i927zXObAgV8WbmyB76+6h5SREREREQezhNPwJgx0KiROe3jY74vWQKtWsGdO2AY5sD2q1eb0yKSfCwfqP7egVkNw0hwsNYDBw7w6quvMmzYMP78809WrFjB8ePHeeGFFxJc/6BBg7hx44b9dfr06WSNXwSAbR85TudRQkVEJLOLjIyiW7df+f33EwBkz+7JqlU9KVDAz9rAREREREQkQ/jf/2DDBnNslWiXL8OwYdCmDXTrBoMGwbx51sUokhG5WLXhHDly4OzsHKdVysWLF+O0Xok2atQo6tSpw9tvvw1A+fLl8fb2pl69eowYMYI8efLEWcbd3R13dz0NKiloRR+4djhmusFnkLWodfGIiIjlDMPgxReX8ttvhwDw9nZl2bLulCqVw+LIREREREQko7DZwMvL7Ars00/h/2+Z8vvvjvW++cas16lTqocokiFZ1lLFzc2NKlWqsHr1aofy1atXU7t27XiXuXPnDk5OjiE7///ITBrwVSwRfhv2T4mZ9isCVd+0LBwREUkbhgxZx48//gWAq6sT8+d3oXr1fBZHJSIiIiIiGZGLi9kVWJ8+ULAgdOwIX30FffvG1Jk1y7LwRDIcy1qqAAwcOJCePXtStWpVatWqxYQJEzh16pS9O69BgwZx9uxZpk6dCkDbtm157rnn+P7772nevDnnz5/n9ddfp3r16uTNm9fKryKZUVQkLO3uWNZ8sjWxiIhImvH99zsZOXITYD459ssvHQgMLGZxVCIiIiIiktG99JL5ilamDPz9N+zcCZGR1sUlktFYmlTp0qULV65c4cMPP+T8+fOUK1eOZcuWUahQIQDOnz/PqVOn7PV79+7NzZs3+e6773jzzTfJmjUrjRs3ZvTo0VZ9BcnMTq2F/xbGTBd/Ego0sC4eERFJExo1KkLBgn6cOnWDb75pSZcu5awOSUREREREMqFs2aB/fzOpcvo03L0Lnp5WRyWS/tmMTNZvVnBwMH5+fty4cQNfX1+rw5H06sJfMK2KY9nTWyBvLWviERGRNOXMmWAWLfqXAQOqWR2KCKBzYEk6HTMiIiIZw5498Oyz5ufGjWHMGEvDEUmzknL+a9mYKiLp2uG5jtOBE5VQERERu/z5fZVQERERERERy8UeMWHzZuviEMlIlFQRSaywm7CoE4zNAbu/iynPVw9KdbUuLhERsdT+/Rd54YUlhIWpk2IREREREUlbcuWCkSPNz6GhEBYG585ZG5NIemfpmCoi6UboDZhQwEys3Kvxt+DqlfoxiYiI5U6evE7z5tM4e/Ymx49f59dfO+Pj42Z1WCIiIiIiInYVKsR8bt4cbt6E4cOhdWvrYhJJz9RSRSQxzu+Im1DxyQ8VXoCc5a2JSURELHXp0m0CA82ECsDVq3fJZEPViYiIiIhIOuDnF/P55v/f3vrxR2jVCn75xZqYRNIztVQRSZRYN8lcvOD5U+CZ3bpwRETEUjdvhtKq1QwOH74CQIkS2Vm2rBtZsrhbHJmIiIiIiIgjDw948024dAmOHoUtW+DMGXPe119D/vzQqFFM/bAw2L4dTp+GDh3A09OauEXSKiVVRB4k/A7sGBUzXfUtJVRERDKx0NAIOnSYza5dZkfEefNmYdWqHuTM6W1xZCIiIiIiIvF7+mnzfe9euH0b3Nxg506z7O23YeZMM+Hy++9m0uXuXXOejw+0a2dJyCJplpIqIg9ycBqc/j1m2snZqkhERMRikZFR9Oz5G2vXHgfA39+DVat6UKhQVmsDExERERERSYQKFWDiRDhxAp55Bu7cMcujky732rVLSRWRe2lMFZEHCT7lOF1M/5OIiGRGhmHw8svLmDv3AACeni4sWdKNsmVzWRyZiIiIiIhI0hQuDBs2QJ48MWVFi0LfvjB1KhQrZpYdPAirV0NIiCVhiqRJaqkikhTtF0CuilZHISIiFvj++12MH/8nAM7ONubN60zt2gUsjkpEREREROThDR8Ohw9D7dpQsGBMeYMG8N9/cPw4DBoEAwaYCRcRUUsVkfsLuQ57vouZdvWxLBQREbFWt26PU6+eeZUxeXJ7WrUqbnFEIiIiIiIij6ZyZeja1TGhAlCqlOP0uHFgGKkXl0hapqSKSEKCT8NYfwi9YXUkIiKSBmTN6sHKlT1YsKALPXtWsDocEclAxo0bR5EiRfDw8KBKlSps3LjxvvXHjh1L6dKl8fT0pGTJkkydOjVOna+++oqSJUvi6elJgQIFeOONNwhRvx0iIiKSSI0amd2Ade0aU7Z8OYSGWheTSFqh7r9E4mMYML+FY5mrD+SuZk08IiJiCcMwsNls9mlPT1faty91nyVERJJm9uzZvP7664wbN446derwww8/0LJlSw4cOEDBex8ZBb7//nsGDRrEjz/+SLVq1dixYwfPPfcc/v7+tG3bFoDp06fzv//9j0mTJlG7dm0OHz5M7969Afjyyy9T8+uJiIhIOmWzQZkyEBAAs2aZZcOGwT//wDvvWBubiNXUUkUkIVcOOE4/fxrcfa2JRUREUt3OnWdp1OhnLl68bXUoIpKBffHFF/Tr149nn32W0qVL89VXX1GgQAG+//77eOv/8ssv9O/fny5dulC0aFG6du1Kv379GD16tL3O1q1bqVOnDt26daNw4cIEBgby9NNPs2vXrtT6WiIiIpJBZM8ORYrETP/5p3WxiKQVSqqIJMZrd8Ejq9VRiIhIKjl06DItW07njz9OUrfuJM6eDbY6JBHJgMLCwvjzzz8JDAx0KA8MDGTLli3xLhMaGoqHh4dDmaenJzt27CA8PByAunXr8ueff7Jjxw4Ajh07xrJly2jdunWCsYSGhhIcHOzwEhEREQGYMAFa/H+HLsePWxuLSFqgpIrIg+SpBS4eD64nIiIZwpkzwQQG/sKVK3cByJs3C9mze1kclYhkRJcvXyYyMpKAgACH8oCAAIKCguJdpnnz5vz000/8+eefGIbBrl27mDRpEuHh4Vy+fBmArl278tFHH1G3bl1cXV0pVqwYjRo14n//+1+CsYwaNQo/Pz/7q0CBAsn3RUVERCRd8/eHWrXMz1FRMeUhIbBuHbz3HgQGwuTJ1sQnkto0poqIiIjI/7ty5Q6Bgb9w+rT5hHbFirlZuLArHh46ZRKRlBN77CaIO55TbEOHDiUoKIiaNWtiGAYBAQH07t2bMWPG4OzsDMDvv//Oxx9/zLhx46hRowZHjx7ltddeI0+ePAwdOjTe9Q4aNIiBAwfap4ODg5VYERERETtX15jPq1bB2rWwebOZWIk2diy0bQs5cqR+fCKpSS1VRERERIDbt8No02YmBw+aT3oXK+bPihXd8fNTa0URSRk5cuTA2dk5TquUixcvxmm9Es3T05NJkyZx584dTpw4walTpyhcuDBZsmQhx//fwRg6dCg9e/bk2Wef5fHHH6dDhw6MHDmSUaNGERX78dJY3N3d8fX1dXiJiIiIRCtcOObz4MFmUiUkBPLmherVY+Z98QXs2gVjxsDzz8OxY2Z5VBQYRqqGLJJi9NiliIiIZHphYZF06jSXbdvOAJA7tw+rVvUkIMDH4shEJCNzc3OjSpUqrF69mg4dOtjLV69eTfv27e+7rKurK/nz5wdg1qxZtGnTBicn85m5O3fu2D9Hc3Z2xjAMDN3NEBERkYeQLRvYbGZipGBBaNIEGjeGUqXgwgVo08ast2qV+Yr25ZfmMjt3QpcuEKthrEi6paSKSLSr/8K6V+DkaqsjERGRVBQVZdCnz0JWrDgKgJ+fOytWdKdoUX+LIxORzGDgwIH07NmTqlWrUqtWLSZMmMCpU6d44YUXALNbrrNnzzJ16lQADh8+zI4dO6hRowbXrl3jiy++4J9//uHnn3+2r7Nt27Z88cUXVKpUyd7919ChQ2nXrp29izARERGRpMiRA2bMACcnKFrUTLBEy50bRo2CQYPMaV9f8PCAixdh69aYejNmwGuvgU5HJL1TUkUk2sZB8SdUPLOnfiwiIpJqpk7dy4wZfwPg4eHC4sVPU6FCboujEpHMokuXLly5coUPP/yQ8+fPU65cOZYtW0ahQoUAOH/+PKdOnbLXj4yM5PPPP+fff//F1dWVRo0asWXLFgrH6pNjyJAh2Gw2hgwZwtmzZ8mZMydt27bl448/Tu2vJyIiIhlI8eIJz2vY0BywPl8+qFwZFi2CkSOhdGnInx9W//8tt40bzboi6ZnNyGTtv4ODg/Hz8+PGjRvqJ1hM147CjOoQci3uvOxloOVUCKiS+nGJiEiqiIyM4qWXlvHTT38xf34X2rUraXVIIslO58CSVDpmRERE5FGFhYGbG9y4YXYXBvDBBzFdhYmkJUk5/1VLFcncQq7B1PIQcTemzMkFXr0Dzq7WxSUiIqnG2dmJ779vzfPPV6Fy5TxWhyMiIiIiIpIhuLmZ735+ULs2bNkCBw+ag9a3bAmuuvUm6ZSSKpJ5hd+BvT84JlQA2s5TQkVEJIMLDY3A3T3mNMhmsymhIiIiIiIiksJmzzbfQ0KgRQtz/BWR9MbJ6gBELHH4V/jGGzYNiinL+hi8HgaPtbcuLhERSXHr1x+nRInv2L37vNWhiIiIiIiIZAq5cjlOjxkDXbtCRITZckUkPVFSRTKn/T/HLas5VC1UREQyuL/+Ok/79rM4deoGDRv+zD//XLQ6JBERERERkQzv7bdh6lRo1iym7OJFaNwYnn4aMteo35LeKakimZMREfM5S0HosBRKd7cuHhERSXFHjlyhRYtp3LwZBkC9egUpWTK7xVGJiIiIiIhkfO7uUKaMmVwZOjSm/M4d+O8/6N4dZsywLj6RpNCYKiLP7AEPf6ujEBGRFHTu3E0CA6dx6dIdAOrUKcCcOU/h6upscWQiIiIiIiKZR7Zs0L493L4N58/DzJlm+eHDMGUKbNsGAQHw3nuWhilyX0qqiIiISIZ27dpdmjefxokT1wEoVy4Xixc/jZeXunwUERERERGxQrdu5rvNBnv2wIEDcPUqbNlillesCK1amfNF0holVURERCTDunMnnLZtZ9rHTilUyI+VK3vg7+9pcWQiIiIiIiIycCBcvgx9+5pdhB0/bpa//76ZUDlxwny99Vbcwe5FrKKkimQ+RhSc32F1FCIiksLCwyPp0mUemzefBiBnTi9Wr+5J3rxZLI5MREREREREouXIAYsWmYPVP/ss7N1rlg8bFlMnd24zAZNUhgHXroG/v1q9SPLRQPWS+SzpCiFXrI5CRERS2IoVR1my5DAAWbK4sWJFD4oX18D0IiIiIiIiaZHNBhMnQtmycefNmAGnTiVuPYYB//4L331njt8SGAhz5iRvrJK5Kakimcvfk+Dw3Jhp7zzgpieWRUQyorZtSzJ+fGs8PV1YsKArlSvnsTokEREREREReYCRI+Hzz2HDBnj11ZjyP/9MeBnDMAe7HzsWOnaE7t3Nge/PnTPnb96coiFLJmMzDMOwOojUFBwcjJ+fHzdu3MDX19fqcCQ1BZ+GHwsBsQ75Z/ZBzsctC0lERFJeUNAtcuf2sToMEUvpHFiSSseMiIiIpBVVq5rvDRvCZ5/FlBsGHDkCa9aYr9gtWdzdoU4d+OsvuH7dLPvqK6hVC5ydUylwSVeScv6rMVUk87j8Nw4JlSbjlFAREclgLl68Ta5c3g5lSqiIiIiIiIikXzVrwrZt8PvvZiLl6FEzibJ6tWMixc0N6taFpk3Ndy8vmDQJxo0z57/+OgwaBE8+acW3kIxESRXJHC7tg99ax0yX7Q0VX7QsHBERSX6LFv1L167zmDLlCTp3jqcTXhEREREREUl3WrY0kypgJkxu3IiZ5+Zmtkhp1iwmkRJbqVKO07GXFXlYSqpI5nBqreN0rsrWxCEiIiliw4aTdOkyj5CQCLp2nUfevFmoW7eg1WGJiIiIiIjII2rdGkaMgPBwMyni5mZ249WsGdSvHzeRElutWubYKlOnwrp1qRayZHBKqkjGFhUJx5fD7wNjygo2hfLPWReTiIgkq717g2jXbiYhIREAdOv2OLVrF7A4KhEREREREUkuX34JK1dCtWrQoAF4ez94GQCbDcqVAz+/lI1PMhclVSRjunMRrhyAxZ3h7iXHeeWfBxcPa+ISEZFkdezYNVq0mM6NG6EAtGz5GJMnt8fJyWZxZCIiIiIiIpJcatY0X49q3DjImhU6dnz0dUnmpaSKZDxXDsEvFSAyLO48Z3fIXz/1YxIRkWR34cItAgN/ISjoFgA1a+Zn7tyncHV1tjgyERERERERSUtssZ67GzkSfHzM7sNseh5PHoKT1QGIJKsrB2FK6fgTKs1+gBcvgHdA6sclIiLJ6saNEFq0mM5//10DoEyZnCxd2g1vbzeLIxMREREREZG0pkULxy7DBg+G06cd69y9Cxs2wPHjqRubpD9qqSIZy19fOU77F4eyfaFsL/DJY0lIIiKSvEJCImjffhZ79gQBULCgHytX9iBbNk+LIxMREREREZG0qHJlWLYMunSBIPNSkrNnIXdu2LwZVq+GVavM8uzZ4b33oHBhKFjQspAlDVNSRTKWkOsxn/1LQs+/wNXLsnBERCT57dkTxPbtZwHIkcOLVat6kD+/r8VRiYiIiIiISFrm7Q1LlkDVqub0wIHg6gp37jjWu3LFnJc/P8yfD07q60nuoUNCMq5Oq5RQERHJgGrWzM/KlT3Ily8Ly5Z1o2TJHFaHJCIiIiIiIulEoULme3i4mVAJCIAePeCrrxzHWDlzxuwmLCwMoqIsCVXSKLVUkYwl4q7VEYiISCqoX78Q//33Ku7uOpURERERERGRxOvYEX79FWrVgsBAePzxmNYo06aZCZSePc3pNWtg/XqoVw8++8y6mCVt0Z0IyTh2jIZji62OQkREUsCePUFUrJjboUwJFREREREREUmq7t3NV3xKljTfv/sOXn7Z/BwZCb//DsHB4KuepwV1/yUZxa3zsPF/MdPObuDuZ108IiKSbKZN20elSj/w3ntrMQzD6nBEREREREQkg6teHfr2hSZNYsoWLLAsHEljlFSR9C8iFKZWcCxrPllJFRGRDGDZsiP06bMQgJEjN7F48WGLIxIREREREZGMzskJBgyAUaNiyr75xrp4JG1RUkXSv+CTcPdSzHTxJ6F0N+viERGRZLF162k6dZpDRIQ5IuCAAVVp27aExVGJiIiIiIhIZuHkBG+8ETNdsybs2GFdPJI2KKki6VtEKKzpHzPt7AYtf7YuHhERSRb791+kdesZ3L0bAUDnzmX55puW2Gw2iyMTERERERGRzKRp05jPERFmC5YePczPkjkpqSLp2/GlcPr3mOmyfcDV26poREQkGZw8eZ3AwGlcuxYCQNOmRZk69QmcnXXaIiIiIiIiIqkrIABGjjRbrUQ7dAhq14bRo62LS6yjuxOSPt25BH99y/+xd+dxNtb9H8ffZ/bBzDCYyb6GEVooWypi7JF9jxDxC2m5Sau6KWVJxS2yr1mzm6kUoWSNkLKNfWeYMes5vz9OzZiGmmFmvmd5PR+PeVzX95rrjNf8fm50PnNdl/bNSnu8ck8zPQCALHH+fIzCw2fr1KlrkqRq1QpryZJ28vX1MlwGAAAAAHBX4eHSvHnSU0+lHrNapYULpX37zHXBDIYqcE5rn5HWD5D+WJZ6LHyKdM/DxpIAAHfn2rV4NWkyVwcPXpQklS+fX6tXd1JAgK/hMgAAAACAuytTRnrzTWnwYKlChdTjEyaYa4IZDFXgnC78mnZt8ZSK1DHTAgDIEmfOXNfp0/YrVIoUCdC6dV1UsCC3dAQAAAAAOI5OnaQZMyQ/P/v6xx95voq7YagC5+aXT2q5XOp9VAouZ7oGAHAX7r03vzZtelY1axbVunVdVKJEXtNJAAAAAACk4+kpTZ2aum7bVoqOlqKipG3bJJvNXBuyHzcoh3Pz9JPKNDddAQDIIiVK5NWmTc/KYrGYTgEAAAAA4LbK3fTz3cePS/Xqpa7HjJEeeyznm5AzuFIFzufED9K1KNMVAIAssGzZASUmJqc5xkAFAAAAAOAM/vvfWx8fPFjaulX6+mvpww/tQxe4Dq5UgXNJjJGWNkld88YbADitTz/dqhdeWKNmzcppwYI2ypXL23QSAAAAAAAZ1rChFBMjrV8v1aljH6Ls2GH/XL9+qeetWSPNni0VLmymE1mLK1XgXG5ckBKupa7LtjLXAgC4Y/Pm7dGAAWskSStXHtTy5b8ZLgIAAAAAIPNatZI++URq10766CMpX77050RHSx98kPNtyB5cqQLnVfB+6clPTFcAADJp3bo/1K3bspQH97322qPq0KGS2SgAAAAAAO5SYKD0+edSRIRUo4Z0/bo0cKD9c5s2mW1D1uFKFTiv4AqmCwAAmfTTTyfUuvWXSkqySpJ6935I771X719eBQAAAACAcyhVSurTR7r/fql2bftVLH/5/XdzXcg6DFUAAECO2L//vJo2nauYmERJUqtWYZo4sSkPpgcAAAAAuKyKFVP3J08214Gsw1AFzsNmkw6tNF0BALgDx49fVXj4bF28eEOS9MQTJTVnTit5evJPEQAAAACA6woKkoKD7ftxcWZbkDV4JwPO48zP0rf/Z7oCAJBJFy/GKjx8tk6ciJYkPfjgPfrqqw7y8+PRbgAAAAAA1/d/f76luXmz2Q5kDYYqcB6XDqRdlwg30wEAyBSLxaJ8+fwkSWXLBmvNms4KDPQ1XAUAAAAAQM7wuOld+P37U/etVvvNeeBcGKrAecSeS91/cIBU+VlzLQCADAsO9ldkZFd17/6AIiK6KDQ0j+kkAAAAAAByTK1aqftdu0pffSUNHizVrCmNHWuuC3eG+27AORxeLW14JXUdXMFcCwAg03Ln9tG0aS1MZwAAAAAAkOPy5ZNKlJCOHbOv33039XO//GKmCXeOK1Xg+E7/JC1tmvZYcHkzLQCAf2Wz2fTxxz/q4sVY0ykAAAAAABhnsUjz5tmHK5J9+9fVK5cuSW+9JT37rHT6tLlGZBxDFTi+715Ku37sQ6lYXTMtAIB/NWLERg0atE516kzT8eNXTecAAAAAAGCcj480Y4Y0daq0bp3Upo39+KlT0qpV9itWliwx24iMYagCx3b6J+nUptR12ZZStZfs410AgMOZNGmbXn99vSRp//4L+uGHKMNFAAAAAAA4hsKFpSpV7A+uL1pU8vKSAgNTPz9tmrk2ZBxDFTiupDhpUYPUtV8+qcVSBioA4KAWLdqn559flbIeNaq+OnasbLAIAAAAAADHVLq0/YqVtWulF16wH8uTx2wTMoahChxX3GUp4Vrqusab5loAAP/o22+PqHPnJbLZ7OtXXqmlV16pbTYKAAAAAAAHFhRkvy1Y8+b29fXrUnS02Sb8O4YqcA6lGktVB5muAADcwvbtp9SixXwlJCRLkrp3f0AffFDfcBUAAAAAAM4hKCh1f+VKcx3IGIYqcA6efqYLAAC3cPDgRTVuPEfXrydIkpo1K6fJk5vLwq0aAQAAAADIEE/P1CcejBkjbdsmHTxotgm3x1AFAADckXPnYhQePkvnz8dKkh59tLi+/LKNvLz45wUAAAAAAJlRs2bqft++Us+eUlKSuR7cHu96AACAOxIc7K969UpJkipXDtGKFR3l7+9tuAoAAAAAAOfTvXva9Y0bUlyckRT8Cy/TAQAAwDl5eXnoiy+eUvny+dWt2/3Km5dbNQIAAAAAcCfuv18aMkTKm9e+laTz56U8eYxm4RYYqgAAgDtmsVj0n/88ajoDAAAAAACn5ukptWmT9tjx41KpUmZ6cHvc/gsAAGSI1WrTyy9HaM+es6ZTAAAAAABwefPmmS7ArTBUAQAA/8pms+mVVyI0evQWPfbYdG3aFGU6CQAAAAAAl+Tpad/+/LMUG2u2BekxVAEAAP9q1KhNGjPmR0lSdHS8zp6NMVwEAAAAAIBreuut1P2GDaW2baXffjPXg7QYqgAAgH/0xRc7NGTINynr//2vqVq1CjNYBAAAAACA63riidT9GzekI0ekn34yloO/YagCx3VktekCAHB7y5Yd0HPPrUxZjxhRT717VzVYBAAAAACAa8uVSxowQCpYUAoJMV2Dv2OoAsd0+icpopfpCgBwaxs2HFOHDotktdokSYMGVdeQIY8argIAAAAAwPV16yatWSNVr25fjx9vtgepGKrAsZzbLU0uKc2tkfZ4yYZGcgDAXe3efUbNm89TfHyyJKlz58oaPbqhLBaL4TIAAAAAANxHYmLqfni4dOyYuRbYMVSBY/n+JSn6b38y1Hxbur+PkRwAcEdXrsSpUaM5io6OlyQ1blxW06a1kIcHAxUAAAAAAHLS88+n7l+6JG3bZq4FdgxV4FjO/u1PhZpvS48MMZICAO4qb14/vfaa/TZfNWsW1cKFbeXt7Wm4CgAAAAAA91O4sPTii6nrP/4w1wI7L9MBQIqvnpbir6auX7KZawEAN/fCC9VVvHiQ6tQpody5fUznAAAAAADgliwWqXNnacUK+0Dl999NF4GhChzDL59LfyxLXQeHGUsBANi1aFHBdAIAAAAAALjJ4cOmC8Dtv2Deud1S5N+emdJ4hpkWAHBDSUlWdey4WMuWHTCdAgAAAAAAbqFmTfs2OlqycYMfoxiqwLyrfxuvhk+R7nnYTAsAuBmbzaa+fVdq/vy9at36S02fvst0EgAAAAAA+Jsnnkjd37jRWAbEUAWO5sEXpMo9TVcAgNt47bVv9MUXOyVJnp4WFS0aaLgIAAAAAAD8XdhNT0s4d85cBxiqwNHkKWq6AADcxpgxW/T++5sk2R98N2dOK9WvX9pwFQAAAAAA+DsfH6lePft+QoLZFnfHUAUAADc0a9ZuvfRSRMr6s8+aqG3b+wwWAQAAAACAf3Ljhn27dKnZDnfHUAUAADezatVB9ejxVcr6nXee0PPP8ywrAAAAAAAcWXy8fXvkiNkOd8dQBQAAN7JpU5Tatl2o5GSbJKl//4f1xhuPGa4CAAAAAAD/Jjw8dX/3bnMd7o6hCgAAbiI2NlFt2izUjRtJkqT27e/T+PGNZbFYDJcBAAAAAIB/c/NQ5fXXzXW4O4YqAAC4iVy5vDVnTivlyeOj8PAymjnzaXl4MFABAAAAAMAZBAZKxYvb90+fls6dM9vjrhiqwLyr3AQQAHJKvXqltGnTs1q8uJ18fDxN5wAAAAAAgEwYNy51f8YMYxlujaEKzDn2tTTaIn3/kukSAHBZSUnWdMeqVAlVnjw+BmoAAAAAAMDd+OtKFUlasICH1pvAUAXm/Phe+mPBFXK+AwBcVHx8kho2nK133vlONpvNdA4AAAAAAMgC/fun7rdtK33+ubkWd8RQBeYkXEu7bjpfKt3UTAsAuJjkZKu6dFmqb789orff/l7Dhn1rOgkAAAAAAGSB1q3Trhmq5CyGKjDPw1t6ySZVaC95cH9/ALhbNptN/fuv1qJF+yTZH1D/1FPlDVcBAAAAAICsEBgoTZ0qFSiQeuzyZXM97oahCgAALuatt77TpEnbJUleXh5avLidatQoargKAAAAAABklSpVpNWrU9exseZa3I2X6QC4oYOLpKhvpXM7TJcAgMv55JOf9O67G1LWM2a0VKNGZQ0WAQAAAACA7OBx0yUTCQnmOtwNV6ogZ13cL61oK+2emHrMy99cDwC4kHnz9mjAgLUp648/bqROnSobLAIAAAAAADlh/nzTBe6DoQpyjs0m/Toj7TEvf6nmW2Z6AMCFrFv3h7p1W5ayHjasjgYMqG4uCAAAAAAA5JjFi6UzZ0xXuAeGKsg5vy+Wfv4gdV2+vfR/V6Vqg801AYALSExMVv/+q5WUZJUkPffcQ3r33bqGqwAAAAAAQHbr3Tt1/+ZnrCD7MFRBzjnzc9r1fd0lT28jKQDgSry9PbVuXReVLp1PrVqFacKEprJYLKazAAAAAABANnvuudT98+fNdbgTHlQPM+q8L5VsaLoCAFxGmTLB2rKlpwIDfeXpyc9MAAAAAADgDiwWqVUrackSycfHdI174F0XmFGohv1/8QCAO3L1apySk61pjoWE5JafHz8vAQAAAACAOwkIMF3gXhiqAADgZK5fT1B4+Gy1a7dIcXFJpnMAAAAAAIAD2LzZdIF7YKgCAIATSUhIVuvWX2rr1pNasmS/nn32K9NJAAAAAADAoGvX7NsjR8x2uAuGKgAAOAmr1aZnnlmmiIhDkqS8ef00dOijhqsAAAAAAIBJj/751kCuXGY73AVDFeScGxdNFwCA07LZbBo4cI3mz98rSfLz89KKFR1VuXKo4TIAAAAAAGBSoUL2bWysdJG3YLMdQxXkjF0Tpb1fmK4AAKf13nsb9OmnP0uSPD0tWriwrR59tLjhKgAAAAAAYNrNV6g0bCht2GCuxR0wVEHO2D8n7TqgqJkOAHBC//vfNr355ncp66lTW6hZs3LmggAAAAAAgMMoUiTtevBg6coVIylugaEKst/5X6RTm1LXTWZLecuY6wEAJ7Jo0T7167cqZf3RRw3Urdv9BosAAAAAAICjGT1aeuCB1PWWLcZSXB5DFWQvm01a0jR1bfGUwjqb6wEAJ2K12jRu3I+y2ezrV1+tpZdeqmU2CgAAAAAAOJzHH5emTJEKF7avExPN9rgyhirIXtYk6fqJ1HXlXuZaAMDJeHhYtGZNZ9WvX1rPPvuA3n+/vukkAAAAAADgwEqXtm8/+MBshyvzMh0AN5K3jNTgf6YrAMCpBAT4auXKjvL09JDFYjGdAwAAAAAAHFhCgn0bH2+2w5VxpQpyTu7CpgsAwOGdOnVNV67EpTnm6+slLy/+ygYAAAAAAP9s4MDU/UuXzHW4Mt6hAQDAQVy6dEPh4bP02GPTdPr0NdM5AAAAAADAyZQvn7rfooX0v/9Jhw+b63FFDFWQfZLipesnTVcAgFOIjU1U8+bz9Ouv57Vnzzl17brUdBIAAAAAAHBiN27YH17frp20fbvpGtfBUAXZ4/weaVIRaUop0yUA4PASE5PVtu1Cbd58XJIUEpJb//tfM8NVAAAAAADAGU2dmv7Y0KGS1ZrzLa6IoQqy3pVD0swqUtzFtMfzFDHTAwAOzGq16dlnl2v16t8lSYGBvlq7trPKlg02XAYAAAAAAJxRlSrSzJnS+PFSvnz2Y5cuSUePGs1yGQxVkLXiLktflE17zC+fVKWP9Oh7ZpoAwEHZbDa9/HKEZs/+RZLk6+upr77qoAcfLGS4DAAAAAAAOLOKFaVataTISCkkxH7s2DGzTa7Cy3QAXMyi8LTrfOWlbrslL18zPQDgwEaN2qSxY3+UJHl4WDRvXms98URJs1EAAAAAAMClnDtn3165YjTDZXClCrLW2W2p+7lCpR77GagAwC188cUODRnyTcp60qRmevrpMINFAAAAAADAFeXPb9/+9pvZDlfBUAXZp8d+yWIxXQEADsdms2njxqiU9YgR9dSr10MGiwAAAAAAgKv66y3awoXNdrgK40OVCRMmqFSpUvLz81PVqlW1cePGfzw/Pj5ew4YNU4kSJeTr66syZcpo6tSpOVSLDCtU3f4sFQBAOhaLRVOnttDAgdX14os1NGTIo6aTAAAAAACAi6pe3b5du9Zsh6sw+kyVBQsWaNCgQZowYYJq166tSZMmqXHjxtq3b5+KFy9+y9e0a9dOZ8+e1RdffKGyZcvq3LlzSkpKyuFyAADujoeHRWPHNpRkH7IAAAAAAABkh5gY+/bgQbMdrsLoUGXMmDHq2bOnevXqJUkaN26c1q1bp4kTJ2rkyJHpzl+7dq2+//57HT58WMHBwZKkkiVL5mQyAAB35NChS0pISFZYWMGUYwxTAAAAAABAdmvWTPruO8nD+H2rXIOx/zMmJCRo+/btCg8PT3M8PDxcmzdvvuVrli9frmrVqmnUqFEqUqSIypUrp5dfflk3bty47a8THx+v6OjoNB8AAOSk06evKTx8turUmaatW0+azgEAAAAAAG6kVCn71mqVli832+IKjA1VLly4oOTkZIWGhqY5HhoaqjNnztzyNYcPH9YPP/ygvXv3aunSpRo3bpwWLVqk/v373/bXGTlypIKCglI+ihUrlqXfBwAA/+TKlTg1bjxHhw9f1sWLNzRgwBrZbDbTWQAAAAAAwE0EBKTub91qrsNVGL/g5++3PrHZbLe9HYrVapXFYtGcOXP0yCOPqEmTJhozZoymT59+26tVhg4dqqtXr6Z8HD9+PMu/B/wpMdZ0AQA4lBs3EtWixXzt3n1WklSiRJAWL27Hbb8AAAAAAECOCQ6W6tc3XeE6jA1VChQoIE9Pz3RXpZw7dy7d1St/KVSokIoUKaKgoKCUY2FhYbLZbDpx4sQtX+Pr66vAwMA0H8hi8dHSmmek8blNlwCAw0hKsqpDh8XasOGYJKlAgVyKiOiqIkX4ewgAAAAAAOSs++6zb9eulU6dMtvi7IwNVXx8fFS1alVFRkamOR4ZGalatWrd8jW1a9fWqVOndP369ZRjBw8elIeHh4oWLZqtvfgHB+ZK+2amPZavnJkWAHAANptNzz23QsuX/yZJypPHR2vWdFa5cvkNlwEAAAAAAHd0+XLqfrt20m2ewIEMMHr7r8GDB2vKlCmaOnWq9u/frxdffFFRUVHq27evJPutu7p165ZyfqdOnZQ/f3716NFD+/bt04YNG/TKK6/o2Weflb+/v6lvwz39NFKaW0OaU136+vm0n3tooPT4R2a6AMABDB36jaZN2yVJ8vHx1LJl7VWtWmGzUQAAAAAAwG3VqJG6HxcnffaZuRZn52XyF2/fvr0uXryo4cOH6/Tp06pUqZJWr16tEiVKSJJOnz6tqKiolPPz5MmjyMhIvfDCC6pWrZry58+vdu3a6b333jP1Lbiny79LP7x268+1+Voq8WTO9gCAAxk9erM++GCTJMlikebMaaUnnyxtuAoAAAAAALizatWk99+XPvjAftWK1Wq6yHkZHapIUr9+/dSvX79bfm769OnpjlWoUCHdLcOQw+Iu/e3Anw9cLl5PKlonx3MAwFFNmNBUbdpUNJ0BAAAAAADcnIeH/WH1585JY8ZICQmmi5yX8aEKnNxDA6W640xXAIDDeOmlWipQIJdOnIhW377VTOcAAAAAAACkSEqyb9evN9vhzBiqAACQxZ555gHTCQAAAAAAAOnkzm3fBgaa7XBmd/Sg+qSkJH399deaNGmSrl27Jkk6deqUrl+/nqVxAAA4uj17zmr16t9NZwAAAAAAAPyrin/epTw6OvWqFWROpocqx44dU+XKldWiRQv1799f58+flySNGjVKL7/8cpYHwgHFXTZdAAAO4ejRK2rYcLaeemqeZs3abToHAAAAAADgH1ksqfurVpnrcGaZHqoMHDhQ1apV0+XLl+Xv759y/Omnn9Y333yTpXFwQOf3SEsam64AAOPOnYtRePgsnT59XcnJNn322c9KTraazgIAAAAAALitMmVS9999V7LZzLU4q0wPVX744Qe9/vrr8vHxSXO8RIkSOnnyZJaFwUEdXZd2HVTm1ucBgAuLjo5X48Zz9PvvlyRJFSoU0MqVneTpeUd31QQAAAAAAMgR3t5So0ap61dekSIiJCs/J5phmX73x2q1Kjk5Od3xEydOKCAgIEui4MhuGl0WfUyq3NNcCgAYEBeXpJYt52vHjtOSpKJFA7VuXRcVKJDLcBkAAAAAAMC/GzIkdf+776TXXpN27jSW43QyPVRp0KCBxo0bl7K2WCy6fv263nrrLTVp0iQr2+DoHhooefMmIgD3kZxsVZcuS7R+/VFJUnCwvyIiuqh48SCzYQAAAAAAABmUJ4/Ur1/aY336SLNnS7GxZpqcSaaHKmPHjtX333+vihUrKi4uTp06dVLJkiV18uRJffDBB9nRCACAcTabTf37r9bixfslSblyeWv16k4KCytouAwAAAAAACBznnlGGj067bFx46SxY43kOJVMD1UKFy6sXbt26ZVXXlGfPn304IMP6v3339fOnTsVEhKSHY1wJBf2mC4AACPeeed7TZq0XZLk5eWhJUvaqXr1ooarAAAAAAAAMs/TU3r8cem//017fOlSiUen/zOvzL5gw4YNqlWrlnr06KEePXqkHE9KStKGDRv02GOPZWkgHMiO8dK+WaYrAMCImjWLKlcub924kaiZM1uqYcOyppMAAAAAAADuSoMGUkCAtH+/NHGi/diWLVKbNma7HFmmr1SpW7euLl26lO741atXVbdu3SyJgoM6FpF2HVzBTAcAGNCwYVl9+203/e9/zdSxY2XTOQAAAAAAAHfNw0OqVUvq2VNq1cp+bPNms02OLtNXqthsNlkslnTHL168qNy5c2dJFJxAk7lS/oqmKwAgR1WvXpRbfgEAAAAAAJf0++/27alTZjscXYaHKq3+HFNZLBZ1795dvr6+KZ9LTk7WL7/8olq1amV9IRxTiQamCwAgW/344wlt3HhML79c65Y/TAAAAAAAAOBKwsKkPXukP/4wXeLYMjxUCQoKkmS/UiUgIED+/v4pn/Px8VGNGjXUu3fvrC8EACCH7dt3Xk2bztWlSzd0+vR1ffRRuDw8GKwAAAAAAADXVb68fRsSYrbD0WV4qDJt2jRJUsmSJfXyyy9zqy8AgEuKirqqhg1n69KlG5Kk3bvPKinJKh8fT8NlAAAAAAAA2ad4cfv23DmzHY4u089Ueeutt7KjA45u52fS4VWmKwAgW124EKvw8Fk6cSJaklS1aiEtW9aegQoAAAAAAHB5SUmp+zduSDfdrAo3yfRQRZIWLVqkL7/8UlFRUUpISEjzuR07dmRJGBzMj++m7nv6SN78LwqAa7l+PUFNmszRb79dlCTde2+wVq/urIAA3395JQAAAAAAgPMrUiR1f/Zsiad93JpHZl8wfvx49ejRQyEhIdq5c6ceeeQR5c+fX4cPH1bjxo2zoxGOICk2df+JsZI3t38D4Dri45PUqtUC/fzzKUlS4cIBiojoqpAQ/qwDAAAAAADuITQ0dX/SJHMdji7TQ5UJEybo888/16effiofHx+9+uqrioyM1IABA3T16tXsaIQjyV9ReqCf6QoAyDLJyVY988wyRUYeliTlzeundeu6qGTJvGbDAAAAAAAAcpCnp/Tcc6nrkyfNtTiyTA9VoqKiVKtWLUmSv7+/rl27Jknq2rWr5s2bl7V1AABkszfeWK8FC36VJPn7e2nlyo6qVCnEcBUAAAAAAEDOe+SR1P3t2811OLJMD1XuueceXbxov998iRIl9OOPP0qSjhw5IpvNlrV1AABks+7dH1CJEkHy9LRo4cK2ql27uOkkAAAAAAAAI6pUSd0fPlzasMFci6PK9FClXr16WrFihSSpZ8+eevHFF9WgQQO1b99eTz/9dJYHwgFcPyUlXDNdAQDZoly5/Nq8uacWLWqnpk3Lmc4BAAAAAAAwxsND+vNGVZKkwYMlq9VcjyPyyuwLPv/8c1n//L9i3759FRwcrB9++EHNmzdX3759szwQhiXGStPvM10BANmqcOEAtWxZwXQGAAAAAACAcVWrSps3p6737k17BYu7y/SVKh4eHvLySp3FtGvXTuPHj9eAAQN0/vz5LI2DA7h8UIq/kroOechYCgBkha+/PqxnnlmmhIRk0ykAAAAAAAAO55lnpPXrU9c3bphrcUSZHqrcypkzZ/TCCy+obNmyWfHl4Kh8g6QGk0xXAMAd+/nnk2rZcr5mztyt5s3nKSYmwXQSAAAAAACAwwkIkEqUsO9/8okUF2e2x5FkeKhy5coVde7cWQULFlThwoU1fvx4Wa1WvfnmmypdurR+/PFHTZ06NTtbYVqFjpJ3LtMVAHBHfvvtgpo0mauYmERJkr+/l3x9M30XTAAAAAAAALdw6ZJ9e+CA1LSpdP262R5HkeGhymuvvaYNGzbomWeeUXBwsF588UU1a9ZMP/zwg9asWaOff/5ZHTt2zM5WAADuyMmT0QoPn60LF2IlSY89VkLz5rWWl1eWXLAJAAAAAADgcoYNS92/elU6dsxciyPJ8LtJq1at0rRp0/TRRx9p+fLlstlsKleunL799ls9/vjj2dkIAMAdu3TphsLDZysq6qok6f77Q7V8eQf5+3sbLgMAAAAAAHBc9epJgwalrv+6csXdZXiocurUKVWsWFGSVLp0afn5+alXr17ZFgYHkBgjRT5nugIA7lhMTIKaNZurffvOS5JKl86ntWu7KCjIz3AZAAAAAACAY/PwkLp0kWrXtq8vXzbb4ygyPFSxWq3y9k79qV5PT0/lzp07W6LgIH5fIp35OXXtyZuQAJxHYmKy2rZdqC1bTkiSQkNzKyKii+65J4/hMgAAAAAAAOeRL599u2yZ0QyHkeEn9NpsNnXv3l2+vr6SpLi4OPXt2zfdYGXJkiVZWwgzTmyQ1nRLe+y+7kZSAOBOvPfeBq1Z84ckKTDQV2vXdlGZMsGGqwAAAAAAAJzLCfvPq+rUKbMdjiLDV6o888wzCgkJUVBQkIKCgtSlSxcVLlw4Zf3XB1zE7klp163XSSH3m2kBgDsweHBNPfZYCfn6emr58g564IF7TCcBAHBLEyZMUKlSpeTn56eqVatq48aN/3j+Z599prCwMPn7+6t8+fKaOXNmunOuXLmi/v37q1ChQvLz81NYWJhWr16dXd8CAAAAXFi5cvbthQuS1Wq2xRFk+EqVadOmZWcHHEl8tHRgbuq6XBup+JPmegDgDgQF+Wndui7aseO0atUqZjoHAIBbWrBggQYNGqQJEyaodu3amjRpkho3bqx9+/apePHi6c6fOHGihg4dqsmTJ+vhhx/W1q1b1bt3b+XLl0/NmzeXJCUkJKhBgwYKCQnRokWLVLRoUR0/flwBAQE5/e0BAADABVSuLH35pX1/4EDpk0/M9phmsdlsNtMROSk6OlpBQUG6evWqAgMDTec4psWNpKPrUtd9Tkp5CpvrAYAMSk62ytMzwxdhAoDb4N/Ajqt69ep66KGHNHHixJRjYWFhatmypUaOHJnu/Fq1aql27dr68MMPU44NGjRI27Zt0w8//CBJ+t///qcPP/xQBw4cSPNczMzg9wwAAAD+cvmy1KmTdP68fb18uVTYxd4uzsy/f3nnCekdjUjdDygu5Qo11wIAGbRkyX7VqPGFzp69bjoFAIAMSUhI0Pbt2xUeHp7meHh4uDZv3nzL18THx8vPzy/NMX9/f23dulWJiYmSpOXLl6tmzZrq37+/QkNDValSJY0YMULJycm3bYmPj1d0dHSaDwAAAECyP6h+zZrUdVycuRZHwFAFdsmJ0ua3pdEWSTddvNRlm+ThaaoKADJk/foj6thxsbZtO6Xatafq/PkY00kAABcXExOjN954Q7Vq1VLZsmVVunTpNB8ZceHCBSUnJys0NO0PMYWGhurMmTO3fE3Dhg01ZcoUbd++XTabTdu2bdPUqVOVmJioCxcuSJIOHz6sRYsWKTk5WatXr9brr7+u0aNH67///e9tW0aOHJnmWZnFinHrTAAAAKTl9efDRGJjzXaYluFnqsDFHVktbXkn7bEij0q5CprpAYAM2rnztFq0mK+EBPtP39auXVz58+cyXAUAcHW9evXS999/r65du6pQoUKyWCx3/LX+/lqbzXbbr/fGG2/ozJkzqlGjhmw2m0JDQ9W9e3eNGjVKnp72H4ayWq0KCQnR559/Lk9PT1WtWlWnTp3Shx9+qDfffPOWX3fo0KEaPHhwyjo6OprBCgAAANJISrJvu3eXZs6UKlY0mmMMQxXYxZxOuw4oLtUdb6YFADLojz8uqVGjObp2LUGS1LTpvZoypbk8PO78jS0AADJizZo1WrVqlWrXrn3HX6NAgQLy9PRMd1XKuXPn0l298hd/f39NnTpVkyZN0tmzZ1WoUCF9/vnnCggIUIECBSRJhQoVkre3d8qQRbI/p+XMmTNKSEiQj49Puq/r6+srX1/fO/5eAAAA4Pry5pWuXLHvDxokRUT8w8ku7I5u/zVr1izVrl1bhQsX1rFjxyRJ48aN01dffZWlcTCkwSTpuWNS6IOmSwDgtk6fvqbw8Fk6d85+q69atYrpyy/bytubWxYCALJfvnz5FBwcfFdfw8fHR1WrVlVkZGSa45GRkapVq9Y/vtbb21tFixaVp6en5s+fr2bNmsnDw/6fd7Vr19Yff/whq9Wacv7BgwdVqFChWw5UAAAAgIx47bXU/UuXzHWYlumhysSJEzV48GA1adJEV65cSXnYYd68eTVu3Lis7oMJnvyEGgDHduVKnBo1mqMjR65IkipVCtHKlR2VK5e32TAAgNt499139eabbyr2Lm8oPXjwYE2ZMkVTp07V/v379eKLLyoqKkp9+/aVZL8tV7du3VLOP3jwoGbPnq3ff/9dW7duVYcOHbR3716NGDEi5Zznn39eFy9e1MCBA3Xw4EGtWrVKI0aMUP/+/e+qFQAAAO6tXj1p4sTU9U0/w+NWMn37r08++USTJ09Wy5Yt9f7776ccr1atml5++eUsjQMA4O9u3EjUU0/N0y+/nJUklSgRpLVrOytfPn/DZQAAdzJ69GgdOnRIoaGhKlmypLy90w72d+zYkaGv0759e128eFHDhw/X6dOnValSJa1evVolSpSQJJ0+fVpRUVEp5ycnJ2v06NH67bff5O3trbp162rz5s0qWbJkyjnFihVTRESEXnzxRVWpUkVFihTRwIED9Z///Ofuv3EAAAC4tQcflHx8pIQEafp06dlnTRflvEwPVY4cOaIHH0x/WyhfX1/FxMRkSRQAALczceI2bdxof3OpQIFciojoqiJFAg1XAQDcTcuWLbPsa/Xr10/9+vW75eemT5+eZh0WFqadO3f+69esWbOmfvzxx6zIAwAAAFJ4edkHKpI0YYJUtap0//1mm3JapocqpUqV0q5du1J+cuova9asUcWKFbMsDACAWxk4sLr++OOSZs36RWvXdla5cvlNJwEA3NBbb71lOgEAAAAwok0badEi+/733zNU+VevvPKK+vfvr7i4ONlsNm3dulXz5s3TyJEjNWXKlOxoBAAghaenhz77rIleeqmmypS5uwcEAwBwt7Zv3679+/fLYrGoYsWKt7yqHwAAAHAlgwalDlV83fDx3JkeqvTo0UNJSUl69dVXFRsbq06dOqlIkSL6+OOP1aFDh+xoBAC4uejoeAUGpv4tbbFYGKgAAIw6d+6cOnTooO+++0558+aVzWbT1atXVbduXc2fP18FCxY0nQgAAABkCz8/qXFjac0a9xyqeNzJi3r37q1jx47p3LlzOnPmjI4fP66ePXtmdRsAAJoxY5fKlftEO3acNp0CAECKF154QdHR0fr111916dIlXb58WXv37lV0dLQGDBhgOg8AAADIVl5/Xq7x7bdmO0zI9FDlnXfe0aFDhyRJBQoUUEhISJZHAQAgSStW/KaePZfr7NkYPfHEdB05ctl0EgAAkqS1a9dq4sSJCgsLSzlWsWJFffbZZ1qzZo3BMgAAACD7HThg3165YjTDiEwPVRYvXqxy5cqpRo0a+vTTT3X+/Pns6AIAuLkffohSu3aLlJxskyR17/6ASpbMazYKAIA/Wa1WeXt7pzvu7e0tq9VqoAgAAADIOaVL27enTpntMCHTQ5VffvlFv/zyi+rVq6cxY8aoSJEiatKkiebOnavY2NjsaAQAuJlffjmrZs3mKi4uSZLUsWMljRvXSBaLxXAZAAB29erV08CBA3Xqpv+KPHnypF588UU9+eSTBssAAACA7FepkukCc+7omSr33XefRowYocOHD2v9+vUqVaqUBg0apHvuuSer+wAAbubIkctq1Gi2rl6NlyQ1bFhG06e3lIcHAxUAgOP49NNPde3aNZUsWVJlypRR2bJlVapUKV27dk2ffPKJ6TwAAAAgW908VLl61VyHCV53+wVy584tf39/+fj46Nq1a1nRBABwU2fPXld4+GydPn1dklS9ehEtXtxOPj6ehssAAEirWLFi2rFjhyIjI3XgwAHZbDZVrFhR9evXN50GAAAAZLsKFVL3z52TgoLMteS0OxqqHDlyRHPnztWcOXN08OBBPfbYY3r77bfVtm3brO4DALiJ6Oh4NW48R3/8cUmSFBZWQKtWdVLu3D6GywAAuL0GDRqoQYMGpjMAAACAHOXtbf9ITJTi403X5KxMD1Vq1qyprVu3qnLlyurRo4c6deqkIkWKZEcbAMCNrFx5UDt3npEkFSsWqHXruih//lyGqwAASDV+/Hg999xz8vPz0/jx4//x3AEDBuRQFQAAAGBGYqJ9+/PP7vWMlUwPVerWraspU6bovvvuy44eAICb6tSpsmJiEvT66+sVEdFVxYq50XWjAACnMHbsWHXu3Fl+fn4aO3bsbc+zWCwMVQAAAOA2du82XZCzMj1UGTFiRHZ0AACg3r2rqn37SgoM9DWdAgBAOkeOHLnlPgAAAOCOChaUzp+XfvhBstkki8V0Uc7I0FBl8ODBevfdd5U7d24NHjz4H88dM2ZMloQBAFzfoUOXVKZMcJpjDFQAAM4oOTlZe/bsUYkSJZQvXz7TOQAAAEC269hR+uuuuGvWSE2amO3JKRkaquzcuVOJf94gbefOndkaBABwDx9//KNeeSVSs2Y9rfbt3ejGmwAAlzBo0CBVrlxZPXv2VHJysh577DFt2bJFuXLl0sqVK/XEE0+YTgQAAACyVefO0qZN0vbtUmQkQ5U01q9ff8t9AADuxJw5v2jQoHWSpI4dF6tixYKqXDnUcBUAABm3aNEidenSRZK0YsUKHT16VAcOHNDMmTM1bNgwbdq0yXAhAAAAkL08PaUXXpC6d5f27HGfW4B5ZPYFzz77rK5du5bueExMjJ599tksiQIAuK61a/9Q9+5fpazfeOMxBioAAKdz4cIF3XPPPZKk1atXq23btipXrpx69uypPXv2GK4DAAAAcka5cpKXl3TlinT8uOmanJHpocqMGTN048aNdMdv3LihmTNnZkkUAMA1/fjjCbVu/aWSkqySpL59q+rtt58wGwUAwB0IDQ3Vvn37lJycrLVr16p+/fqSpNjYWHl6ehquAwAAAHKGj48UFmbfnz3bbEtOydDtvyQpOjpaNptNNptN165dk5+fX8rnkpOTtXr1aoWEhGRLJADA+e3bd15Nm85VbKz9GV1t2lTUp582kcUdrgsFALicHj16qF27dipUqJAsFosaNGggSfrpp59UoUIFw3UAAABAzvH6c8rw52PZXV6Ghyp58+aVxWKRxWJRuXLl0n3eYrHonXfeydI4AIBriIq6qvDwWbp0yX6l45NPltLs2U/L0zPTF0wCAOAQ3n77bVWqVEnHjx9X27Zt5evrK0ny9PTUkCFDDNcBAAAAOadlS2nnTunsWdMlOSPDQ5X169fLZrOpXr16Wrx4sYKDg1M+5+PjoxIlSqhw4cLZEgkAcF4XLsQqPHyWTp60P4+ratVCWrq0vXx9M/xXEAAADqlNmzbpjj3zzDMGSgAAAABzQv98VC5Dlb95/PHHJUlHjhxR8eLFuV0LACBD9uw5q2PHrkqSypXLrzVrOisgwNdwFQAAmTd+/Hg999xz8vPz0/jx4//x3AEDBuRQFQAAAGDWX08FOXtWstkkVx8dZGio8ssvv6hSpUry8PDQ1atXtWfPntueW6VKlSyLAwA4v7p1Sykysqv69l2plSs7qWDB3KaTAAC4I2PHjlXnzp3l5+ensWPH3vY8i8XCUAUAAABu468rVeLipGvXpMBAsz3ZLUNDlQceeEBnzpxRSEiIHnjgAVksFtlstnTnWSwWJScnZ3kkAMC5Pfpoce3e3ZdnqAAAnNqRI0duuQ8AAAC4M9+bbkhy4ID0yCPmWnJChoYqR44cUcGCBVP2AQC4HZvNpm+/PaInnyyd5jgDFQAAAAAAANc2fbrrD1Uy9A5XiRIlUp6hUqJEiX/8AAC4t+HDv1f9+rM0dOjXt7yqEQAAV9CmTRu9//776Y5/+OGHatu2rYEiAAAAwBx/f/t261azHTkh0z82PGPGDK1atSpl/eqrrypv3ryqVauWjh07lqVxAADnMmHCz3r77e8lSe+/v0lbtpwwXAQAQPb4/vvv1bRp03THGzVqpA0bNhgoAgAAAMx5/vnU/YQEcx05IdNDlREjRsj/z7HTli1b9Omnn2rUqFEqUKCAXnzxxSwPBAA4hy+//FX/93+rU9ZjxzZUrVrFDBYBAJB9rl+/Lh8fn3THvb29FR0dbaAIAAAAMKdZs9T9GzfMdeSETA9Vjh8/rrJly0qSli1bpjZt2ui5557TyJEjtXHjxiwPBAA4vsjIQ+rSZYn+utvX0KGPatCgGmajAADIRpUqVdKCBQvSHZ8/f74qVqxooAgAAAAwJ08e0wU5J0MPqr9Znjx5dPHiRRUvXlwREREpV6f4+fnphquPoFyVzSad2Wa6AoCT+vnnk3r66QVKTLRKknr2fFD//W89w1UAAGSvN954Q61bt9ahQ4dUr579771vvvlG8+bN08KFCw3XAQAAAMgumR6qNGjQQL169dKDDz6ogwcPptxH+Ndff1XJkiWzug854ZdJ0t4vTFcAcEIHDlxQ48ZzFBOTKElq2bKC/ve/ZrJYLIbLAADIXk899ZSWLVumESNGaNGiRfL391eVKlX09ddf6/HHHzedBwAAABjz22/SI4+Yrsg+mR6qfPbZZ3r99dd1/PhxLV68WPnz55ckbd++XR07dszyQOSA49+nXReobKYDgFM5cSJa4eGzdPGi/SrFxx8voXnzWsvLK9N3lgQAwCk1bdr0lg+rBwAAANyNx01vB02ZwlAljbx58+rTTz9Nd/ydd97JkiAY1nCaFPqQ6QoATuD69YSUZ6g88MA9+uqrDvLzy/RfKwAAOK0rV65o0aJFOnz4sF5++WUFBwdrx44dCg0NVZEiRUznAQAAADmqcGHp1Ckpb17TJdnrjt79unLlir744gvt379fFotFYWFh6tmzp4KCgrK6D9ltzxfSb/NT18WeMJYCwLlUqFBAmzc/q379VmvKlOYKCvIznQQAQI755ZdfVL9+fQUFBeno0aPq1auXgoODtXTpUh07dkwzZ840nQgAAADkqC5dpFGjpNhY0yXZK9P3aNm2bZvKlCmjsWPH6tKlS7pw4YLGjh2rMmXKaMeOHdnRiOy0dWTqvsVD8gkw1wLA6RQrFqQVKzoqNDSP6RQAAHLU4MGD1b17d/3+++/y80v9wYLGjRtrw4YNBssAAAAAM/4apvz4o9mO7JbpocqLL76op556SkePHtWSJUu0dOlSHTlyRM2aNdOgQYOyIRHZKulG6v7joyX//OZaADg0q9Wmzz/frsTEZNMpAAAY9/PPP6tPnz7pjhcpUkRnzpwxUAQAAACY5etr35YrZ7Yju93RlSr/+c9/5OWVeucwLy8vvfrqq9q2bVuWxiEHBRSTqg4yXQHAQdlsNg0evE59+qzU008vUGxsoukkAACM8vPzU3R0dLrjv/32mwoWLGigCAAAADDrr8cKHjxotiO7ZXqoEhgYqKioqHTHjx8/roAAbh0FAK5o5Mgf9PHHP0mS1qz5Qz/9dMJwEQAAZrVo0ULDhw9XYqL9Bw0sFouioqI0ZMgQtW7d2nAdAAAAkPNc/Vkqf8n0UKV9+/bq2bOnFixYoOPHj+vEiROaP3++evXqpY4dO2ZHIwDAoMmTt2vYsG9T1lOmNFfduqUMFgEAYN5HH32k8+fPKyQkRDdu3NDjjz+usmXLKiAgQP/9739N5wEAAAA5rlix1H2r1VxHdvP691PS+uijj2SxWNStWzclJSVJkry9vfX888/r/fffz/JAAIA5S5bsV9++q1LWH3xQXz16PGiwCAAAxxAYGKgffvhB3377rXbs2CGr1aqHHnpI9evXN50GAAAAGFGoUOr+vn1SpUrmWrJTpocqPj4++vjjjzVy5EgdOnRINptNZcuWVa5cubKjDwBgyPr1R9Sx42JZrTZJ0ksv1dQrr9QyXAUAgHlJSUny8/PTrl27VK9ePdWrV890EgAAAGBcUFDq/sKFrjtUyfDtv2JjY9W/f38VKVJEISEh6tWrlwoVKqQqVaowUAEAF7Njx2m1aDFfCQnJkqRu3e7XqFENZLFYDJcBAGCel5eXSpQooeTkZNMpAAAAgMPw9Ezdj4mR/rzRlcvJ8FDlrbfe0vTp09W0aVN16NBBkZGRev7557OzDQBgwB9/XFKjRrN17VqCJKlZs3KaMqW5PDwYqAAA8JfXX39dQ4cO1aVLl0ynAAAAAA6jWzf79rvvpMcekw4cMJqTLTJ8+68lS5boiy++UIcOHSRJXbp0Ue3atZWcnCzPm0dQAACnFhzsr3vvza/z52NVu3YxLVjQRt7e/DkPAMDNxo8frz/++EOFCxdWiRIllDt37jSf37Fjh6EyAAAAwJyAgNT9hATp4EGpQgVzPdkhw0OV48ePq06dOinrRx55RF5eXjp16pSKFSuWLXEAgJwXHOyvyMiueu21b/TWW48rVy5v00kAADicli1bymKxyGazmU4BAAAAHMaDD0rBwdJfF3QnJprtyQ4ZHqokJyfLx8cn7Yu9vJTkqjdGAwA3liuXt8aNa2Q6AwAAhxMbG6tXXnlFy5YtU2Jiop588kl98sknKlCggOk0AAAAwLgHHpAiIqSuXaX9+6UtW6TWrU1XZa0MD1VsNpu6d+8uX1/flGNxcXHq27dvmkvdlyxZkrWFAIBslZRk1euvf6uXX66lAgVymc4BAMCh/fWsyc6dO8vf319z587V888/r4ULF5pOAwAAABzG5cv27XffGc3IFhkeqjzzzDPpjnXp0iVLYwAAOctqtalXr+WaMWO3li07oIiIripePMh0FgAADuvvz5rs3Lkzz5oEAAAA/qZWLWnJEilvXtMlWS/DQ5Vp06ZlZwdM2Dtdun7KdAUAg/7zn0jNmLFbknTkyBUdOXKZoQoAAP+AZ00CAAAA/65pU/tQJTDQdEnW8zAdAENuXJIie6euPX1ufy4Al/Thh5v00UdbJEkeHhbNm9dajz9e0mwUAAAOjmdNAgAAABkXF2e6IOtl+EoVuJj4y5L1pv/we/AFcy0Acty0aTv16qtfp6wnTmyqVq3CDBYBAOAceNYkAAAA8O+Sk+3bc+fMdmQHhiqQyrWRHhpougJADlm+/Df17r0iZf3ee3X13HNVDRYBAOA8eNYkAAAA8O8CAkwXZB+GKpA8ff/9HAAuYcOGY2rffpGSk22SpAEDHtFrr9X5l1cBAIC/8KxJAAAA4N/d/CyVixel/PnNtWQ1nqkCAG7ijz8u6amn5ikuzn7rv06dKmvs2EayWCyGywAAAAAAAOBKvL1T9xcuNNeRHe5oqDJr1izVrl1bhQsX1rFjxyRJ48aN01dffZWlcchGZ342XQAgh5UsmVdt2lSUJDVqVFbTprWQhwcDFQAAAAAAAGSt4ODU/eXLzXVkh0wPVSZOnKjBgwerSZMmunLlipL/fOJM3rx5NW7cuKzuQ3Y4u1Na1dF0BYAc5uXlocmTm2vChCZatKitfHw8TScBAAAAAADARTVvbt+WKWO2I6tleqjyySefaPLkyRo2bJg8PVPfkKtWrZr27NmTpXHIJud2pl0XqmGmA0COs1gsev75h5U7t4/pFAAAAAAAALiwRx6xb/+8LsNlZHqocuTIET344IPpjvv6+iomJiZLopCDwjpLD/Q3XQEgG8TFJaljx8X65ZezplMAAAAAAADgZnx97duEBLMdWS3TQ5VSpUpp165d6Y6vWbNGFStWzIom5KSij0s8pBpwOUlJVnXqtFjz5+/VY49N0w8/RJlOAgAAAAAAgBv562H1V6+a7chqXpl9wSuvvKL+/fsrLi5ONptNW7du1bx58zRy5EhNmTIlOxoBAJlgs9n0/PMrtXTpAUn2AQvPTwEAAAAAAIAJR45IVqvkkelLPBxTpocqPXr0UFJSkl599VXFxsaqU6dOKlKkiD7++GN16NAhOxoBAJnw+uvfasoU+7OTvL09tHRpez3ySBHDVQAAAAAAAHAnRW56O+q776R69YylZKlMD1UkqXfv3urdu7cuXLggq9WqkJCQrO4CANyBceN+1IgRP0iy39lv1qyn1aBBGcNVAAAAAAAAcDfFi6fuL1zo5kOVvxQoUCCrOgAAd2n27F/04ovrUtafftpE7dtXMlgEAAAAAAAAd+XlJYWESOfOST//bLom62R6qFKqVClZ/uHB5ocPH76rIABA5q1e/bt69PgqZf3WW4+rX7+HDRYBAAAAAADA3TVuLM2YYboia2V6qDJo0KA068TERO3cuVNr167VK6+8klVdAIAMOnEiWm3afKmkJKskqV+/anrrrccNVwEAAAAAAMDdPfkkQxUNHDjwlsc/++wzbdu27a6DAACZU7RooEaNaqABA9aobdv7NH5843+8ohAAAAAAAADICTc/QeT6dSlPHnMtWcUjq75Q48aNtXjx4qz6cgCATPi//3tEERFdNXNmS3l6Ztkf7QAAAAAAAMAdCwpK3f/1V3MdWemuHlR/s0WLFik4ODirvhwA4B/YbLZ0V6PUr1/aUA0AAAAAAACQnq9v6v7Zs+Y6slKmhyoPPvhgmjfybDabzpw5o/Pnz2vChAlZGgcASO/atXg1bjxHgwfXVKtWYaZzAAAAAAAAgNsqUEC6cEHKlct0SdbI9FClZcuWadYeHh4qWLCgnnjiCVWoUCGrugAAtxAfn6Snn16gTZuOa8uWE5oxo6W6dKliOgsAAAAAAAC4peLF7UMVV5GpoUpSUpJKliyphg0b6p577smuJgDALSQnW9W161J9880RSVJQkK8efJA/iwEAAAAAAICckqmnGXt5een5559XfHx8dvUAAG7BZrPphRfWaOHCfZKkXLm8tWpVJ913X4jhMgAAAAAAAOD24uLsW6vVbEdWydRQRZKqV6+unTt3ZkcLAOA23nnne02cuE2S5OXloUWL2qpmzWKGqwAAAAAAAIB/9tdQ5bXXpNmzzbZkhUw/U6Vfv3566aWXdOLECVWtWlW5c+dO8/kqVbi3PwBkpc8+26p33vk+ZT19egs1bnyvwSIAAAAAAAAgY44fT93/8kupSxdzLVkhw0OVZ599VuPGjVP79u0lSQMGDEj5nMVikc1mk8ViUXJyctZXAoCbWrBgr154YU3Kety4hurcmeE1AAAAAAAAnMOLL0qjRtn3T52y3wbMI9P30HIcGR6qzJgxQ++//76OHDmSnT0AgD9dunRDvXuvkM1mX7/22qMaOLCG2SgAAAAAAAAgE9q1k8qWlZ57zr7etk165BGzTXcjw0MV25/v6pUoUSLbYgAAqYKD/fXVVx3UosV8tW9/n957r57pJAAAAAAAACDTKlRI3V+61LmHKpm6yMZisWRXBwDgFurWLaVt257TxInN+DMYAAAAAAAATilXrtT9yEgpOtpcy93K1IPqy5Ur969v6l26dOmugpADLu41XQDgNmJiEpQ7t0+aY+XK5TdUAwAAAAAAAGSN7t2l6dPt+xcuSIGBJmvuXKaGKu+8846CgoKyqwU54deZ0vaxpisA3MLFi7GqU2ea2ratqLfffoIrUwAAAAAAAOAy+vRJHaqsXCkNGGA0545laqjSoUMHhYSEZFcLcsLxb9Ou899npgNAGjExCWradK7277+g4cM3yMvLQ2+88bjpLAAAAAAAACBLeHun7s+cKfXtK/n43P58R5XhZ6rwE9Mu4Mga6dcZqesnxkqFa5rrASBJSkhIVuvWX+qnn05Kku65J486d65iuAoAAAAAAADIWu3bp+7/+qu5jruR4aGKzWbLzg7khI1D067vbS0xLAOMslpt6t59mdatOyRJCgry1bp1XVS6dD7DZQAAAAAAAEDWGjgwdf+PP8x13I0M3/7LarVmZwdyQvzV1P0HX5ACi5lrASCbzaZBg9Zq3ry9kiQ/Py+tWNFRVaqEGi4DAAAAAAAAst7Nt/s6eNBcx93I8JUqcCG5QqR6401XAG5vxIiN+uSTrZIkT0+LvvyyjerUKWG4CgAAAAAAAMg+f908qZiT/sw/QxUAMGDSpG16/fX1KespU55S8+blDRYBAAAAAAAA2a9ZM9MFd8f4UGXChAkqVaqU/Pz8VLVqVW3cuDFDr9u0aZO8vLz0wAMPZG+gq9j5qRR91HQFAEkxMQl6990NKesPP2yg7t0fMBcEAAAAAAAA5JDkZPs2JsZsx50yOlRZsGCBBg0apGHDhmnnzp2qU6eOGjdurKioqH983dWrV9WtWzc9+eSTOVTq5OIuS+sHpa69/I2lAJBy5/bRxo09VLZssF55pZZefrmW6SQAAAAAAAAgR1y/bt/Onm22404ZHaqMGTNGPXv2VK9evRQWFqZx48apWLFimjhx4j++rk+fPurUqZNq1qyZQ6VOLuG6ZEtOXT8y1FwLAElSqVL5tHVrL33wQX3TKQAAAAAAAECO8fS0b+PjzXbcKWNDlYSEBG3fvl3h4eFpjoeHh2vz5s23fd20adN06NAhvfXWWxn6deLj4xUdHZ3mw62Vayvd38d0BeB2Tp26pqQka5pj+fL5y/LXk7kAAAAAAAAAN1DfyX/G2NhQ5cKFC0pOTlZoaGia46GhoTpz5swtX/P7779ryJAhmjNnjry8vDL064wcOVJBQUEpH8WKFbvrdgDIjFOnrql27alq23ah4uKSTOcAAAAAAAAAxoSFmS64O8YfVP/3n9K22Wy3/Mnt5ORkderUSe+8847KlSuX4a8/dOhQXb16NeXj+PHjd90MABl1+fINNWw4W0ePXtGyZQc0aNBa00kAAAAAAACAMR43TSUSEsx13KmMXe6RDQoUKCBPT890V6WcO3cu3dUrknTt2jVt27ZNO3fu1P/93/9JkqxWq2w2m7y8vBQREaF69eqle52vr698fX2z55sAgH8QG5uo5s3nae/ec5KkkiXz6s03HzdcBQAAAAAAAJgTEpK6n5Qk+fiYa7kTxq5U8fHxUdWqVRUZGZnmeGRkpGrVqpXu/MDAQO3Zs0e7du1K+ejbt6/Kly+vXbt2qXr16jmVDgD/KjExWe3aLdSmTfar40JCcisioosKFw4wXAYAAAAAAADgThm7UkWSBg8erK5du6patWqqWbOmPv/8c0VFRalv376S7LfuOnnypGbOnCkPDw9VqlQpzetDQkLk5+eX7jgAmGS12tSr1wqtWvW7JCkgwEdr1nTWvffmN1wGAAAAAAAA4G4YHaq0b99eFy9e1PDhw3X69GlVqlRJq1evVokSJSRJp0+fVlRUlMlEAMgUm82mV1+N1MyZuyVJPj6e+uqrDnrooUKGywAAAAAAAADzbn6mynffSU2aGEu5IxabzWYzHZGToqOjFRQUpKtXryowMNB0Ts6IPi5NLm7fL9dWav6l2R7AhY0atUn/+c/XkiQPD4sWLmyrVq3CDFcBANydW/4bGHeF3zMAAADITtWq2bdFi0rLlhlNkZS5f/8ae6YKALiahIRkLV68P2X9v/81ZaACAAAAAAAA/E2ZMvZtrlxmO+4EQxUAyCI+Pp765ptuatCgtP7733rq3buq6SQAAAAAAADA4fTpY98641DF6DNVAMDV5Mnjo9WrO8vT02I6BQAAAAAAAEAW40oVd3As0nQB4LL27Tuvy5dvpDnm5eUhi4WhCgAAAAAAAOBqGKq4urM7pYiepisAl3T48GXVqzdDjz02XSdPRpvOAQAAAAAAAJDNGKq4ukv70q6LP2mmA3AxZ85cV4MGs3T2bIz27j2nwYMjTCcBAAAAAAAAyGYMVdxJ5V7S/X1MVwBO7+rVODVqNFuHD1+WJFWsWFATJzY1XAUAAAAAAAA4l127TBdkHkMVd1LwftMFgNOLi0vSU0/N1+7dZyVJxYsHad26LgoO9jdcBgAAAAAAADgHqzV132Yz13EnGKoAQAYlJVnVseNibdhwTJJUoEAuRUR0UdGigYbLAAAAAAAAAOfx0EOp+7//bq7jTjBUAYAMsNls6tt3pZYtOyBJyp3bW6tXd1L58gUMlwEAAAAAAADOJTg4dd/ZbgHGUAUAMuC1177RF1/slCR5e3to2bIOevjhIoarAAAAAAAAAOcW6GQ3gWGoAgD/wmq16eTJa5Iki0WaPbuV6tcvbbgKAAAAAAAAcF6VK9u3znb7Ly/TAQDg6Dw8LJo+vaUKFsylMmWC1a7dfaaTAAAAAAAAAKf21zBlxgzphRfMtmQGQxUAyAAPD4tGj25oOgMAAAAAAABwCU8+Ka1aZd+PjZVy5TLbk1Hc/gsAbuHHH09o//7zpjMAAAAAAAAAl9SrV+q+1WquI7MYqgDA3+zde06NG8/Ro49O008/nTCdAwAAAAAAALice+4xXXBnGKoAwE2OHr2ihg1n68qVOF26dEMjR/5gOgkAAAAAAACAg2CoAgB/OncuRuHhs3Tq1DVJ0sMPF9asWU8brgIAAAAAAADgKBiqAICka9fi1aTJHP3++yVJUvny+bVqVScFBPgaLgMAAAAAAADgKBiquLrEWNMFgMOLj0/S008v0PbtpyVJRYoEKCKiqwoWzG24DAAAAAAAAIAjYajiyo5/L0U+Z7oCcGjJyVZ16bJU33xzRJKUL5+fIiK6qnjxIMNlAAAAAAAAABwNQxVX9tuCtOs8Rcx0AA7KZrPp//5vtRYt2idJypXLW6tWdVLFigUNlwEAAAAAAABwRAxVXJk1KXW/fHupdDNzLYADslgsuvfe/JIkLy8PLV7cTjVrFjNcBQAAAAAAALg+iyV1/+JFcx2Z5WU6ADmk+jDJ09t0BeBwBg+uqYIFc8nT00ONGpU1nQMAAAAAAAC4Ba+bphOJieY6MouhCgC317Xr/aYTAAAAAAAAALcVHW26IOO4/RcAtxIRcUgrVvxmOgMAAAAAAADAn65cMV2QcQxVALiNn346oVatFujppxdoxoxdpnMAAAAAAAAAt1a4sH3r42O2IzMYqgBwC/v3n1eTJnMVE5Oo5GSbVq78XTabzXQWAAAAAAAA4Lby5jVdkHkMVQC4vOPHryo8fLYuXbohSapbt6RmzXpaFovFcBkAAAAAAADgvmJi7NvDh812ZAZDFQAu7eLFWIWHz9aJE/anXT30UCEtW9ZBfn5ehssAAAAAAAAA93bsmH3r5URv1TFUAeCyrl9PUNOmc3XgwAVJUtmywVqzprMCA30NlwEAAAAAAACoWdO+vX7dbEdmMFRxVUnx0qHlpisAYxISktW69Zf66aeTkqRChfIoIqKLQkJyGy4DAAAAAAAAIElXr9q3a9ea7cgMhiquKvI5Kfas6QrAmOeeW6GIiEOSpLx5/bRuXReVKpXPcBUAAAAAAACAvxQoYN+WLWu2IzMYqriq0z+l7vsGSXlLm2sBDOjcubJy5/aWn5+XVq7sqMqVQ00nAQAAAAAAALjJww/bt99+a7YjM5zo8S+4Y113St7c8gjupUGDMvr222d04UKsatcubjoHAAAAAAAAwN+cOmW6IPMYqrg637xSUCnTFYARjzxSxHQCAAAAAAAAgNu4997U/dhYKVcucy0Zxe2/ALiExYv3aeTIjbLZbKZTAAAAAAAAAGTAzc9S2bLFXEdmcKUKAKf37bdH1KnTEiUkJOvcuRiNHt1QHh4W01kAAAAAAAAA/kH58qn7CQnmOjKDK1Vckc0mRR81XQHkiO3bT6lFi/lKSEiWJF29Gi8L8xQAAAAAAADA4Xl6StWrm67IHIYqrmhpMyk53nQFkO0OHryoxo3n6Pp1+xj7qafK6/PPm8vCVAUAAAAAAABwCjdu2LerV5vtyCiGKq7mxkXpyE2/+wKLm2sBstHJk9EKD5+l8+djJUl16hTX/Pmt5eXFH2sAAAAAAACAszh0yL51lmeq8O6jq7Elp103nG4kA8hOly7dUMOGs3Xs2FVJUpUqoVq+vKP8/b0NlwEAAAAAAADIjJYtTRdkDkMVV1bmKSn0QdMVQJaKjU1U8+bz9Ouv5yVJpUrl1dq1nZU3r5/hMgAAAAAAAACZ9dczVfLnN9uRUQxVADiVfv1WafPm45Kk0NDciojoqkKFAgxXAQAAAAAAALgTPj72bVCQ2Y6MYqgCwKkMG1ZHJUvmVWCgr9as6ayyZYNNJwEAAAAAAABwE16mAwAgM+69N782bXpWR49e0YMPFjKdAwAAAAAAAMCNMFQB4PBsNpssFkvKunDhABUuzC2/AAAAAAAAAOQsbv8FwKFNmbJDHTosVnx8kukUAAAAAAAAAG6OK1UAOKxlyw6oT5+VslptungxVqtWdZKvL39sAQAAAAAAADCDK1UAOKTvvz+qDh0WyWq1SZKqVAmVj4+n4SoAAAAAAAAA7oyhiqu5cdF0AXDXdu06o6eemq/4+GRJUpcuVfTRR+FpnqsCAAAAAAAAwHXExZkuyBiGKq7k+ilpekXTFcBdOXTokho1mq3o6HhJUuPGZTV16lPy8GCgAgAAAAAAALiaZPvPVevUKbMdGcVQxZWc3JR2HRxmpgO4Q6dPX1N4+GydPRsjSapZs6gWLmwrb29u+wUAAAAAAAC4ooAA0wWZw1DFVeUtK9UYZroCyLArV+LUuPEcHT58WZJ0330FtXJlJ+XO7WO4DAAAIHtNmDBBpUqVkp+fn6pWraqNGzf+4/mfffaZwsLC5O/vr/Lly2vmzJm3PXf+/PmyWCxq2bJlFlcDAAAAWSMw0L7NlctsR0Z5mQ5ANrn/ecnHyUZ8cGuvvfaNdu8+K0kqXjxI69Z1UXCwv+EqAACA7LVgwQINGjRIEyZMUO3atTVp0iQ1btxY+/btU/HixdOdP3HiRA0dOlSTJ0/Www8/rK1bt6p3797Kly+fmjdvnubcY8eO6eWXX1adOnVy6tsBAAAAXB5XqgBwCCNHPqknniipAgVyKSKii4oUCTSdBAAAkO3GjBmjnj17qlevXgoLC9O4ceNUrFgxTZw48Zbnz5o1S3369FH79u1VunRpdejQQT179tQHH3yQ5rzk5GR17txZ77zzjkqXLp0T3woAAADgFhiqAHAIQUF+WrOmszZu7KHy5QuYzgEAAMh2CQkJ2r59u8LDw9McDw8P1+bNm2/5mvj4ePn5+aU55u/vr61btyoxMTHl2PDhw1WwYEH17NkzQy3x8fGKjo5O8wEAAAAgPYYqAIyJi0tKs/bz81KFCgxUAACAe7hw4YKSk5MVGhqa5nhoaKjOnDlzy9c0bNhQU6ZM0fbt22Wz2bRt2zZNnTpViYmJunDhgiRp06ZN+uKLLzR58uQMt4wcOVJBQUEpH8WKFbvzbwwAAABwYQxVABgxevRm1agxRWfOXDedAgAAYJTFYkmzttls6Y795Y033lDjxo1Vo0YNeXt7q0WLFurevbskydPTU9euXVOXLl00efJkFSiQ8R9WGTp0qK5evZrycfz48Tv+fgAAAABXxlAFQI6bOXO3Xn45Urt3n9Wjj07VtWvxppMAAAByXIECBeTp6ZnuqpRz586lu3rlL/7+/po6dapiY2N19OhRRUVFqWTJkgoICFCBAgV06NAhHT16VM2bN5eXl5e8vLw0c+ZMLV++XF5eXjp06NAtv66vr68CAwPTfAAAAABIj6EKgBy1cuVBPfvsVynrbt3uV0CAr8EiAAAAM3x8fFS1alVFRkamOR4ZGalatWr942u9vb1VtGhReXp6av78+WrWrJk8PDxUoUIF7dmzR7t27Ur5eOqpp1S3bl3t2rWL23oBAAAAd8nLdAAA9/HDD1Fq23ahkpNtkqT/+7+H9cYbjxmuAgAAMGfw4MHq2rWrqlWrppo1a+rzzz9XVFSU+vbtK8l+W66TJ09q5syZkqSDBw9q69atql69ui5fvqwxY8Zo7969mjFjhiTJz89PlSpVSvNr5M2bV5LSHQcAAACQeQxVAOSIPXvOqnnzeSkPp+/QoZI+/rjxbe8XDgAA4A7at2+vixcvavjw4Tp9+rQqVaqk1atXq0SJEpKk06dPKyoqKuX85ORkjR49Wr/99pu8vb1Vt25dbd68WSVLljT0HQAAAABZIzbWdEHGMFQBkO2OHr2ihg1n68qVOElSeHgZzZjRUh4eDFQAAAD69eunfv363fJz06dPT7MOCwvTzp07M/X1//41AAAAAEficdNDSuLjJV8Hf1IAz1RxFcmJ0sGFpiuAdM6di1GDBrN0+vR1SdIjjxTR4sXt5OPjabgMAAAAAAAAgGkFC6bux8eb68gorlRxBRf2SjMqm64AbunTT7fqjz8uSZIqVCigVas6KU8eH8NVAAAAAAAAAByBsz0dgKGKK/jupfTHivLwbziGt99+Qpcv39CyZb9p3bouKlAgl+kkAAAAAAAAALgj3P7LFcScTrt+9nfpnmpmWoC/8fCwaPz4xtq+/TkVLx5kOgcAAAAAAAAA7hhDFVcz2CrlK2u6Am7MZrPpzJnraY5ZLBaFhOQ2VAQAAAAAAAAAWYOhiivx8ne+G9DB5bz11neqUmWitm8/ZToFAAAAAAAAALIUQxUAWeaTT37Su+9u0PnzsapXb6bOn48xnQQAAAAAAADAScTFmS74dwxVAGSJefP2aMCAtSnrd9+tq4IFueUXAAAAAAAAgNvzuGlK8euv5joyiqEKgLu2du0f6tZtWcr69dfraMCA6uaCAAAAAAAAADiFm4cqnp7mOjKKoQqAu/LTTyfUuvWXSkqySpKee+4hDR9e13AVAAAAAAAAAGdRqZLpgoxjqALgju3ff15NmsxVbGyiJKl16zBNmNBUFovFcBkAAAAAAAAAZD2GKgDuSFTUVYWHz9alSzckSfXqldKcOa3k6ckfKwAAAAAAAABck5fpANwFm03aPVG6sMd0CdzQunV/6MSJaEnSQw8V0tKl7eXryx8pAAAAAAAAAFwX74A6q8RYaeb90pU/Uo955zbXA7fTu3dVWSwWjR69RWvWdFZgoK/pJAAAAAAAAADIVgxVnNXx79IOVCSp1nATJXBjvXo9pG7d7pePj6fpFAAAAAAAAADIdjz8wFlZk9Kuex2WHnjeTAvcgtVq044dp9MdZ6ACAAAAAAAAwF0wVHEFj46QgkqZroALs9lsGjhwjapXn6J583iGDwAAAAAAAAD3xFDFGSXekHZ+YroCbuS99zbo009/VlKSVc88s0xRUVdNJwEAAAAAAABAjmOo4ox+nSZFfX3TAYuxFLi+iRN/1ptvfpeynjLlKRUvHmQuCAAAAAAAAAAMYajijK4cSrsu2dBMB1zewoW/qn//1Snr0aPD1a3b/QaLAAAAAAAAAMAchirOruVyKfRB0xVwQV9/fVidOy+RzWZfDxlSW4MH1zQbBQAAAAAAAAAGMVRxdn75TRfABW3bdkpPP71AiYlWSdKzzz6gESOeNFwFAAAAAAAAAGYxVAGQxsGDF9W48Rxdv54gSWrRorwmTWoui4Vn9wAAAAAAAABwbwxVAKRx9OgVxcTYByqPPVZC8+a1lpcXf1QAAAAAAAAAAO+UAkgjPLyMIiO7qm7dklq+vIP8/b1NJwEAAAAAAACAQ/AyHQDA8dSuXVzffNONW34BAAAAAAAAyHZWq+mCjONKFcDNJSYma/HifemOM1ABAAAAAAAAkBOuXLFvd+82mpEhDFUAN2a12vTss8vVps1C/ec/kbLZbKaTAAAAAAAAALiZU6fs29BQsx0ZwVAFcFM2m00vvxyh2bN/kSR9/PFPOnDgguEqAAAAAAAAAO6mfn3TBRnHUAVwUx98sEljx/4oSfLwsGj+/DYKCytouAoAAAAAAAAAHBdDFcANTZmyQ0OHfpOy/vzzZmrZsoLBIgAAAAAAAABwfAxVnFHcZdMFcGJLl+5Xnz4rU9bvv/+kevZ8yGARAAAAAAAAADgHhirOZtcE6ddppivgpL777qg6dlwsq9X+QPrBg2vo1VdrG64CAAAAAAAAAOfAUMXZ7JuZdp2nsJkOOJ2dO0/rqafmKT4+WZLUtWsVffhhuCwWi+EyAAAAAAAAAHAODFWcSew56fRPqevGM6WgksZy4Fz8/LyUN6+fJKlp03v1xRdPycODgQoAAAAAAAAAZBRDFWdhs0kLHk9dWzylil3N9cDphIUV1KZNz6p79wf05Zdt5e3taToJAAAAAAAAAJyKl+kAZFDSDenSgdR1oermWuC0ihUL0rRpLUxnAAAAAAAAAIBT4koVZ9V6rekCOLgbNxI1YsRGJSYmm04BAAAAAAAAAJfAUMUZFasr+QSYroADS0qyqn37RRo27Fu1aDFfMTEJppMAAAAAAAAAwOkxVAFcjM1mU+/eK7RixUFJ0saNUTp8+LLhKgAAAAAAAABwfgxVABczZMjXmj59lyTJx8dTX33VQZUrh5qNAgAAAAAAAAAXwFAFcCEffbRZo0ZtliRZLNKcOa1Ur14pw1UAAAAAAAAA4BoYqgAuYvr0XXrllciU9cSJTdWmTUWDRQAAAAAAAADgWhiqAC5gxYrf1KvX8pT1u+/WVZ8+1QwWAQAAAAAAAIDrYagCOLmffjqhdu0WKTnZJkl64YVHNGxYHcNVAAAAAAAAAOB6GKoATq58+QKqVq2wJKljx0oaN66RLBaL4SoAAAAAAAAAcD1epgMA3J28ef0UEdFFo0dv0auv1paHBwMVAAAAAAAAAMgODFWchc1qugAOzN/fW6+//pjpDAAAAAAAAABwadz+yxkkxkrTwkxXwEFER8erZ8+vdP58jOkUAAAAAAAAAHArDFWcwclN0vUTqeuAouZaYFRcXJJatpyvqVN3qU6daTp27IrpJAAAAAAAAABwGwxVnMHlg2nXtd810wGjkpOt6tx5idavPypJOn8+VrGxiWajAAAAAAAAAMCNMFRxdMe+lr79v9R1zbelwBLGcmCGzWbT88+v0pIl+yVJuXJ5a/XqTgoLK2i4DAAAAAAAAADcB0MVRxf1bdp1/opmOmDUG2+s1+TJOyRJ3t4eWrKknapX5zZwAAAAAAAAAJCTGKo4k/ufl+5tZboCOezjj3/Uf/+7UZJksUgzZrRUw4ZlDVcBAAAAAAAAgPthqOJMyrWVPDxNVyAHzZnziwYNWpey/vjjRurYsbLBIgAAAAAAAABwXwxVAAf1888n1b37VynrN954TC+8UN1gEQAAAAAAAAC4N4YqgIN66KFC6tHjAUlSnz5V9c47T5jMAQAAAAAAAAC352U6AMCteXp6aNKkZqpbt6TatbtPFovFdBIAAAAAAAAAuDWGKoADs1gsPEMFAAAAAAAAABwEt/8CHMT58zGqW3eGdu06YzoFAAAAAAAAAHALDFUAB3DtWryaNJmr7747qscfn67Nm4+bTgIAAAAAAAAA/A1DFcCw+PgktWr1pbZtOyVJypPHR4ULBxiuAgAAAAAAAAD8HUMVwKDkZKueeWaZvv76sCQpb14/rVvXRSVL5jUbBgAAAAAAAABIh6EKYIjNZtOAAWu0YMGvkiR/fy+tWtVJlSqFGC4DAAAAAAAAANwKQxXAkOHDv9eECdskSZ6eFi1a1E61ahUzXAUAAAAAAAAAuB3jQ5UJEyaoVKlS8vPzU9WqVbVx48bbnrtkyRI1aNBABQsWVGBgoGrWrKl169blYC2QNSZM+Flvv/19ynratBZq0uReg0UAAAAAAAAAgH9jdKiyYMECDRo0SMOGDdPOnTtVp04dNW7cWFFRUbc8f8OGDWrQoIFWr16t7du3q27dumrevLl27tyZw+XAndu377z+7/9Wp6zHjAlX1673GywCAAAAAAAAAGSE0aHKmDFj1LNnT/Xq1UthYWEaN26cihUrpokTJ97y/HHjxunVV1/Vww8/rHvvvVcjRozQvffeqxUrVuRweQ65cUna8bHpCmSxihUL6rPPmshikYYMqa0XX6xpOgkAAAAAAAAAkAFepn7hhIQEbd++XUOGDElzPDw8XJs3b87Q17Barbp27ZqCg4Nve058fLzi4+NT1tHR0XcWbMKGV6SkWNMVyAbPP/+wqlYtrIcfLmw6BQAAAAAAAACQQcauVLlw4YKSk5MVGhqa5nhoaKjOnDmToa8xevRoxcTEqF27drc9Z+TIkQoKCkr5KFbMiR4EfulA6r5vkHRPNXMtuCuJicnpjj3ySBFZLBYDNQAAAAAAAACAO2H8QfV/f1PZZrNl6I3mefPm6e2339aCBQsUEhJy2/OGDh2qq1evpnwcP378rpuN6HVU8gkwXYE7cOJEtO67b4IWLdpnOgUAAAAAAAAAcBeMDVUKFCggT0/PdFelnDt3Lt3VK3+3YMEC9ezZU19++aXq16//j+f6+voqMDAwzYdT8nXSbjd36dINNWw4W7//fknt2i3UwoW/mk4CAAAAAAAAANwhY0MVHx8fVa1aVZGRkWmOR0ZGqlatWrd93bx589S9e3fNnTtXTZs2ze5M4I7FxCSoWbO52rfvvCSpdOl8euyxEoarAAAAAAAAAAB3ytiD6iVp8ODB6tq1q6pVq6aaNWvq888/V1RUlPr27SvJfuuukydPaubMmZLsA5Vu3brp448/Vo0aNVKucvH391dQUJCx7wP4u8TEZLVtu1BbtpyQJN1zTx5FRHRVaGgew2UAAAAAAAAAgDtldKjSvn17Xbx4UcOHD9fp06dVqVIlrV69WiVK2H+a//Tp04qKiko5f9KkSUpKSlL//v3Vv3//lOPPPPOMpk+fntP5wC1ZrTb16PGV1qz5Q5IUFOSrtWs7q3TpfIbLAAAAAAAAAAB3w+hQRZL69eunfv363fJzfx+UfPfdd9kfBNwFm82mwYPXac6cPZIkPz8vrVjRUffff4/hMgAAAAAAAADA3TL2TBXAFb3//g/6+OOfJEmenhYtWNBGderwHBUAAAAAAAAAcAUMVYAsEhV1VcOHb0hZT57cXE89Vd5gEQAAAAAAAAAgKzFUAbJI8eJBWru2swIDffXBB/XVo8eDppMAAAAAAAAAAFnI+DNVAFfy+OMltW9fPxUuHGA6BQAAAAAAAACQxbhSxVElXJNObTZdgX9x4UJsumNFigTKYrEYqAEAAAAAAAAAZCeGKo5q4ZOmC/Avfv/9ou67b4LeeONb2Ww20zkAAAAAAAAAgGzGUMURWZOlMz+nrgtWkcSVD47k1KlrCg+frXPnYvTeexs1btyPppMAAAAAAAAAANmMoYozaBMpcTsph3HlSpwaNZqto0evSJIqVQpR9+4PGG0CAAAAAAAAAGQ/hiqOrnBtKVeI6Qr86caNRDVvPk979pyTJJUoEaR167ooXz5/w2UAAAAAAAAAgOzGUMXRWJOl+CumK3ALSUlWtW+/SD/8ECVJKlgwlyIju6pw4QDDZQAAAAAAAACAnOBlOgA3uXhAWlRfun7SdAn+xmazqVev5Vqx4qAkKSDAR2vXdtG99+Y3XAYAAAAAAAAAyClcqeJIfl+cfqCS+x4zLUjjP//5WjNm7JYk+fh4atmyDnrooUKGqwAAAAAAAAAAOYkrVRyJNTF1P7SqFBwmVX/NXA8kSRcuxGr27F8kSRaLNHduK9WrV8pwFQAAAAAAAAAgp3GliqN69L9Sk1lS/jDTJW6vQIFc2ry5p+69N1gTJzZV69YVTScBAAAAAAAAAAzgShUgA0qWzKvdu/vK39/bdAoAAAAAAAAAwBCuVAFuYf/+80pKsqY5xkAFAAAAAAAAANwbQxXgb3bvPqOaNb9Q69Zf6saNxH9/AQAAAAAAAADALTBUAW5y+PBlNWo0R1evxmv58t/07rsbTCcBAAAAAAAAABwEQxXgT2fPXld4+CydOXNdklS9ehENG1bHcBUAAAAAAAAAwFEwVAEkXb0ap0aN5ujQocuSpLCwAlq1qpNy5/YxXAYAAAAAAAAAcBQMVeD24uKS1KLFfO3adUaSVKxYoNat66L8+XMZLgMAAAAAAAAAOBKGKnBrSUlWdey4WN9/f0ySlD+/vyIiuqpYsSDDZQAAAAAAAAAAR8NQBW7LZrPp+edXatmyA5Kk3Lm9tXp1Z1WoUMBwGQAAAAAAAADAETFUgduKiUnUnj3nJEne3h5aurS9HnmkiOEqAAAAAAAAAICjYqgCt5Unj4++/rqbGjcuq1mznlaDBmVMJwEAAAAAAAAAHJiX6QD8KeastOUd0xVuJ08eH61a1UkWi8V0CgAAAAAAAADAwXGliqP4YdjfDvD/muywceMxXbp0I80xBioAAAAAAAAAgIzgnXtH8PtSae8XqWvfIKlwTXM9LmrLluNq2HC26tSZphMnok3nAAAAAAAAAACcDEMVRxDZJ+36ueOSTx4zLS7q11/PqWnTubpxI0n79p3XqFGbTCcBAAAAAAAAAJwMQxVHEHcxdf+RIZJPgLkWF3Ts2BU1bDhbly/HSZKefLKUPvywgeEqAAAAAAAAAICzYajiSEKrSXVGmq5wKefPxyg8fLZOnrwmSapatZCWLm0vX18vw2UAAAAAAAAAAGfDUAUu69q1eDVpMlcHD9qvBCpXLr/WrOmsgABfw2UAAAAAAAAAAGfEUAUuKT4+SU8/vUDbtp2SJBUuHKCIiC4qWDC34TIAAAAAAAAAgLNiqAKXk5xsVdeuS/XNN0ckSfny+SkiootKlMhrNgwAAAAAAAAA4NQYqsDl2GxSrlzekiR/fy+tXNlJ990XYrgKAAAAAAAAAODseFo3XI6Xl4emTWuhe+7Jo8ceK6FatYqZTgIAAAAAAAAAuACGKqZdOyHZrKYrXI7FYtH779c3nQEAAAAAAAAAcCHc/suk+KvStAqmK1zC0qX79euv50xnAAAAAAAAAABcGEMVky7slRJjUtehD5lrcWIREYfUvv0i1akzTVu2HDedAwAAAAAAAABwUQxVHEWeItIT40xXOJ2tW0+qVasFSky06vLlOM2du8d0EgAAAAAAAADARTFUcRTlO0je/qYrnMqBAxfUpMkcxcQkSpKefrqCxo5tZLgKAAAAAAAAAOCqGKrAKZ04Ea3w8Fm6ePGGJOmJJ0pq7tzW8vLitzQAAAAAAAAAIHvwDjSczsWLsQoPn6Xjx6MlSQ8+eI+++qqD/Py8DJcBAAAAAAAAAFwZQxU4lZiYBDVtOlf791+QJJUpk09r1nRWYKCv4TIAAAAAAAAAgKtjqAKnkZxsVZs2C/XTTyclSffck0cREV0VGprHcBkAAAAAAAAAwB0wVIHT8PT0UKNGZSRJQUG+Wreui0qXzme4CgAAAAAAAADgLngIBZzKwIE1VKBALhUvHqQqVUJN5wAAAAAAAAAA3AhDFTidzp2rmE4AAAAAAAAAALghbv8Fh/bFFzu0fPlvpjMAAAAAAAAAAGCoYsyNS9Ly1qYrHNqSJfv13HMr1arVAk2fvst0DgAAAAAAAADAzXH7L1N+my/Fnk1de+c21+KA1q8/oo4dF8tqtUmS9u07b7gIAAAAAAAAAODuuFLFlPiradf3dTPT4YB27DitFi3mKyEhWZLUvfsD+uCD+oarAAAAAAAAAADujqGKI2jxlZS3jOkKh/D77xfVqNFsXbuWIElq1qycJk9uLovFYrgMAAAAAAAAAODuGKrAYZw6dU3h4bN1/nysJOnRR4trwYI28vLitykAAAAAAAAAwDzerYZDuHz5hho2nK2jR69IkipXDtGKFR2VK5e32TAAAAAAAAAAAP7EUAXG2Ww2tWmzUHv3npMklSyZV+vWdVHevH6GywAAAAAAAAAASMVQBcZZLBYNHfqocuf2VkhIbkVEdFGhQgGmswAAAAAAAAAASMPLdAAgSfXrl9b69c/I09ND996b33QOAAAAAAAAAADpMFQxxZpkusDhPPxwEdMJAAAAAAAAAADcFrf/MuGP5dLmN01XGDVq1Ca9994G2Ww20ykAAAAAAAAAAGQIV6qYsGdK2nWuEDMdhkydulP/+c/XkqTz52M0blwjWSwWw1UAAAAAAAAAAPwzrlQxwZqQul/tZalQdXMtOWzZsgPq3XtFyjo0NA8DFQAAAAAAAACAU2CoYlqN1yU3GSps2HBMHTosktVqv+XXoEHVNXToo4arAAAAAAAAAADIGIYqyBG7d59R8+bzFB+fLEnq3LmyRo9uyFUqAAAAAAAAAACnwVAF2e7QoUtq2HC2oqPjJUmNG5fVtGkt5OHBQAUAAAAAAAAA4DwYqiBbnTlzXeHhs3X2bIwkqUaNolq4sK28vT0NlwEAAAAAAAAAkDkMVZCtundfpsOHL0uSKlYsqFWrOil3bh/DVQAAAAAAAAAAZB5DFWSrzz5rolKl8qpYsUCtW9dFwcH+ppMAAAAAAAAAALgjXqYD4NrKlAnWpk3P6tq1BBUtGmg6BwAAAAAAAACAO8ZQBVnKZrPJarXJ0zP1IqhChQJUqJDBKAAAAAAAAAAAsgC3/0KWGjbsW3XosFjx8UmmUwAAAAAAAAAAyFJcqYIsM3bsFo0c+YMk6cqVOK1b10UeHhbDVQAAAAAAAAAAZA2uVEGWmDVrtwYPjkhZP/10BQYqAAAAAAAAAACXwlAFd23VqoPq0eOrlPXbbz+ufv0eNlgEAAAAAAAAAEDWY6iCu7J583G1bbtQyck2SVK/ftX05puPG64CAAAAAAAAACDrMVTBHdu795yaNp2rGzfsD6Vv1+4+jR/fWBYLt/0CAAAAMmrChAkqVaqU/Pz8VLVqVW3cuPEfz//ss88UFhYmf39/lS9fXjNnzkzz+cmTJ6tOnTrKly+f8uXLp/r162vr1q3Z+S0AAAAAboOhCu7I0aNX1LDhbF25EidJql+/tGbObClPT35LAQAAABm1YMECDRo0SMOGDdPOnTtVp04dNW7cWFFRUbc8f+LEiRo6dKjefvtt/frrr3rnnXfUv39/rVixIuWc7777Th07dtT69eu1ZcsWFS9eXOHh4Tp58mROfVsAAACAy+IdcNyRIUO+1qlT1yRJDz9cWEuWtJOvr5fhKgAAAMC5jBkzRj179lSvXr0UFhamcePGqVixYpo4ceItz581a5b69Omj9u3bq3Tp0urQoYN69uypDz74IOWcOXPmqF+/fnrggQdUoUIFTZ48WVarVd98801OfVsAAACAy+JdcNyRzz9vrvPnY3XyZLRWreqkgABf00kAAACAU0lISND27ds1ZMiQNMfDw8O1efPmW74mPj5efn5+aY75+/tr69atSkxMlLe3d7rXxMbGKjExUcHBwbdtiY+PV3x8fMo6Ojo6M98KAAAA4Da4UgV3JDDQV6tXd9L69c+oYMHcpnMAAAAAp3PhwgUlJycrNDQ0zfHQ0FCdOXPmlq9p2LChpkyZou3bt8tms2nbtm2aOnWqEhMTdeHChVu+ZsiQISpSpIjq169/25aRI0cqKCgo5aNYsWJ3/o0BAAAALoyhCjIkOdmq6Oj4NMd8fb1UqFCAoSIAAADANVgsljRrm82W7thf3njjDTVu3Fg1atSQt7e3WrRooe7du0uSPD09050/atQozZs3T0uWLEl3hcvNhg4dqqtXr6Z8HD9+/M6/IQAAAMCFMVTBv7LZbOrff7Xq1Jmm06evmc4BAAAAXEKBAgXk6emZ7qqUc+fOpbt65S/+/v6aOnWqYmNjdfToUUVFRalkyZIKCAhQgQIF0pz70UcfacSIEYqIiFCVKlX+scXX11eBgYFpPgAAAACkx1AF/+rtt7/TpEnb9csvZ1W37gwlJCSbTgIAAACcno+Pj6pWrarIyMg0xyMjI1WrVq1/fK23t7eKFi0qT09PzZ8/X82aNZOHR+p/3n344Yd69913tXbtWlWrVi1b+gEAAAB3xIPq8Y8+/XSrhg/fkLJ+443H5OOT/rYCAAAAADJv8ODB6tq1q6pVq6aaNWvq888/V1RUlPr27SvJfluukydPaubMmZKkgwcPauvWrapevbouX76sMWPGaO/evZoxY0bK1xw1apTeeOMNzZ07VyVLlky5EiZPnjzKkydPzn+TAAAAgAthqILbmjdvjwYMWJOyHjeuoTp3/ufbBgAAAADIuPbt2+vixYsaPny4Tp8+rUqVKmn16tUqUaKEJOn06dOKiopKOT85OVmjR4/Wb7/9Jm9vb9WtW1ebN29WyZIlU86ZMGGCEhIS1KZNmzS/1ltvvaW33347J74tAAAAwGUxVMEtrVv3h7p1Wyabzb4eNqyOBg6sYTYKAAAAcEH9+vVTv379bvm56dOnp1mHhYVp586d//j1jh49mkVlAAAAAP6OZ6ognZ9+OqHWrb9UUpJVktS790N69926hqsAAAAAAAAAADCLoQrS2L//vJo2nauYmERJUqtWYZo4saksFovhMgAAAAAAAAAAzGKogjQmT96hixdvSJKeeKKk5sxpJU9PfpsAAAAAAAAAAMAzVZDGRx+FKzExWZs2HddXX3WQnx+/RQAAAAAAAAAAkBiq4G88PCwaP76xYmISlSePj+kcAAAAAAAAAAAcBvd1cnMJCck6cuRymmMWi4WBCgAAAAAAAAAAf8NQxY1ZrTZ1775MjzwyRT//fNJ0DgAAAAAAAAAADo2hipuy2WwaNGit5s3bqwsXYtW06VzFxCSYzgIAAAD+v707j6/p2v8//j6ZI03MQ0jEVA2uOa3iS2hVSC/KVVRaQw1VVcpV11ANV0tLhWpN1xRVM6WUFtWa3ZqixmtMDCVXqZoSIsn6/eGXcx0ZJCQ5OK/n45HHI3uftff+7LP22fus89lrbQAAAAB4ZJFUcVAff7xZX3yxQ5Lk7GzRjBnN5eXFkF8AAAAAAAAAAKSHpIoDmjp1l4YO/dk6PWNGczVr9owdIwIAAAAAAAAA4NFHUsXBLFlySG+/vco6PWbMS+rYsZr9AgIAAAAAAAAA4DFBUsWB/PRTtMLCvpExd6YHDKij/v3r2DcoAAAAAAAAAAAeEyRVHMTu3efUosUCJSQkSZI6d66mTz5pZOeoAAAAAAAAAAB4fJBUsYdrZ3N9k//+91ldv54gSWre/Bn961/NZLFYcj0OAAAAAAAAAAAeVy72DsDh/Pi2dOlgrm/2nXeeU548rpozZ58WLPibXFzIpwEAAAAAAAAAkBUkVXLboa//979HAcklT65tunPn6urYsZqcnOihAgAAAAAAAABAVtFdIbeZpP/933yp5OyaI5uJi7utzZtPpZpPQgUAAAAAAAAAgAdDUsVeCleR/BvkyKpv305SmzaL9cILX2nu3H05sg0AAAAAAAAAABwNSZUnTHKyUdeuK7Vq1TElJibrnXdW69KlOHuHBQAAAAAAAADAY4+kyhPEGKP331+rr776VZLk7u6s5cvbqWDB3HtuCwAAAAAAAAAATyqSKk+QMWO2KSLi35LuPDtl/vy/qUGDUvYNCgAAAAAAAACAJwRJlSfEzJlR+sc/frROT536V7VsWcGOEQEAAAAAAAAA8GQhqfIE+Pbb/6hbt5XW6ZEjX1DXrjXsGBEAAAAAAAAAAE8ekiqPuU2bTqlt2yVKTjaSpPfeq6WBA//PzlEBAAAAAAAAAPDkIanymIuLuy0nJ4sk6fXXq2js2BBZLBY7RwUAAAAAAAAAwJOHpMpjrkmTclq/voPCwipr5szm1gQLAAAAAAAAAADIXi72DgAPr3Ztf9Wu7W/vMAAAAAAAAAAAeKLRU+Ux8+efNzVrVpS9wwAAAAAAAAAAwOHQU+UxEh9/Wy1aLNCmTad06NDvGj36JZ6fAgAAAAAAAABALqGnymMiMTFZ7dot1aZNpyRJkZG/6ty5a3aOCgAAAAAAAAAAx0FS5TFgjFH37iu1YsURSdJTT7np++/DVKKEj50jAwAAAAAAAADAcZBUeQwMGrRes2btlSS5uTlr+fK2Cgoqbt+gAAAAAAAAAABwMCRVHnFjx27Tp59ulSRZLNLXX7fUiy+WsXNUAAAAAAAAAAA4HpIqj7CvvvpV/fuvs05PnBiqV1+tZMeIAAAAAAAAAABwXCRVHlGrVx/Tm29+a50ePryB3n77WfsFBAAAAAAAAACAgyOp8ogqUya/9UH0vXo9q6FD69s5IgAAAAAAAAAAHJuLvQNA2gIDC2nr1jc1adJOffTRC7JYLPYOCQAAAAAAAAAAh0ZS5RHm5+ejkSNftHcYAAAAAAAAAABADP/1yLhw4YYGDFinhIQke4cCAAAAAAAAAADSQE+VR8DVq7fUtOlc7dlzXvv2/VdLl7aRl5ebvcMCAAAAAAAAAAB3oaeKnd28maiWLRdqz57zkqQDBy7ojz/i7RwVAAAAAAAAAAC4F0kVO0pKStbrr3+jn36KliQVKOCptWvfkL9/XjtHBgAAAAAAAAAA7kVSxU6Mkd55Z7WWLj0sScqTx1WrVrVXxYqF7RwZAAAAAAAAAABIi92TKpMmTVLp0qXl4eGhmjVravPmzRmW37hxo2rWrCkPDw+VKVNGU6ZMyaVIs1f4sgqaOnW3JMnFxUlLl7bR88/72TkqAAAAAAAAAACQHrsmVRYuXKj33ntPQ4YMUVRUlOrVq6emTZvq9OnTaZaPjo5WaGio6tWrp6ioKA0ePFi9e/fW0qVLcznyhzNhcy2N+LaCdXr27FfUpEk5O0YEAAAAAAAAAADux65JlYiICHXp0kVdu3ZVhQoVNH78ePn7+2vy5Mlplp8yZYpKliyp8ePHq0KFCuratavefPNNffbZZ7kc+YNbeaCs+nzb1Dr9+edN1L59ZTtGBAAAAAAAAAAAMsNuSZWEhATt3r1bjRs3tpnfuHFjbdu2Lc1ltm/fnqp8SEiIdu3apdu3b6e5zK1bt3T16lWbP3uqV+as6pU+JUn64IN66t27ll3jAQAAAAAAAAAAmWO3pMrFixeVlJSkokWL2swvWrSoYmNj01wmNjY2zfKJiYm6ePFimsuMGjVKefPmtf75+/tnzw48oHx5bmlN9zma0ilK//xnQ7vGAgAAAAAAAACAvQUESFWrSoUK2TuS+3OxdwAWi8Vm2hiTat79yqc1P8WgQYPUr18/6/TVq1ftm1h585g8ZfSWk6uUwX4CAAAAAAAAAOAI3n7b3hFknt2SKoUKFZKzs3OqXikXLlxI1RslRbFixdIs7+LiooIFC6a5jLu7u9zd3bMn6OzgXcLeEQAAkGuMMUpMTFRSUpK9QwGeaM7OznJxccnw5iQgu3GOB/Ao49oIAMgpdkuquLm5qWbNmlq3bp1atmxpnb9u3Tq1aNEizWVq166tlStX2sxbu3atgoKC5OrqmqPxAgCArElISND58+cVFxdn71AAh5AnTx75+vrKzc3N3qHAAXCOB/A44NoIAMgJdh3+q1+/fnrjjTcUFBSk2rVr61//+pdOnz6tHj16SLozdNdvv/2mr776SpLUo0cPffnll+rXr5+6deum7du3a8aMGZo/f749dwMAANwjOTlZ0dHRcnZ2VvHixeXm5sZdgkAOMcYoISFBv//+u6Kjo/X000/Lycluj06EA+AcD+BRx7URAJCT7JpUadu2rS5duqR//vOfOn/+vP7yl79o9erVCggIkCSdP39ep0+ftpYvXbq0Vq9erb59+2rixIkqXry4JkyYoL/97W/22gUAAJCGhIQEJScny9/fX3ny5LF3OMATz9PTU66urjp16pQSEhLk4eFh75DwBOMcD+BxwLURAJBT7P6g+p49e6pnz55pvhYZGZlqXnBwsPbs2ZPDUQEAgOzAHYFA7uHzhtzGMQfgUcd5CgCQE7i6AAAAAAAAAAAAZAJJFQAAAAAAAAAAgEwgqQIAAIBscenSJRUpUkQxMTH2DuWJ8+WXX6p58+b2DgPAAyhVqpTGjx+f7WWfBBaLRcuXL5ckxcTEyGKxaO/evXaNKTslJCSoXLly2rp1q71DeeJ89913ql69upKTk+0dCgDAAZFUAQAAuEunTp1ksVhksVjk4uKikiVL6u2339bly5dTld22bZtCQ0OVP39+eXh4qHLlyho7dqySkpJSlf35558VGhqqggULKk+ePKpYsaL+/ve/67fffsuN3coVo0aNUrNmzVSqVCl7h5JjNm7cqJo1a8rDw0NlypTRlClT7rvMzp079eKLLypfvnzKnz+/GjdunOpHw0WLFqlatWrKkyePAgICNGbMGJvXu3Xrpp07d2rLli3ZuTuAQ7n7/O7q6qoyZcqof//+unHjRo5ud+fOnerevXu2l30YDRo0sL4Xbm5uKlu2rAYNGqRbt27l+LYdyb/+9S8FBASobt269g4lx+zfv1/BwcHy9PRUiRIl9M9//lPGmAyX2bNnj1566SXly5dPBQsWVPfu3XX9+nWbMuvXr1edOnXk7e0tX19f/eMf/1BiYqL19b/+9a+yWCyaN29ejuwXAAAZIakCAABwjyZNmuj8+fOKiYnR9OnTtXLlSvXs2dOmzLJlyxQcHCw/Pz/9/PPP+s9//qM+ffro448/Vrt27Wx+UJg6daoaNWqkYsWKaenSpTp06JCmTJmiK1euaOzYsbm2XwkJCTm27vj4eM2YMUNdu3Z9qPXkZIwPKzo6WqGhoapXr56ioqI0ePBg9e7dW0uXLk13mWvXrikkJEQlS5bUL7/8oi1btsjHx0chISG6ffu2JOn7779XWFiYevTooQMHDmjSpEmKiIjQl19+aV2Pu7u72rdvry+++CLH9xN4kqWc30+ePKmPPvpIkyZNUv/+/dMsm/IZfViFCxdWnjx5sr3sw+rWrZvOnz+v48ePa/To0Zo4caKGDRuWK9t+VGRXHafniy++eOjrYk7H+DCuXr2ql156ScWLF9fOnTv1xRdf6LPPPlNErsZiEwAALsZJREFURES6y5w7d06NGjVSuXLl9Msvv+iHH37QwYMH1alTJ2uZffv2KTQ0VE2aNFFUVJQWLFigFStWaODAgTbr6ty5M9dFAIB9GAdz5coVI8lcuXLF3qEAAPDEio+PN4cOHTLx8fH2DiXLOnbsaFq0aGEzr1+/fqZAgQLW6evXr5uCBQuaVq1apVp+xYoVRpJZsGCBMcaYM2fOGDc3N/Pee++lub3Lly+nG8vly5dNt27dTJEiRYy7u7upVKmSWblypTHGmPDwcFO1alWb8uPGjTMBAQGp9mXkyJHG19fXBAQEmIEDB5patWql2lblypXNhx9+aJ2eOXOmCQwMNO7u7uaZZ54xEydOTDdOY4xZunSpKVSokM28xMRE8+abb5pSpUoZDw8PU758eTN+/HibMmnFaIwxZ8+eNW3atDH58uUzBQoUMM2bNzfR0dHW5Xbs2GEaNWpkChYsaHx8fEz9+vXN7t27M4zxYQ0YMMAEBgbazHvrrbfM888/n+4yO3fuNJLM6dOnrfP27dtnJJnjx48bY4x57bXXTOvWrW2WGzdunPHz8zPJycnWeRs2bDBubm4mLi4uzW1l9LnjOzCyKqNj5t5jLTnZmLg4+/zd9RG5r7TO7127djXFihUzxvzvvDpjxgxTunRpY7FYTHJysvnzzz9Nt27dTOHChY23t7dp2LCh2bt3r816vv32W1OzZk3j7u5uChYsaFq2bGl9LSAgwIwbN846HR4ebvz9/Y2bm5vx9fU17777brplT506ZZo3b268vLyMt7e3efXVV01sbKzNuqpWrWq++uorExAQYHx8fEzbtm3N1atXM3wvgoODTZ8+fWzmtWrVytSoUcM6nZycbD799FNTunRp4+HhYapUqWIWL15ss8yBAwdMaGio8fb2Nk899ZT5v//7P+u5LTPnaUlm2bJlxhhjoqOjjSQTFRWVbtw3b94077//vvHz8zNubm6mXLlyZvr06cYYY2bNmmXy5s1rU37ZsmXm7p890qrjKVOmmOLFi5ukpCSbZZs1a2Y6dOhgnV6xYoWpUaOGcXd3N6VLlzbDhg0zt2/fTjfW3bt3Gycnp1SfoQEDBpinn37aeHp6mtKlS5sPPvjAJCQkZBhjZo7D48ePm+bNm5siRYoYLy8vExQUZNatW5dufNlh0qRJJm/evObmzZvWeaNGjTLFixe3uX7dberUqaZIkSI273dUVJSRZI4dO2aMMWbQoEEmKCjIZrlly5YZDw8Pm2M7JibGSDInTpxIN8bH+TspACB3ZaXN5GKfVA4AAHBIXwdJN2Jzf7texaTXdz3QoidPntQPP/wgV1dX67y1a9fq0qVLad7d3KxZM5UvX17z589X27ZttXjxYiUkJGjAgAFprj9fvnxpzk9OTlbTpk117do1ff311ypbtqwOHTokZ2fnLMW/fv16+fj4aN26ddbeM5988olOnDihsmXLSpIOHjyo/fv3a8mSJZKkadOmKTw8XF9++aWqV6+uqKgodevWTV5eXurYsWOa29m0aZOCgoJS7YOfn58WLVqkQoUKadu2berevbt8fX3Vpk2bdGOMi4tTw4YNVa9ePW3atEkuLi766KOP1KRJE+3bt09ubm66du2aOnbsqAkTJkiSxo4dq9DQUB07dkze3t5pxjh37ly99dZbGb5fU6dOVVhYWJqvbd++XY0bN7aZFxISohkzZuj27ds2x0iKZ555RoUKFdKMGTM0ePBgJSUlacaMGapUqZICAgIkSbdu3Up1Z7qnp6fOnj2rU6dOWYdTCwoK0u3bt7Vjxw4FBwdnuB9Abrp5U6pXzz7b3rxZ8vR88OU9PT1tegIcP35cixYt0tKlS63n25dfflkFChTQ6tWrlTdvXk2dOlUvvviijh49qgIFCmjVqlVq1aqVhgwZojlz5ighIUGrVq1Kc3tLlizRuHHjtGDBAlWqVEmxsbH69ddf0yxrjNErr7wiLy8vbdy4UYmJierZs6fatm2rDRs2WMudOHFCy5cv13fffafLly+rTZs2+uSTT/Txxx9n+n349ddftXXrVpvhGz/44AN98803mjx5sp5++mlt2rRJr7/+ugoXLqzg4GD99ttvql+/vho0aKCffvpJPj4+2rp1q3WIpgc5T99Phw4dtH37dk2YMEFVq1ZVdHS0Ll68mKV13FvHJUqUUO/evfXzzz/rxRdflCRdvnxZa9as0cqVKyVJa9as0euvv64JEyaoXr16OnHihHWYtvDw8DS3s2nTJpUvX14+Pj428729vRUZGanixYtr//796tatm7y9vW2+JzzIcXj9+nWFhobqo48+koeHh2bPnq1mzZrpyJEjKlmyZJoxbt68WU2bNs3w/Ro8eLAGDx6c5mvbt29XcHCw3N3drfNCQkI0aNAgxcTEqHTp0qmWuXXrltzc3OTk9L+BUzz//4d4y5YtKleunG7duiUPDw+b5Tw9PXXz5k3t3r1bDRo0kCQFBASoSJEi2rx5s8qUKZPhfgAAkJ1IqgAAgNxzI1a6/ug/Q+S7777TU089paSkJN28eVOSbIayOHr0qCSpQoUKaS4fGBhoLXPs2DH5+PjI19c3SzH8+OOP2rFjhw4fPqzy5ctL0gP9YODl5aXp06fLzc3NOq9KlSqaN2+ehg4dKulOsuHZZ5+1bmfEiBEaO3asWrVqJUkqXbq0Dh06pKlTp6abVImJiVHx4sVt5rm6umr48OHW6dKlS2vbtm1atGiRTVLl3hhnzpwpJycnTZ8+XRaLRZI0a9Ys5cuXTxs2bFDjxo31wgsv2Gxr6tSpyp8/vzZu3Ki//vWvacbYvHlz1apVK8P3q2jRoum+Fhsbm+r1okWLKjExURcvXkyzjr29vbVhwwa1aNFCI0aMkCSVL19ea9askYvLna/iISEh6tu3rzp16qSGDRvq+PHj1gdVnz9/3vojp5eXl/Lly6eYmBiSKkA22LFjh+bNm2f9IV26MwThnDlzVLhwYUnSTz/9pP379+vChQvWH44/++wzLV++XEuWLFH37t2twz7efb6rWrVqmts8ffq0ihUrpkaNGsnV1VUlS5bUc889l2bZH3/8Ufv27VN0dLT8/f0lSXPmzFGlSpW0c+dOPfvss5LuJLAjIyOtiYo33nhD69evv29SZdKkSZo+fbpu376thIQEOTk5aeLEiZKkGzduKCIiQj/99JNq164t6c41aMuWLZo6daqCg4M1ceJE5c2bVwsWLLAmlVOuI5Ie6DydkaNHj2rRokVat26dGjVqZI0pq+6tY+nOsHB3HwuLFy9WgQIFrNMff/yxBg4caL0GlilTRiNGjNCAAQPSTaqkdV2U7iSrUpQqVUp///vftXDhQpukyoMch1WrVrU57j766CMtW7ZMK1asUK9evdKMMSgoKNUzvu5VoECBdF+LjY1N9Ry1lOtkbGxsmkmVF154Qf369dOYMWPUp08f3bhxw5q0OX/+vKQ718Xx48dr/vz5atOmjWJjY/XRRx/ZlElRokQJxcTEZLgPAABkN5IqAAAg93gVeyy227BhQ02ePFlxcXGaPn26jh49qnfffTdVOZPOg1iNMdZkwN3/Z8XevXvl5+dn8wPVg6hcubJNQkWSwsLCNHPmTA0dOlTGGM2fP1/vvfeeJOn333/XmTNn1KVLF3Xr1s26TGJiovLmzZvuduLj41PdVSpJU6ZM0fTp03Xq1CnFx8crISFB1apVyzDG3bt36/jx46nuZL5586ZOnDghSbpw4YI+/PBD/fTTT/rvf/+rpKQkxcXF6fTp0+nG6O3t/cB3R6e4ty5TjoH06jg+Pl5vvvmm6tatq/nz5yspKUmfffaZQkNDtXPnTnl6eqpbt246ceKE/vrXv+r27dvy8fFRnz59NGzYsFQ9kzw9PRUXF/dQ+wBkNw+POz1G7LXtrEhJmicmJur27dtq0aKFzTMZAgICbH5s3717t65fv66CBQvarCc+Pt56Ptq7d6/N+TIjr776qsaPH68yZcqoSZMmCg0NVbNmzaxJ1rsdPnxY/v7+1oSKJFWsWFH58uXT4cOHrUmVUqVK2ZzbfH19deHCBUmpe+h9//33qvf/uxWFhYVpyJAhunr1qj799FP5+Pjob3/7myTp0KFDunnzpl566SWbmBISElS9enXrfterVy/NXnrSg52nM7J37145Ozs/dFL53jqW7rwX3bt316RJk+Tu7q65c+eqXbt21nPw7t27tXPnTptEVcqNF3FxcWk+Bye96+KSJUs0fvx4HT9+XNevX1diYmKq3iwPchzeuHFDw4cP13fffadz584pMTFR8fHxGb7fnp6eKleuXLqvZ0ZWr4uVKlXS7Nmz1a9fPw0aNEjOzs7q3bu3ihYtan2/GzdurDFjxqhHjx5644035O7urqFDh2rLli1cFwEAjwSSKgAAIPc84BBcuc3Ly8v6I8OECRPUsGFDDR8+3KangXTnB686deqkWv4///mPKlasaC175coVnT9/Pku9VTzvM56Nk5NTqqROWg+z9fLySjWvffv2GjhwoPbs2aP4+HidOXNG7dq1k3TnjmfpzhBg9/bqyGjosUKFCuny5cs28xYtWqS+fftq7Nixql27try9vTVmzBj98ssvGcaYnJysmjVrau7cuam2k/IjU6dOnfT7779r/PjxCggIkLu7u2rXrp3hg+4fdvivYsWKKTbWdvi6CxcuyMXFJdUPXSnmzZunmJgYbd++3TrUybx585Q/f359++23ateunSwWiz799FONHDlSsbGxKly4sNavXy9Jqe4A/uOPP1L9GAjYm8XycENw5aaUpLmrq6uKFy+eKiGQ1vnI19fXZritFCnDN97vfH03f39/HTlyROvWrdOPP/6onj17asyYMdq4cWOqWNJLyt87/97lLBaL9Vx+bw+9EiVKWP/Pmzev9Vr39ddfq1KlSpoxY4a6dOliXX7VqlU2y0iy9pS4334/yHk6Izl5XWzWrJmSk5O1atUqPfvss9q8ebNND9Xk5GQNHz7c2oPzbmklTqQ718X9+/fbzPv3v/9t7dUUEhJi7ekzduzYDGPMzHH4/vvva82aNfrss89Urlw5eXp6qnXr1hm+3w87/Fd610Up456f7du3V/v27fXf//5XXl5eslgsioiIsOnZ0q9fP/Xt21fnz59X/vz5FRMTo0GDBqXq/cJ1EQBgDyRVAAAA7iM8PFxNmzbV22+/reLFi6tx48YqUKCAxo4dmyqpsmLFCh07dsyagGndurUGDhyo0aNHa9y4canW/eeff6b5XJUqVaro7NmzOnr0aJq9VQoXLqzY2FibH9fuN4RHCj8/P9WvX19z585VfHy8GjVqZP3xo2jRoipRooROnjyZbnIhLdWrV9fXX39tM2/z5s2qU6eOevbsaZ2XckdtRmrUqKGFCxeqSJEiqe7evXvdkyZNUmhoqCTpzJkz9x1X/2GH/6pdu7Z1fP0Ua9euVVBQULp3asfFxcnJycnmB9CU6ZQfLVOkjO0vSfPnz1ft2rVVpEgR6+snTpzQzZs3rXeJA8i6u5PmmVGjRg3FxsbKxcUlVZIzRZUqVbR+/Xp17tw5U+v09PRU8+bN1bx5c73zzjsKDAzU/v37VaNGDZtyFStW1OnTp3XmzBlrb5VDhw7pypUr6Q4/ea/M9tBzdXXV4MGDNWjQIL322muqWLGi3N3ddfr06XR7hlSpUkWzZ89O95lSD3KezkjlypWVnJysjRs3Wof/ulvhwoV17do13bhxw5qUyOx10dPTU61atdLcuXN1/PhxlS9fXjVr1rS+XqNGDR05ciRLx0716tU1efJkm+v01q1bFRAQoCFDhljLnTp16r7rysxxuHnzZnXq1EktW7aUJF2/fv2+w2I97PBftWvX1uDBg5WQkGDtcbp27VoVL1483TjvlnLNnTlzpjw8PFL1jLJYLNYh1ObPny9/f3+bz0lKD1auiwCA3OZ0/yIAAACOrUGDBqpUqZJGjhwp6c6PclOnTtW3336r7t27a9++fYqJidGMGTPUqVMntW7d2vrMEH9/f40bN06ff/65unTpoo0bN+rUqVPaunWr3nrrLWvy5V7BwcGqX7++/va3v2ndunWKjo7W999/rx9++MEa0++//67Ro0frxIkTmjhxor7//vtM71NYWJgWLFigxYsX6/XXX7d5bdiwYRo1apQ+//xzHT16VPv379esWbNs7tq9V0hIiA4ePGjTW6VcuXLatWuX1qxZo6NHj2ro0KHauXNnpmIrVKiQWrRooc2bNys6OlobN25Unz59dPbsWeu658yZo8OHD+uXX35RWFjYfe9i9vb2Vrly5TL8y+jHxx49eujUqVPq16+fDh8+rJkzZ2rGjBnq37+/tcyyZcsUGBhonX7ppZd0+fJlvfPOOzp8+LAOHjyozp07y8XFRQ0bNpQkXbx4UVOmTNF//vMf7d27V3369NHixYutz1VJkfIg3rJly973PQSQPRo1aqTatWvrlVde0Zo1axQTE6Nt27bpgw8+0K5dd3pfhoeHa/78+QoPD9fhw4e1f/9+jR49Os31RUZGasaMGTpw4IBOnjypOXPmyNPTUwEBAWluu0qVKgoLC9OePXu0Y8cOdejQQcHBwQoKCsr2fW3fvr0sFosmTZokb29v9e/fX3379tXs2bN14sQJRUVFaeLEiZo9e7YkqVevXrp69aratWunXbt26dixY5ozZ46OHDki6cHO0xkpVaqUOnbsqDfffFPLly9XdHS0NmzYoEWLFkmSatWqpTx58mjw4ME6fvy45s2bp8jIyEyvPywsTKtWrdLMmTNTXRc//PBDffXVVxo2bJgOHjyow4cPa+HChTbPR7lXw4YNdePGDR08eNA6r1y5cjp9+rQWLFigEydOaMKECVq2bNl9Y8vMcViuXDl988032rt3r3799Ve1b98+VfL+XinDf2X0l1FSpX379nJ3d1enTp104MABLVu2TCNHjlS/fv2siaQdO3YoMDBQv/32v2fqffnll9qzZ4+OHj2qiRMnqlevXho1apTNTSZjxozR/v37dfDgQY0YMUKffPKJJkyYYNNr9t///re1BxQAALmJpAoAAEAm9OvXT9OmTdOZM2ck3emB8vPPP+vMmTOqX7++nnnmGUVERGjIkCFasGCBTc+Enj17au3atfrtt9/UsmVLBQYGqmvXrvLx8bH5Qf5eS5cu1bPPPmu9a3jAgAFKSkqSJFWoUEGTJk3SxIkTVbVqVe3YsSPDdd3r1Vdf1aVLlxQXF6dXXnnF5rWuXbtq+vTpioyMVOXKlRUcHKzIyMg0HzibonLlygoKCrL+uCXdSUK0atVKbdu2Va1atXTp0iWbXivpyZMnjzZt2qSSJUuqVatWqlChgt58803Fx8dbe67MnDlTly9fVvXq1fXGG2+od+/eNr06ckLp0qW1evVqbdiwQdWqVdOIESM0YcIE6zMIJOnKlSvWHxQlKTAwUCtXrtS+fftUu3Zt1atXT+fOndMPP/xgMxzc7NmzFRQUpLp16+rgwYPasGFDqodXz58/P9PPbQCQPSwWi1avXq369evrzTffVPny5dWuXTvFxMRY77Jv0KCBFi9erBUrVqhatWp64YUXUg1zmCJfvnyaNm2a6tata+3hsnLlyjSHELRYLFq+fLny58+v+vXrq1GjRipTpowWLlyYI/vq5uamXr16afTo0bp+/bpGjBihDz/8UKNGjVKFChUUEhKilStXWq8FBQsW1E8//aTr168rODhYNWvW1LRp06y9VnLiPD158mS1bt1aPXv2VGBgoLp166YbN25IutOj4uuvv9bq1atVuXJlzZ8/X8OGDcv0ul944QUVKFBAR44cUfv27W1eCwkJ0Xfffad169bp2Wef1fPPP6+IiIg0k2EpChYsaO39kqJFixbq27evevXqpWrVqmnbtm0aOnTofWPLzHE4btw45c+fX3Xq1FGzZs0UEhKSqvdTdsubN6/WrVuns2fPKigoSD179lS/fv3Ur18/a5m4uDgdOXLEZii2HTt26KWXXlLlypX1r3/9S1OnTlXv3r1t1p3y/J+goCCtWrVK3377barvK/Pnz1dYWFiaz7QBACAnWUx6T1h9Ql29elV58+bVlStX0h1OAgAAPJybN28qOjpapUuXTnescTx5Vq9erf79++vAgQPW54cgexw4cEAvvviijh49qrx586ZZJqPPHd+BkVUZHTOc44HM2b9/vxo1aqTjx49nahg2ZN7vv/+uwMBA7dq1K8ObPjhfAQAyKyttJlq7AAAAyBahoaF66623bIb4QPY4d+6cvvrqq3QTKgCAR0/lypU1evTo+z7bBFkXHR2tSZMmZZhQAQAgp/CgegAAAGSbPn362DuEJ1Ljxo3tHQIA4AF07NjR3iE8kZ577rlUw2QCAJBb6KkCAAAAAAAAAACQCSRVAAAAAAAAAAAAMoGkCgAAyDHGGHuHADgMPm/IbRxzAB51nKcAADmBpAoAAMh2rq6ukqS4uDg7RwI4jpTPW8rnD8gpnOMBPC64NgIAcgIPqgcAANnO2dlZ+fLl04ULFyRJefLkkcVisXNUwJPJGKO4uDhduHBB+fLlk7Ozs71DwhOOczyARx3XRgBATiKpAgAAckSxYsUkyfqjG4CclS9fPuvnDshpnOMBPA64NgIAcgJJFQAAkCMsFot8fX1VpEgR3b59297hAE80V1dX7sJFruIcD+BRx7URAJBTSKoAAIAc5ezsTIMWAJ5QnOMBAADgaHhQPQAAAAAAAAAAQCaQVAEAAAAAAAAAAMgEkioAAAAAAAAAAACZ4HDPVDHGSJKuXr1q50gAAACA3JHy3TfluzBwP7SbAAAA4Eiy0mZyuKTKtWvXJEn+/v52jgQAAADIXdeuXVPevHntHQYeA7SbAAAA4Igy02ayGAe7XS05OVnnzp2Tt7e3LBZLrm//6tWr8vf315kzZ+Tj45Pr24f9cQw4NurfsVH/jo36d2z2rn9jjK5du6bixYvLyYkRgHF/tJtgT9S/Y6P+HRv179iof8dm7/rPSpvJ4XqqODk5yc/Pz95hyMfHh5ODg+MYcGzUv2Oj/h0b9e/Y7Fn/9FBBVtBuwqOA+nds1L9jo/4dG/Xv2B6HNhO3qQEAAAAAAAAAAGQCSRUAAAAAAAAAAIBMIKmSy9zd3RUeHi53d3d7hwI74RhwbNS/Y6P+HRv179iofyBr+Mw4NurfsVH/jo36d2zUv2N7nOrf4R5UDwAAAAAAAAAA8CDoqQIAAAAAAAAAAJAJJFUAAAAAAAAAAAAygaQKAAAAAAAAAABAJpBUAQAAAAAAAAAAyASSKjlg0qRJKl26tDw8PFSzZk1t3rw5w/IbN25UzZo15eHhoTJlymjKlCm5FClyQlbq/5tvvtFLL72kwoULy8fHR7Vr19aaNWtyMVpkt6x+/lNs3bpVLi4uqlatWs4GiByX1WPg1q1bGjJkiAICAuTu7q6yZctq5syZuRQtsltW63/u3LmqWrWq8uTJI19fX3Xu3FmXLl3KpWiRXTZt2qRmzZqpePHislgsWr58+X2X4fsfQLvJ0dFucmy0mxwbbSbHRpvJcT1J7SaSKtls4cKFeu+99zRkyBBFRUWpXr16atq0qU6fPp1m+ejoaIWGhqpevXqKiorS4MGD1bt3by1dujSXI0d2yGr9b9q0SS+99JJWr16t3bt3q2HDhmrWrJmioqJyOXJkh6zWf4orV66oQ4cOevHFF3MpUuSUBzkG2rRpo/Xr12vGjBk6cuSI5s+fr8DAwFyMGtklq/W/ZcsWdejQQV26dNHBgwe1ePFi7dy5U127ds3lyPGwbty4oapVq+rLL7/MVHm+/wG0mxwd7SbHRrvJsdFmcmy0mRzbE9VuMshWzz33nOnRo4fNvMDAQDNw4MA0yw8YMMAEBgbazHvrrbfM888/n2MxIudktf7TUrFiRTN8+PDsDg254EHrv23btuaDDz4w4eHhpmrVqjkYIXJaVo+B77//3uTNm9dcunQpN8JDDstq/Y8ZM8aUKVPGZt6ECROMn59fjsWInCfJLFu2LMMyfP8DaDc5OtpNjo12k2OjzeTYaDMhxePebqKnSjZKSEjQ7t271bhxY5v5jRs31rZt29JcZvv27anKh4SEaNeuXbp9+3aOxYrs9yD1f6/k5GRdu3ZNBQoUyIkQkYMetP5nzZqlEydOKDw8PKdDRA57kGNgxYoVCgoK0ujRo1WiRAmVL19e/fv3V3x8fG6EjGz0IPVfp04dnT17VqtXr5YxRv/973+1ZMkSvfzyy7kRMuyI739wdLSbHBvtJsdGu8mx0WZybLSZkFWP8vc/F7tu/Qlz8eJFJSUlqWjRojbzixYtqtjY2DSXiY2NTbN8YmKiLl68KF9f3xyLF9nrQer/XmPHjtWNGzfUpk2bnAgROehB6v/YsWMaOHCgNm/eLBcXTsePuwc5Bk6ePKktW7bIw8NDy5Yt08WLF9WzZ0/98ccfjBH8mHmQ+q9Tp47mzp2rtm3b6ubNm0pMTFTz5s31xRdf5EbIsCO+/8HR0W5ybLSbHBvtJsdGm8mx0WZCVj3K3//oqZIDLBaLzbQxJtW8+5VPaz4eD1mt/xTz58/XsGHDtHDhQhUpUiSnwkMOy2z9JyUlqX379ho+fLjKly+fW+EhF2TlHJCcnCyLxaK5c+fqueeeU2hoqCIiIhQZGcmdV4+prNT/oUOH1Lt3b3344YfavXu3fvjhB0VHR6tHjx65ESrsjO9/AO0mR0e7ybHRbnJstJkcG20mZMWj+v2PFH82KlSokJydnVNlVy9cuJAqq5aiWLFiaZZ3cXFRwYIFcyxWZL8Hqf8UCxcuVJcuXbR48WI1atQoJ8NEDslq/V+7dk27du1SVFSUevXqJenOl0VjjFxcXLR27Vq98MILuRI7sseDnAN8fX1VokQJ5c2b1zqvQoUKMsbo7Nmzevrpp3M0ZmSfB6n/UaNGqW7dunr//fclSVWqVJGXl5fq1aunjz76iLuun2B8/4Ojo93k2Gg3OTbaTY6NNpNjo82ErHqUv//RUyUbubm5qWbNmlq3bp3N/HXr1qlOnTppLlO7du1U5deuXaugoCC5urrmWKzIfg9S/9KdO606deqkefPmMSbkYyyr9e/j46P9+/dr79691r8ePXromWee0d69e1WrVq3cCh3Z5EHOAXXr1tW5c+d0/fp167yjR4/KyclJfn5+ORovsteD1H9cXJycnGy/ijk7O0v63903eDLx/Q+OjnaTY6Pd5NhoNzk22kyOjTYTsuqR/v6XzQ++d3gLFiwwrq6uZsaMGebQoUPmvffeM15eXiYmJsYYY8zAgQPNG2+8YS1/8uRJkydPHtO3b19z6NAhM2PGDOPq6mqWLFlir13AQ8hq/c+bN8+4uLiYiRMnmvPnz1v//vzzT3vtAh5CVuv/XuHh4aZq1aq5FC1yQlaPgWvXrhk/Pz/TunVrc/DgQbNx40bz9NNPm65du9prF/AQslr/s2bNMi4uLmbSpEnmxIkTZsuWLSYoKMg899xz9toFPKBr166ZqKgoExUVZSSZiIgIExUVZU6dOmWM4fsfkBbaTY6NdpNjo93k2GgzOTbaTI7tSWo3kVTJARMnTjQBAQHGzc3N1KhRw2zcuNH6WseOHU1wcLBN+Q0bNpjq1asbNzc3U6pUKTN58uRcjhjZKSv1HxwcbCSl+uvYsWPuB45skdXP/91oHDwZsnoMHD582DRq1Mh4enoaPz8/069fPxMXF5fLUSO7ZLX+J0yYYCpWrGg8PT2Nr6+vCQsLM2fPns3lqPGwfv755wyv53z/A9JGu8mx0W5ybLSbHBttJsdGm8lxPUntJosx9JUCAAAAAAAAAAC4H56pAgAAAAAAAAAAkAkkVQAAAAAAAAAAADKBpAoAAAAAAAAAAEAmkFQBAAAAAAAAAADIBJIqAAAAAAAAAAAAmUBSBQAAAAAAAAAAIBNIqgAAAAAAAAAAAGQCSRUAAAAAAAAAAIBMIKkCAI+RyMhI5cuXz95hPLBSpUpp/PjxGZYZNmyYqlWrlivxAAAAAMCT4t72lsVi0fLly+0WDwA8qUiqAEAu69SpkywWS6q/48eP2zs0RUZG2sTk6+urNm3aKDo6OlvWv3PnTnXv3t06ndaX/P79+2v9+vXZsr303LufRYsWVbNmzXTw4MEsr+dxTnIBAAAAyB53t/NcXFxUsmRJvf3227p8+bK9QwMAZDOSKgBgB02aNNH58+dt/kqXLm3vsCRJPj4+On/+vM6dO6d58+Zp7969at68uZKSkh563YULF1aePHkyLPPUU0+pYMGCD72t+7l7P1etWqUbN27o5ZdfVkJCQo5vGwAAAMCTJ6WdFxMTo+nTp2vlypXq2bOnvcMCAGQzkioAYAfu7u4qVqyYzZ+zs7MiIiJUuXJleXl5yd/fXz179tT169fTXc+vv/6qhg0bytvbWz4+PqpZs6Z27dplfX3btm2qX7++PD095e/vr969e+vGjRsZxmaxWFSsWDH5+vqqYcOGCg8P14EDB6w9aSZPnqyyZcvKzc1NzzzzjObMmWOz/LBhw1SyZEm5u7urePHi6t27t/W1u7ujlypVSpLUsmVLWSwW6/Tdw3+tWbNGHh4e+vPPP2220bt3bwUHB2fbfgYFBalv3746deqUjhw5Yi2TUX1s2LBBnTt31pUrV6x3pA0bNkySlJCQoAEDBqhEiRLy8vJSrVq1tGHDhgzjAQAAAPB4S2nn+fn5qXHjxmrbtq3Wrl1rfX3WrFmqUKGCPDw8FBgYqEmTJtksf/bsWbVr104FChSQl5eXgoKC9Msvv0iSTpw4oRYtWqho0aJ66qmn9Oyzz+rHH3/M1f0DANxBUgUAHiFOTk6aMGGCDhw4oNmzZ+unn37SgAED0i0fFhYmPz8/7dy5U7t379bAgQPl6uoqSdq/f79CQkLUqlUr7du3TwsXLtSWLVvUq1evLMXk6ekpSbp9+7aWLVumPn366O9//7sOHDigt956S507d9bPP/8sSVqyZInGjRunqVOn6tixY1q+fLkqV66c5np37twp6U7D4vz589bpuzVq1Ej58uXT0qVLrfOSkpK0aNEihYWFZdt+/vnnn5o3b54kWd8/KeP6qFOnjsaPH2/t8XL+/Hn1799fktS5c2dt3bpVCxYs0L59+/Tqq6+qSZMmOnbsWKZjAgAAAPD4OnnypH744Qdr+2LatGkaMmSIPv74Yx0+fFgjR47U0KFDNXv2bEnS9evXFRwcrHPnzmnFihX69ddfNWDAACUnJ1tfDw0N1Y8//qioqCiFhISoWbNmOn36tN32EQAclgEA5KqOHTsaZ2dn4+XlZf1r3bp1mmUXLVpkChYsaJ2eNWuWyZs3r3Xa29vbREZGprnsG2+8Ybp3724zb/PmzcbJycnEx8enucy96z9z5ox5/vnnjZ+fn7l165apU6eO6datm80yr776qgkNDTXGGDN27FhTvnx5k5CQkOb6AwICzLhx46zTksyyZctsyoSHh5uqVatap3v37m1eeOEF6/SaNWuMm5ub+eOPPx5qPyUZLy8vkydPHiPJSDLNmzdPs3yK+9WHMcYcP37cWCwW89tvv9nMf/HFF82gQYMyXD8AAACAx9Pd7TwPDw9rGyMiIsIYY4y/v7+ZN2+ezTIjRowwtWvXNsYYM3XqVOPt7W0uXbqU6W1WrFjRfPHFF9bpzLS3AAAPz8WO+RwAcFgNGzbU5MmTrdNeXl6SpJ9//lkjR47UoUOHdPXqVSUmJurmzZu6ceOGtczd+vXrp65du2rOnDlq1KiRXn31VZUtW1aStHv3bh0/flxz5861ljfGKDk5WdHR0apQoUKasV25ckVPPfWUjDGKi4tTjRo19M0338jNzU2HDx+2edC8JNWtW1eff/65JOnVV1/V+PHjVaZMGTVp0kShoaFq1qyZXFwe/HITFham2rVr69y5cypevLjmzp2r0NBQ5c+f/6H209vbW3v27FFiYqI2btyoMWPGaMqUKTZlslofkrRnzx4ZY1S+fHmb+bdu3cqVZ8UAAAAAsI+Udl5cXJymT5+uo0eP6t1339Xvv/+uM2fOqEuXLurWrZu1fGJiovLmzStJ2rt3r6pXr64CBQqkue4bN25o+PDh+u6773Tu3DklJiYqPj6enioAYAckVQDADry8vFSuXDmbeadOnVJoaKh69OihESNGqECBAtqyZYu6dOmi27dvp7meYcOGqX379lq1apW+//57hYeHa8GCBWrZsqWSk5P11ltv2TzTJEXJkiXTjS0l2eDk5KSiRYumSh5YLBabaWOMdZ6/v7+OHDmidevW6ccff1TPnj01ZswYbdy40WZYrax47rnnVLZsWS1YsEBvv/22li1bplmzZllff9D9dHJystZBYGCgYmNj1bZtW23atEnSg9VHSjzOzs7avXu3nJ2dbV576qmnsrTvAAAAAB4fd7fzJkyYoIYNG2r48OHWoYmnTZumWrVq2SyT0mZIGXY5Pe+//77WrFmjzz77TOXKlZOnp6dat26thISEHNgTAEBGSKoAwCNi165dSkxM1NixY+XkdOeRV4sWLbrvcuXLl1f58uXVt29fvfbaa5o1a5ZatmypGjVq6ODBg6mSN/dzd7LhXhUqVNCWLVvUoUMH67xt27bZ9Abx9PRU8+bN1bx5c73zzjsKDAzU/v37VaNGjVTrc3V1VVJS0n1jat++vebOnSs/Pz85OTnp5Zdftr72oPt5r759+yoiIkLLli1Ty5YtM1Ufbm5uqeKvXr26kpKSdOHCBdWrV++hYgIAAADw+AoPD1fTpk319ttvq0SJEjp58qT12ZD3qlKliqZPn64//vgjzd4qmzdvVqdOndSyZUtJd56xEhMTk5PhAwDSwYPqAeARUbZsWSUmJuqLL77QyZMnNWfOnFTDUd0tPj5evXr10oYNG3Tq1Clt3bpVO3futCY4/vGPf2j79u165513tHfvXh07dkwrVqzQu++++8Axvv/++4qMjNSUKVN07NgxRURE6JtvvrE+oD0yMlIzZszQgQMHrPvg6empgICANNdXqlQprV+/XrGxsbp8+XK62w0LC9OePXv08ccfq3Xr1vLw8LC+ll376ePjo65duyo8PFzGmEzVR6lSpXT9+nWtX79eFy9eVFxcnMqXL6+wsDB16NBB33zzjaKjo7Vz5059+umnWr16dZZiAgAAAPD4atCggSpVqqSRI0dq2LBhGjVqlD7//HMdPXpU+/fv16xZsxQRESFJeu2111SsWDG98sor2rp1q06ePKmlS5dq+/btkqRy5crpm2++0d69e/Xrr7+qffv21ofYAwByF0kVAHhEVKtWTREREfr000/1l7/8RXPnztWoUaPSLe/s7KxLly6pQ4cOKl++vNq0aaOmTZtq+PDhku7c6bRx40YdO3ZM9erVU/Xq1TV06FD5+vo+cIyvvPKKPv/8c40ZM0aVKlXS1KlTNWvWLDVo0ECSlC9fPk2bNk1169ZVlSpVtH79eq1cuTLdZ4mMHTtW69atk7+/v6pXr57udp9++mk9++yz2rdvX6o7u7JzP/v06aPDhw9r8eLFmaqPOnXqqEePHmrbtq0KFy6s0aNHS5JmzZqlDh066O9//7ueeeYZNW/eXL/88ov8/f2zHBMAAACAx1e/fv00bdo0hYSEaPr06YqMjFTlypUVHBysyMhIlS5dWtKdXvBr165VkSJFFBoaqsqVK+uTTz6xDg82btw45c+fX3Xq1FGzZs0UEhKS5mgAAICcZzHGGHsHAQAAAAAAAAAA8KijpwoAAAAAAAAAAEAmkFQBAAAAAAAAAADIBJIqAAAAAAAAAAAAmUBSBQAAAAAAAAAAIBNIqgAAAAAAAAAAAGQCSRUAAAAAAAAAAIBMIKkCAAAAAAAAAACQCSRVAAAAAAAAAAAAMoGkCgAAAAAAAAAAQCaQVAEAAAAAAAAAAMgEkioAAAAAAAAAAACZ8P8A3uPgPh7n1pEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 2000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Classifcation Report - LightGBM\n",
      "**************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.60      0.58       515\n",
      "           1       0.96      0.95      0.96      5426\n",
      "\n",
      "    accuracy                           0.92      5941\n",
      "   macro avg       0.76      0.78      0.77      5941\n",
      "weighted avg       0.93      0.92      0.92      5941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_lgbm = lgb.LGBMClassifier(objective='binary',class_weight= {0: 5.0, 1: 1.0}, n_estimators= 460)\n",
    "\n",
    "best_lgbm.fit(X_tr_vec, y_tr)\n",
    "y_pr = best_lgbm.predict(X_te_vec)\n",
    "y_prob = best_lgbm.predict_proba(X_te_vec)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_te, y_prob[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "precision, recall, pthresholds = precision_recall_curve(y_te, y_prob[:, 1])\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "# Plot ROC curve\n",
    "\n",
    "ax[0].plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "ax[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax[0].set_xlabel('False Positive Rate')\n",
    "ax[0].set_ylabel('True Positive Rate')\n",
    "ax[0].set_title('Receiver Operating Characteristic (ROC) Curve - LightGBM')\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "ax[1].plot(recall, precision, color='b', alpha=0.8, label='Precision-Recall curve (area = %0.2f)' % pr_auc)\n",
    "ax[1].set_xlabel('Recall')\n",
    "ax[1].set_ylabel('Precision')\n",
    "ax[1].set_title('Precision-Recall Curve - LightGBM')\n",
    "ax[1].legend(loc='lower left')\n",
    "\n",
    "plt.show();\n",
    "# Classification report\n",
    "\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_te, y_pr), cmap='Blues', cbar=True, annot=True, xticklabels= True, yticklabels=True, fmt = 'd')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - LightGBM')\n",
    "\n",
    "plt.show();\n",
    "\n",
    "print('*'*50)\n",
    "print('Classifcation Report - LightGBM')\n",
    "print('*'*50)\n",
    "print(classification_report(y_te, y_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ead33655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Classifcation Report - LinearSVC\n",
      "**************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.41      0.54       515\n",
      "           1       0.95      0.99      0.97      5426\n",
      "\n",
      "    accuracy                           0.94      5941\n",
      "   macro avg       0.86      0.70      0.75      5941\n",
      "weighted avg       0.93      0.94      0.93      5941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_svc = LinearSVC(dual='auto', C= 1, class_weight= {0: 1.0, 1: 1.0}, max_iter= 1000)\n",
    "\n",
    "clf = CalibratedClassifierCV(best_svc) \n",
    "clf.fit(X_tr_vec, y_tr)\n",
    "y_pr = clf.predict(X_te_vec)\n",
    "y_prob = clf.predict_proba(X_te_vec)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_te, y_prob[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "precision, recall, pthresholds = precision_recall_curve(y_te, y_prob[:, 1])\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "# Plot ROC curve\n",
    "\n",
    "ax[0].plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "ax[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax[0].set_xlabel('False Positive Rate')\n",
    "ax[0].set_ylabel('True Positive Rate')\n",
    "ax[0].set_title('Receiver Operating Characteristic (ROC) Curve - LinearSVC')\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "ax[1].plot(recall, precision, color='b', alpha=0.8, label='Precision-Recall curve (area = %0.2f)' % pr_auc)\n",
    "ax[1].set_xlabel('Recall')\n",
    "ax[1].set_ylabel('Precision')\n",
    "ax[1].set_title('Precision-Recall Curve - LinearSVC')\n",
    "ax[1].legend(loc='lower left')\n",
    "\n",
    "plt.show();\n",
    "# Classification report\n",
    "\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_te, y_pr), cmap='Blues', cbar=True, annot=True, xticklabels= True, yticklabels=True, fmt = 'd')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - LinearSVC')\n",
    "\n",
    "plt.show();\n",
    "\n",
    "print('*'*50)\n",
    "print('Classifcation Report - LinearSVC')\n",
    "print('*'*50)\n",
    "print(classification_report(y_te, y_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e2fe8c",
   "metadata": {},
   "source": [
    "### Well this is a tight race. Looks like Light Gradient Boosting Machine and Linear Support Vector Machine are both going for gold on this one. It's a tough day for Random Forest, but he'll be alright with a bronze trophy. \n",
    "\n",
    "### A quick note on what we're looking for here:\n",
    "\n",
    "#### The goal of this model is to identify the most likely context surrounding a successful terrorist incident given a certain motive. In this case, I don't want to miss True incidents, and it's alright if I call a few cases true when they are not. I'd much rather be overprepared than underprepared in this situation. Thus, I'm looking to optimize recall on the positive (1) class. \n",
    "\n",
    "#### However, I can't optimize recall on the positive class too much, as then the model will not be useful - it will tell us we need to prepare more for events that are less likely to be successful than for events that are truly likely to be successful. That's not good either. So, I'm also accounting for the weighted avg of recall. \n",
    "\n",
    "#### Lastly, of course I want to identify true positives and true negatives on the positive class correctly.\n",
    "\n",
    "### As it is, I think it's too close to tell between LightGBM and LinearSVC as they are not fully optimized yet. The next step is to perform extensive CV on both and evaluate the metrics again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebaf9b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 1/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=918; f1: (test=0.959) precision: (test=0.952) recall: (test=0.966) total time=  12.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 2/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=918; f1: (test=0.957) precision: (test=0.954) recall: (test=0.959) total time=  13.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.821539 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 3/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=918; f1: (test=0.958) precision: (test=0.958) recall: (test=0.957) total time= 1.7min\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678175 -> initscore=0.745399\n",
      "[LightGBM] [Info] Start training from score 0.745399\n",
      "[CV 4/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=918; f1: (test=0.959) precision: (test=0.952) recall: (test=0.966) total time=  11.6s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117794 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678369 -> initscore=0.746285\n",
      "[LightGBM] [Info] Start training from score 0.746285\n",
      "[CV 5/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=918; f1: (test=0.960) precision: (test=0.957) recall: (test=0.963) total time=  12.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067232 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=778; f1: (test=0.967) precision: (test=0.948) recall: (test=0.986) total time=  10.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=778; f1: (test=0.963) precision: (test=0.947) recall: (test=0.981) total time=  10.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=778; f1: (test=0.969) precision: (test=0.951) recall: (test=0.987) total time=  10.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=778; f1: (test=0.965) precision: (test=0.946) recall: (test=0.985) total time=  11.3s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=778; f1: (test=0.966) precision: (test=0.950) recall: (test=0.983) total time=  11.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=715; f1: (test=0.967) precision: (test=0.949) recall: (test=0.987) total time=   9.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=715; f1: (test=0.964) precision: (test=0.947) recall: (test=0.982) total time=  10.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=715; f1: (test=0.969) precision: (test=0.951) recall: (test=0.988) total time=  10.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=715; f1: (test=0.965) precision: (test=0.946) recall: (test=0.985) total time=  10.7s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=715; f1: (test=0.967) precision: (test=0.950) recall: (test=0.983) total time=  10.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=334; f1: (test=0.967) precision: (test=0.947) recall: (test=0.988) total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=334; f1: (test=0.966) precision: (test=0.947) recall: (test=0.985) total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=334; f1: (test=0.969) precision: (test=0.951) recall: (test=0.988) total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=334; f1: (test=0.966) precision: (test=0.946) recall: (test=0.986) total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102110 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=334; f1: (test=0.967) precision: (test=0.948) recall: (test=0.987) total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 1/5] END class_weight=balanced, n_estimators=212; f1: (test=0.938) precision: (test=0.967) recall: (test=0.911) total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/5] END class_weight=balanced, n_estimators=212; f1: (test=0.935) precision: (test=0.967) recall: (test=0.905) total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/5] END class_weight=balanced, n_estimators=212; f1: (test=0.935) precision: (test=0.973) recall: (test=0.899) total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 4/5] END class_weight=balanced, n_estimators=212; f1: (test=0.941) precision: (test=0.970) recall: (test=0.914) total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 5/5] END class_weight=balanced, n_estimators=212; f1: (test=0.937) precision: (test=0.974) recall: (test=0.902) total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 1/5] END class_weight=balanced, n_estimators=997; f1: (test=0.952) precision: (test=0.954) recall: (test=0.949) total time=  13.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/5] END class_weight=balanced, n_estimators=997; f1: (test=0.950) precision: (test=0.954) recall: (test=0.946) total time=  12.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/5] END class_weight=balanced, n_estimators=997; f1: (test=0.951) precision: (test=0.958) recall: (test=0.943) total time=  12.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075696 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 4/5] END class_weight=balanced, n_estimators=997; f1: (test=0.953) precision: (test=0.954) recall: (test=0.952) total time=  11.8s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080667 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 5/5] END class_weight=balanced, n_estimators=997; f1: (test=0.954) precision: (test=0.961) recall: (test=0.947) total time=  13.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058592 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 1/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=74; f1: (test=0.939) precision: (test=0.970) recall: (test=0.911) total time=   2.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 2/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=74; f1: (test=0.938) precision: (test=0.966) recall: (test=0.911) total time=   2.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 3/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=74; f1: (test=0.938) precision: (test=0.975) recall: (test=0.904) total time=   2.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678175 -> initscore=0.745399\n",
      "[LightGBM] [Info] Start training from score 0.745399\n",
      "[CV 4/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=74; f1: (test=0.946) precision: (test=0.971) recall: (test=0.922) total time=   2.1s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678369 -> initscore=0.746285\n",
      "[LightGBM] [Info] Start training from score 0.746285\n",
      "[CV 5/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=74; f1: (test=0.941) precision: (test=0.975) recall: (test=0.909) total time=   2.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099022 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 1/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=346; f1: (test=0.952) precision: (test=0.958) recall: (test=0.946) total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 2/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=346; f1: (test=0.951) precision: (test=0.959) recall: (test=0.943) total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=346; f1: (test=0.955) precision: (test=0.966) recall: (test=0.944) total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678175 -> initscore=0.745399\n",
      "[LightGBM] [Info] Start training from score 0.745399\n",
      "[CV 4/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=346; f1: (test=0.956) precision: (test=0.961) recall: (test=0.951) total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678369 -> initscore=0.746285\n",
      "[LightGBM] [Info] Start training from score 0.746285\n",
      "[CV 5/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=346; f1: (test=0.956) precision: (test=0.965) recall: (test=0.947) total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=506; f1: (test=0.967) precision: (test=0.948) recall: (test=0.987) total time=   7.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.106383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=506; f1: (test=0.965) precision: (test=0.948) recall: (test=0.982) total time=   8.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=506; f1: (test=0.969) precision: (test=0.951) recall: (test=0.986) total time=   8.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=506; f1: (test=0.966) precision: (test=0.946) recall: (test=0.986) total time=   7.9s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090435 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=506; f1: (test=0.966) precision: (test=0.949) recall: (test=0.984) total time=   8.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063288 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=495; f1: (test=0.967) precision: (test=0.948) recall: (test=0.987) total time=   7.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=495; f1: (test=0.965) precision: (test=0.948) recall: (test=0.982) total time=   7.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=495; f1: (test=0.969) precision: (test=0.951) recall: (test=0.986) total time=   6.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=495; f1: (test=0.966) precision: (test=0.946) recall: (test=0.986) total time=   7.3s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=495; f1: (test=0.966) precision: (test=0.949) recall: (test=0.984) total time=   7.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077497 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END class_weight=balanced, n_estimators=472; f1: (test=0.946) precision: (test=0.959) recall: (test=0.933) total time=   7.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/5] END class_weight=balanced, n_estimators=472; f1: (test=0.947) precision: (test=0.961) recall: (test=0.934) total time=   7.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085125 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/5] END class_weight=balanced, n_estimators=472; f1: (test=0.947) precision: (test=0.966) recall: (test=0.930) total time=   7.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 4/5] END class_weight=balanced, n_estimators=472; f1: (test=0.949) precision: (test=0.962) recall: (test=0.936) total time=   7.5s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 5/5] END class_weight=balanced, n_estimators=472; f1: (test=0.950) precision: (test=0.967) recall: (test=0.933) total time=   7.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=200; f1: (test=0.967) precision: (test=0.945) recall: (test=0.990) total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=200; f1: (test=0.966) precision: (test=0.947) recall: (test=0.986) total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=200; f1: (test=0.970) precision: (test=0.951) recall: (test=0.990) total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=200; f1: (test=0.965) precision: (test=0.943) recall: (test=0.989) total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=200; f1: (test=0.967) precision: (test=0.947) recall: (test=0.989) total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 1/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=278; f1: (test=0.942) precision: (test=0.964) recall: (test=0.922) total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096402 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 2/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=278; f1: (test=0.938) precision: (test=0.964) recall: (test=0.913) total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 3/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=278; f1: (test=0.943) precision: (test=0.971) recall: (test=0.917) total time=   4.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513060 -> initscore=0.052252\n",
      "[LightGBM] [Info] Start training from score 0.052252\n",
      "[CV 4/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=278; f1: (test=0.945) precision: (test=0.967) recall: (test=0.923) total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096176 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513281 -> initscore=0.053138\n",
      "[LightGBM] [Info] Start training from score 0.053138\n",
      "[CV 5/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=278; f1: (test=0.942) precision: (test=0.972) recall: (test=0.914) total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=485; f1: (test=0.967) precision: (test=0.948) recall: (test=0.987) total time=   8.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057820 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=485; f1: (test=0.965) precision: (test=0.948) recall: (test=0.983) total time=   7.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=485; f1: (test=0.968) precision: (test=0.951) recall: (test=0.986) total time=   7.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=485; f1: (test=0.966) precision: (test=0.946) recall: (test=0.986) total time=   7.7s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=485; f1: (test=0.966) precision: (test=0.949) recall: (test=0.984) total time=   7.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=473; f1: (test=0.967) precision: (test=0.948) recall: (test=0.987) total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=473; f1: (test=0.965) precision: (test=0.948) recall: (test=0.983) total time=   7.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=473; f1: (test=0.968) precision: (test=0.951) recall: (test=0.986) total time=   7.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=473; f1: (test=0.966) precision: (test=0.946) recall: (test=0.986) total time=   7.3s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=473; f1: (test=0.966) precision: (test=0.949) recall: (test=0.984) total time=   7.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 1/5] END class_weight=balanced, n_estimators=765; f1: (test=0.950) precision: (test=0.956) recall: (test=0.944) total time=   9.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102445 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/5] END class_weight=balanced, n_estimators=765; f1: (test=0.949) precision: (test=0.957) recall: (test=0.941) total time=   9.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/5] END class_weight=balanced, n_estimators=765; f1: (test=0.950) precision: (test=0.960) recall: (test=0.940) total time=  10.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 4/5] END class_weight=balanced, n_estimators=765; f1: (test=0.953) precision: (test=0.957) recall: (test=0.949) total time=   8.6s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071675 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 5/5] END class_weight=balanced, n_estimators=765; f1: (test=0.953) precision: (test=0.961) recall: (test=0.945) total time=   9.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=321; f1: (test=0.967) precision: (test=0.947) recall: (test=0.988) total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=321; f1: (test=0.966) precision: (test=0.948) recall: (test=0.984) total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=321; f1: (test=0.969) precision: (test=0.951) recall: (test=0.988) total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=321; f1: (test=0.966) precision: (test=0.946) recall: (test=0.986) total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=321; f1: (test=0.967) precision: (test=0.948) recall: (test=0.987) total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=167; f1: (test=0.967) precision: (test=0.944) recall: (test=0.990) total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=167; f1: (test=0.966) precision: (test=0.946) recall: (test=0.987) total time=   3.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=167; f1: (test=0.970) precision: (test=0.950) recall: (test=0.990) total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=167; f1: (test=0.965) precision: (test=0.942) recall: (test=0.989) total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=167; f1: (test=0.967) precision: (test=0.946) recall: (test=0.990) total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 1/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=75; f1: (test=0.917) precision: (test=0.978) recall: (test=0.864) total time=   2.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 2/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=75; f1: (test=0.915) precision: (test=0.974) recall: (test=0.862) total time=   2.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 3/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=75; f1: (test=0.915) precision: (test=0.980) recall: (test=0.857) total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513060 -> initscore=0.052252\n",
      "[LightGBM] [Info] Start training from score 0.052252\n",
      "[CV 4/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=75; f1: (test=0.924) precision: (test=0.977) recall: (test=0.876) total time=   2.9s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513281 -> initscore=0.053138\n",
      "[LightGBM] [Info] Start training from score 0.053138\n",
      "[CV 5/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=75; f1: (test=0.919) precision: (test=0.979) recall: (test=0.865) total time=   2.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 1/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=436; f1: (test=0.954) precision: (test=0.956) recall: (test=0.952) total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 2/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=436; f1: (test=0.951) precision: (test=0.959) recall: (test=0.944) total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 3/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=436; f1: (test=0.956) precision: (test=0.963) recall: (test=0.948) total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678175 -> initscore=0.745399\n",
      "[LightGBM] [Info] Start training from score 0.745399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=436; f1: (test=0.957) precision: (test=0.959) recall: (test=0.955) total time=   7.0s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678369 -> initscore=0.746285\n",
      "[LightGBM] [Info] Start training from score 0.746285\n",
      "[CV 5/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=436; f1: (test=0.957) precision: (test=0.963) recall: (test=0.950) total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=124; f1: (test=0.965) precision: (test=0.942) recall: (test=0.989) total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079964 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=124; f1: (test=0.966) precision: (test=0.944) recall: (test=0.989) total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=124; f1: (test=0.969) precision: (test=0.949) recall: (test=0.990) total time=   2.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068440 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=124; f1: (test=0.966) precision: (test=0.942) recall: (test=0.991) total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=124; f1: (test=0.968) precision: (test=0.946) recall: (test=0.991) total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 1/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=224; f1: (test=0.939) precision: (test=0.967) recall: (test=0.913) total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 2/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=224; f1: (test=0.934) precision: (test=0.965) recall: (test=0.904) total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 3/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=224; f1: (test=0.938) precision: (test=0.974) recall: (test=0.906) total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513060 -> initscore=0.052252\n",
      "[LightGBM] [Info] Start training from score 0.052252\n",
      "[CV 4/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=224; f1: (test=0.943) precision: (test=0.969) recall: (test=0.918) total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513281 -> initscore=0.053138\n",
      "[LightGBM] [Info] Start training from score 0.053138\n",
      "[CV 5/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=224; f1: (test=0.939) precision: (test=0.974) recall: (test=0.906) total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=59; f1: (test=0.913) precision: (test=0.980) recall: (test=0.855) total time=   2.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 2/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=59; f1: (test=0.912) precision: (test=0.975) recall: (test=0.856) total time=   2.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 3/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=59; f1: (test=0.911) precision: (test=0.980) recall: (test=0.851) total time=   2.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513060 -> initscore=0.052252\n",
      "[LightGBM] [Info] Start training from score 0.052252\n",
      "[CV 4/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=59; f1: (test=0.922) precision: (test=0.980) recall: (test=0.869) total time=   2.2s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513281 -> initscore=0.053138\n",
      "[LightGBM] [Info] Start training from score 0.053138\n",
      "[CV 5/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=59; f1: (test=0.916) precision: (test=0.980) recall: (test=0.860) total time=   2.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057170 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 1/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=150; f1: (test=0.932) precision: (test=0.971) recall: (test=0.896) total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 2/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=150; f1: (test=0.927) precision: (test=0.968) recall: (test=0.890) total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 3/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=150; f1: (test=0.932) precision: (test=0.977) recall: (test=0.891) total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513060 -> initscore=0.052252\n",
      "[LightGBM] [Info] Start training from score 0.052252\n",
      "[CV 4/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=150; f1: (test=0.936) precision: (test=0.972) recall: (test=0.903) total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513281 -> initscore=0.053138\n",
      "[LightGBM] [Info] Start training from score 0.053138\n",
      "[CV 5/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=150; f1: (test=0.931) precision: (test=0.976) recall: (test=0.890) total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 1/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=609; f1: (test=0.948) precision: (test=0.956) recall: (test=0.940) total time=   9.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 2/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=609; f1: (test=0.948) precision: (test=0.958) recall: (test=0.938) total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=609; f1: (test=0.949) precision: (test=0.961) recall: (test=0.937) total time=   7.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.100647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513060 -> initscore=0.052252\n",
      "[LightGBM] [Info] Start training from score 0.052252\n",
      "[CV 4/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=609; f1: (test=0.952) precision: (test=0.959) recall: (test=0.945) total time=   8.0s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513281 -> initscore=0.053138\n",
      "[LightGBM] [Info] Start training from score 0.053138\n",
      "[CV 5/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=609; f1: (test=0.951) precision: (test=0.963) recall: (test=0.939) total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 1/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=988; f1: (test=0.959) precision: (test=0.952) recall: (test=0.967) total time=   9.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 2/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=988; f1: (test=0.957) precision: (test=0.953) recall: (test=0.961) total time=  10.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 3/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=988; f1: (test=0.957) precision: (test=0.958) recall: (test=0.956) total time=   9.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678175 -> initscore=0.745399\n",
      "[LightGBM] [Info] Start training from score 0.745399\n",
      "[CV 4/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=988; f1: (test=0.959) precision: (test=0.952) recall: (test=0.967) total time=  11.0s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678369 -> initscore=0.746285\n",
      "[LightGBM] [Info] Start training from score 0.746285\n",
      "[CV 5/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=988; f1: (test=0.959) precision: (test=0.956) recall: (test=0.963) total time=  10.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 1/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=584; f1: (test=0.949) precision: (test=0.956) recall: (test=0.941) total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 2/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=584; f1: (test=0.947) precision: (test=0.957) recall: (test=0.936) total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 3/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=584; f1: (test=0.950) precision: (test=0.962) recall: (test=0.938) total time=   7.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513060 -> initscore=0.052252\n",
      "[LightGBM] [Info] Start training from score 0.052252\n",
      "[CV 4/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=584; f1: (test=0.952) precision: (test=0.959) recall: (test=0.944) total time=   7.0s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513281 -> initscore=0.053138\n",
      "[LightGBM] [Info] Start training from score 0.053138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=584; f1: (test=0.950) precision: (test=0.963) recall: (test=0.938) total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 1/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=369; f1: (test=0.952) precision: (test=0.957) recall: (test=0.947) total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 2/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=369; f1: (test=0.951) precision: (test=0.960) recall: (test=0.943) total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 3/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=369; f1: (test=0.955) precision: (test=0.965) recall: (test=0.944) total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075667 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678175 -> initscore=0.745399\n",
      "[LightGBM] [Info] Start training from score 0.745399\n",
      "[CV 4/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=369; f1: (test=0.956) precision: (test=0.960) recall: (test=0.952) total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678369 -> initscore=0.746285\n",
      "[LightGBM] [Info] Start training from score 0.746285\n",
      "[CV 5/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=369; f1: (test=0.957) precision: (test=0.965) recall: (test=0.948) total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 1/5] END class_weight=balanced, n_estimators=725; f1: (test=0.949) precision: (test=0.956) recall: (test=0.943) total time=   7.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075677 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/5] END class_weight=balanced, n_estimators=725; f1: (test=0.949) precision: (test=0.958) recall: (test=0.939) total time=   7.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/5] END class_weight=balanced, n_estimators=725; f1: (test=0.949) precision: (test=0.960) recall: (test=0.939) total time=   8.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 4/5] END class_weight=balanced, n_estimators=725; f1: (test=0.953) precision: (test=0.958) recall: (test=0.948) total time=   9.4s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 5/5] END class_weight=balanced, n_estimators=725; f1: (test=0.953) precision: (test=0.961) recall: (test=0.944) total time=   9.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=250; f1: (test=0.967) precision: (test=0.946) recall: (test=0.989) total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=250; f1: (test=0.965) precision: (test=0.947) recall: (test=0.984) total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=250; f1: (test=0.969) precision: (test=0.951) recall: (test=0.988) total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=250; f1: (test=0.966) precision: (test=0.945) recall: (test=0.989) total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=250; f1: (test=0.967) precision: (test=0.947) recall: (test=0.989) total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=778; f1: (test=0.967) precision: (test=0.948) recall: (test=0.986) total time=   8.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=778; f1: (test=0.963) precision: (test=0.947) recall: (test=0.981) total time=   8.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=778; f1: (test=0.969) precision: (test=0.951) recall: (test=0.987) total time=   8.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=778; f1: (test=0.965) precision: (test=0.946) recall: (test=0.985) total time=   8.3s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=778; f1: (test=0.966) precision: (test=0.950) recall: (test=0.983) total time=   8.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 1/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=644; f1: (test=0.958) precision: (test=0.955) recall: (test=0.962) total time=   7.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 2/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=644; f1: (test=0.955) precision: (test=0.955) recall: (test=0.954) total time=   7.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 3/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=644; f1: (test=0.956) precision: (test=0.959) recall: (test=0.954) total time=   6.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678175 -> initscore=0.745399\n",
      "[LightGBM] [Info] Start training from score 0.745399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=644; f1: (test=0.959) precision: (test=0.954) recall: (test=0.964) total time=   7.3s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678369 -> initscore=0.746285\n",
      "[LightGBM] [Info] Start training from score 0.746285\n",
      "[CV 5/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=644; f1: (test=0.959) precision: (test=0.960) recall: (test=0.959) total time=   7.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 1/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=448; f1: (test=0.955) precision: (test=0.956) recall: (test=0.953) total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 2/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=448; f1: (test=0.952) precision: (test=0.959) recall: (test=0.945) total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 3/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=448; f1: (test=0.955) precision: (test=0.962) recall: (test=0.948) total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678175 -> initscore=0.745399\n",
      "[LightGBM] [Info] Start training from score 0.745399\n",
      "[CV 4/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=448; f1: (test=0.957) precision: (test=0.959) recall: (test=0.955) total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678369 -> initscore=0.746285\n",
      "[LightGBM] [Info] Start training from score 0.746285\n",
      "[CV 5/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=448; f1: (test=0.957) precision: (test=0.963) recall: (test=0.951) total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 1/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=422; f1: (test=0.947) precision: (test=0.958) recall: (test=0.935) total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 2/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=422; f1: (test=0.946) precision: (test=0.961) recall: (test=0.931) total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 3/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=422; f1: (test=0.949) precision: (test=0.967) recall: (test=0.932) total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513060 -> initscore=0.052252\n",
      "[LightGBM] [Info] Start training from score 0.052252\n",
      "[CV 4/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=422; f1: (test=0.949) precision: (test=0.964) recall: (test=0.935) total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513281 -> initscore=0.053138\n",
      "[LightGBM] [Info] Start training from score 0.053138\n",
      "[CV 5/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=422; f1: (test=0.946) precision: (test=0.967) recall: (test=0.927) total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=746; f1: (test=0.958) precision: (test=0.953) recall: (test=0.963) total time=   9.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 2/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=746; f1: (test=0.955) precision: (test=0.953) recall: (test=0.957) total time=   9.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 3/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=746; f1: (test=0.955) precision: (test=0.959) recall: (test=0.952) total time=   8.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678175 -> initscore=0.745399\n",
      "[LightGBM] [Info] Start training from score 0.745399\n",
      "[CV 4/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=746; f1: (test=0.960) precision: (test=0.954) recall: (test=0.966) total time=   7.9s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678369 -> initscore=0.746285\n",
      "[LightGBM] [Info] Start training from score 0.746285\n",
      "[CV 5/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=746; f1: (test=0.959) precision: (test=0.958) recall: (test=0.960) total time=   8.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 1/5] END class_weight=balanced, n_estimators=140; f1: (test=0.930) precision: (test=0.973) recall: (test=0.891) total time=   2.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/5] END class_weight=balanced, n_estimators=140; f1: (test=0.925) precision: (test=0.969) recall: (test=0.885) total time=   2.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/5] END class_weight=balanced, n_estimators=140; f1: (test=0.929) precision: (test=0.977) recall: (test=0.885) total time=   2.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 4/5] END class_weight=balanced, n_estimators=140; f1: (test=0.935) precision: (test=0.972) recall: (test=0.901) total time=   2.8s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 5/5] END class_weight=balanced, n_estimators=140; f1: (test=0.928) precision: (test=0.977) recall: (test=0.884) total time=   2.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=684; f1: (test=0.968) precision: (test=0.948) recall: (test=0.988) total time=   7.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=684; f1: (test=0.964) precision: (test=0.947) recall: (test=0.982) total time=   8.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=684; f1: (test=0.968) precision: (test=0.951) recall: (test=0.987) total time=   7.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=684; f1: (test=0.965) precision: (test=0.946) recall: (test=0.985) total time=   8.4s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=684; f1: (test=0.966) precision: (test=0.950) recall: (test=0.983) total time=   7.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 1/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=151; f1: (test=0.932) precision: (test=0.971) recall: (test=0.896) total time=   2.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 2/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=151; f1: (test=0.927) precision: (test=0.968) recall: (test=0.890) total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 3/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=151; f1: (test=0.933) precision: (test=0.978) recall: (test=0.892) total time=   2.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513060 -> initscore=0.052252\n",
      "[LightGBM] [Info] Start training from score 0.052252\n",
      "[CV 4/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=151; f1: (test=0.936) precision: (test=0.972) recall: (test=0.903) total time=   2.7s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513281 -> initscore=0.053138\n",
      "[LightGBM] [Info] Start training from score 0.053138\n",
      "[CV 5/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=151; f1: (test=0.931) precision: (test=0.976) recall: (test=0.890) total time=   2.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=919; f1: (test=0.967) precision: (test=0.948) recall: (test=0.987) total time=   9.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=919; f1: (test=0.964) precision: (test=0.946) recall: (test=0.981) total time=   9.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=919; f1: (test=0.968) precision: (test=0.950) recall: (test=0.986) total time=   9.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=919; f1: (test=0.965) precision: (test=0.946) recall: (test=0.986) total time=   9.5s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061664 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=919; f1: (test=0.966) precision: (test=0.950) recall: (test=0.982) total time=   9.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 1/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=221; f1: (test=0.949) precision: (test=0.961) recall: (test=0.937) total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 2/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=221; f1: (test=0.948) precision: (test=0.961) recall: (test=0.935) total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 3/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=221; f1: (test=0.950) precision: (test=0.968) recall: (test=0.934) total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678175 -> initscore=0.745399\n",
      "[LightGBM] [Info] Start training from score 0.745399\n",
      "[CV 4/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=221; f1: (test=0.953) precision: (test=0.963) recall: (test=0.943) total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678369 -> initscore=0.746285\n",
      "[LightGBM] [Info] Start training from score 0.746285\n",
      "[CV 5/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=221; f1: (test=0.952) precision: (test=0.968) recall: (test=0.937) total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=932; f1: (test=0.967) precision: (test=0.948) recall: (test=0.986) total time=  10.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=932; f1: (test=0.963) precision: (test=0.946) recall: (test=0.981) total time=  10.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=932; f1: (test=0.968) precision: (test=0.950) recall: (test=0.986) total time=   9.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=932; f1: (test=0.965) precision: (test=0.946) recall: (test=0.986) total time=   9.5s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=932; f1: (test=0.966) precision: (test=0.951) recall: (test=0.983) total time=   9.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=565; f1: (test=0.968) precision: (test=0.948) recall: (test=0.988) total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=565; f1: (test=0.965) precision: (test=0.947) recall: (test=0.982) total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066591 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=565; f1: (test=0.969) precision: (test=0.951) recall: (test=0.987) total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=565; f1: (test=0.965) precision: (test=0.945) recall: (test=0.985) total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=565; f1: (test=0.966) precision: (test=0.950) recall: (test=0.983) total time=   6.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 1/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=920; f1: (test=0.959) precision: (test=0.952) recall: (test=0.966) total time=  10.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 2/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=920; f1: (test=0.957) precision: (test=0.954) recall: (test=0.959) total time=   9.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 3/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=920; f1: (test=0.958) precision: (test=0.958) recall: (test=0.957) total time=  10.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070523 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678175 -> initscore=0.745399\n",
      "[LightGBM] [Info] Start training from score 0.745399\n",
      "[CV 4/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=920; f1: (test=0.959) precision: (test=0.952) recall: (test=0.966) total time=  11.3s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678369 -> initscore=0.746285\n",
      "[LightGBM] [Info] Start training from score 0.746285\n",
      "[CV 5/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=920; f1: (test=0.960) precision: (test=0.957) recall: (test=0.963) total time=  11.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=587; f1: (test=0.968) precision: (test=0.948) recall: (test=0.988) total time=  21.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=587; f1: (test=0.965) precision: (test=0.947) recall: (test=0.982) total time=  10.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=587; f1: (test=0.969) precision: (test=0.951) recall: (test=0.987) total time=  10.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=587; f1: (test=0.965) precision: (test=0.946) recall: (test=0.985) total time=  11.3s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.106514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=587; f1: (test=0.966) precision: (test=0.950) recall: (test=0.984) total time=  10.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=76; f1: (test=0.965) precision: (test=0.942) recall: (test=0.990) total time=   2.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115728 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=76; f1: (test=0.966) precision: (test=0.943) recall: (test=0.991) total time=   2.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=76; f1: (test=0.970) precision: (test=0.948) recall: (test=0.993) total time=   2.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=76; f1: (test=0.967) precision: (test=0.942) recall: (test=0.993) total time=   2.4s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=76; f1: (test=0.968) precision: (test=0.945) recall: (test=0.992) total time=   2.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=144; f1: (test=0.966) precision: (test=0.943) recall: (test=0.990) total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=144; f1: (test=0.966) precision: (test=0.944) recall: (test=0.989) total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=144; f1: (test=0.970) precision: (test=0.950) recall: (test=0.990) total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112495 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=144; f1: (test=0.966) precision: (test=0.942) recall: (test=0.990) total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=144; f1: (test=0.968) precision: (test=0.946) recall: (test=0.991) total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=397; f1: (test=0.968) precision: (test=0.947) recall: (test=0.989) total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=397; f1: (test=0.965) precision: (test=0.947) recall: (test=0.983) total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=397; f1: (test=0.969) precision: (test=0.951) recall: (test=0.986) total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=397; f1: (test=0.966) precision: (test=0.946) recall: (test=0.986) total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=397; f1: (test=0.967) precision: (test=0.948) recall: (test=0.986) total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=972; f1: (test=0.967) precision: (test=0.948) recall: (test=0.986) total time=  11.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092679 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=972; f1: (test=0.963) precision: (test=0.946) recall: (test=0.981) total time=  14.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076576 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=972; f1: (test=0.968) precision: (test=0.951) recall: (test=0.986) total time=   9.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=972; f1: (test=0.965) precision: (test=0.946) recall: (test=0.985) total time=  10.3s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=972; f1: (test=0.966) precision: (test=0.950) recall: (test=0.981) total time=  10.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=802; f1: (test=0.967) precision: (test=0.949) recall: (test=0.986) total time=   9.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=802; f1: (test=0.963) precision: (test=0.946) recall: (test=0.981) total time=   9.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=802; f1: (test=0.969) precision: (test=0.951) recall: (test=0.987) total time=   8.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=802; f1: (test=0.965) precision: (test=0.946) recall: (test=0.985) total time=   9.2s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=802; f1: (test=0.966) precision: (test=0.950) recall: (test=0.982) total time=   9.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 1/5] END class_weight=balanced, n_estimators=579; f1: (test=0.947) precision: (test=0.958) recall: (test=0.936) total time=   8.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.123026 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/5] END class_weight=balanced, n_estimators=579; f1: (test=0.949) precision: (test=0.960) recall: (test=0.939) total time=   9.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/5] END class_weight=balanced, n_estimators=579; f1: (test=0.948) precision: (test=0.962) recall: (test=0.935) total time=   7.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 4/5] END class_weight=balanced, n_estimators=579; f1: (test=0.952) precision: (test=0.961) recall: (test=0.944) total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 5/5] END class_weight=balanced, n_estimators=579; f1: (test=0.952) precision: (test=0.965) recall: (test=0.939) total time=   7.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 1/5] END class_weight=balanced, n_estimators=592; f1: (test=0.947) precision: (test=0.958) recall: (test=0.937) total time=   8.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/5] END class_weight=balanced, n_estimators=592; f1: (test=0.950) precision: (test=0.960) recall: (test=0.939) total time=   9.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056680 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/5] END class_weight=balanced, n_estimators=592; f1: (test=0.948) precision: (test=0.962) recall: (test=0.936) total time=   7.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 4/5] END class_weight=balanced, n_estimators=592; f1: (test=0.952) precision: (test=0.961) recall: (test=0.944) total time=   7.4s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END class_weight=balanced, n_estimators=592; f1: (test=0.953) precision: (test=0.964) recall: (test=0.941) total time=   6.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076915 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 1/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=256; f1: (test=0.949) precision: (test=0.960) recall: (test=0.939) total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 2/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=256; f1: (test=0.950) precision: (test=0.960) recall: (test=0.940) total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104373 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 3/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=256; f1: (test=0.952) precision: (test=0.968) recall: (test=0.937) total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678175 -> initscore=0.745399\n",
      "[LightGBM] [Info] Start training from score 0.745399\n",
      "[CV 4/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=256; f1: (test=0.954) precision: (test=0.963) recall: (test=0.945) total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678369 -> initscore=0.746285\n",
      "[LightGBM] [Info] Start training from score 0.746285\n",
      "[CV 5/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=256; f1: (test=0.953) precision: (test=0.966) recall: (test=0.940) total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080472 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 1/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=241; f1: (test=0.940) precision: (test=0.965) recall: (test=0.917) total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 2/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=241; f1: (test=0.936) precision: (test=0.965) recall: (test=0.909) total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 3/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=241; f1: (test=0.941) precision: (test=0.973) recall: (test=0.910) total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513060 -> initscore=0.052252\n",
      "[LightGBM] [Info] Start training from score 0.052252\n",
      "[CV 4/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=241; f1: (test=0.943) precision: (test=0.968) recall: (test=0.920) total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513281 -> initscore=0.053138\n",
      "[LightGBM] [Info] Start training from score 0.053138\n",
      "[CV 5/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=241; f1: (test=0.941) precision: (test=0.974) recall: (test=0.910) total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 1/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=561; f1: (test=0.956) precision: (test=0.955) recall: (test=0.958) total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 2/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=561; f1: (test=0.955) precision: (test=0.957) recall: (test=0.953) total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678352 -> initscore=0.746208\n",
      "[LightGBM] [Info] Start training from score 0.746208\n",
      "[CV 3/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=561; f1: (test=0.955) precision: (test=0.960) recall: (test=0.951) total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071815 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678175 -> initscore=0.745399\n",
      "[LightGBM] [Info] Start training from score 0.745399\n",
      "[CV 4/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=561; f1: (test=0.958) precision: (test=0.955) recall: (test=0.961) total time=   6.7s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678369 -> initscore=0.746285\n",
      "[LightGBM] [Info] Start training from score 0.746285\n",
      "[CV 5/5] END class_weight={0: 5.0, 1: 1.0}, n_estimators=561; f1: (test=0.958) precision: (test=0.960) recall: (test=0.955) total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 1/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=855; f1: (test=0.949) precision: (test=0.954) recall: (test=0.944) total time=   9.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072136 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 2/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=855; f1: (test=0.950) precision: (test=0.956) recall: (test=0.944) total time=   8.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 3/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=855; f1: (test=0.951) precision: (test=0.959) recall: (test=0.943) total time=   8.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513060 -> initscore=0.052252\n",
      "[LightGBM] [Info] Start training from score 0.052252\n",
      "[CV 4/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=855; f1: (test=0.953) precision: (test=0.956) recall: (test=0.950) total time=   9.2s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513281 -> initscore=0.053138\n",
      "[LightGBM] [Info] Start training from score 0.053138\n",
      "[CV 5/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=855; f1: (test=0.953) precision: (test=0.959) recall: (test=0.946) total time=   8.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 1/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=377; f1: (test=0.946) precision: (test=0.960) recall: (test=0.932) total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 2/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=377; f1: (test=0.943) precision: (test=0.961) recall: (test=0.926) total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056875 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 3/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=377; f1: (test=0.948) precision: (test=0.969) recall: (test=0.928) total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513060 -> initscore=0.052252\n",
      "[LightGBM] [Info] Start training from score 0.052252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=377; f1: (test=0.948) precision: (test=0.965) recall: (test=0.931) total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513281 -> initscore=0.053138\n",
      "[LightGBM] [Info] Start training from score 0.053138\n",
      "[CV 5/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=377; f1: (test=0.946) precision: (test=0.968) recall: (test=0.925) total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 1/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=589; f1: (test=0.949) precision: (test=0.956) recall: (test=0.941) total time=   6.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 2/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=589; f1: (test=0.947) precision: (test=0.958) recall: (test=0.937) total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 3/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=589; f1: (test=0.950) precision: (test=0.961) recall: (test=0.938) total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513060 -> initscore=0.052252\n",
      "[LightGBM] [Info] Start training from score 0.052252\n",
      "[CV 4/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=589; f1: (test=0.951) precision: (test=0.959) recall: (test=0.944) total time=   7.1s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513281 -> initscore=0.053138\n",
      "[LightGBM] [Info] Start training from score 0.053138\n",
      "[CV 5/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=589; f1: (test=0.951) precision: (test=0.963) recall: (test=0.939) total time=   6.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088574 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 1/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=418; f1: (test=0.947) precision: (test=0.958) recall: (test=0.935) total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 2/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=418; f1: (test=0.946) precision: (test=0.961) recall: (test=0.930) total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 3/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=418; f1: (test=0.949) precision: (test=0.967) recall: (test=0.932) total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513060 -> initscore=0.052252\n",
      "[LightGBM] [Info] Start training from score 0.052252\n",
      "[CV 4/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=418; f1: (test=0.949) precision: (test=0.964) recall: (test=0.935) total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513281 -> initscore=0.053138\n",
      "[LightGBM] [Info] Start training from score 0.053138\n",
      "[CV 5/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=418; f1: (test=0.946) precision: (test=0.967) recall: (test=0.927) total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=495; f1: (test=0.948) precision: (test=0.958) recall: (test=0.938) total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 2/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=495; f1: (test=0.947) precision: (test=0.960) recall: (test=0.934) total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513262 -> initscore=0.053061\n",
      "[LightGBM] [Info] Start training from score 0.053061\n",
      "[CV 3/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=495; f1: (test=0.950) precision: (test=0.965) recall: (test=0.935) total time=   7.2s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513060 -> initscore=0.052252\n",
      "[LightGBM] [Info] Start training from score 0.052252\n",
      "[CV 4/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=495; f1: (test=0.951) precision: (test=0.962) recall: (test=0.940) total time=   8.3s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513281 -> initscore=0.053138\n",
      "[LightGBM] [Info] Start training from score 0.053138\n",
      "[CV 5/5] END class_weight={0: 10.0, 1: 1.0}, n_estimators=495; f1: (test=0.948) precision: (test=0.965) recall: (test=0.931) total time=   8.8s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111692 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33320\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1524\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 1/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=566; f1: (test=0.968) precision: (test=0.948) recall: (test=0.988) total time=   8.7s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076862 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33478\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 2/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=566; f1: (test=0.965) precision: (test=0.947) recall: (test=0.982) total time=   8.0s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33000\n",
      "[LightGBM] [Info] Number of data points in the train set: 14258, number of used features: 1509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913382 -> initscore=2.355646\n",
      "[LightGBM] [Info] Start training from score 2.355646\n",
      "[CV 3/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=566; f1: (test=0.969) precision: (test=0.951) recall: (test=0.988) total time=   8.1s\n",
      "[LightGBM] [Info] Number of positive: 13023, number of negative: 1236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33164\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1500\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913318 -> initscore=2.354837\n",
      "[LightGBM] [Info] Start training from score 2.354837\n",
      "[CV 4/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=566; f1: (test=0.965) precision: (test=0.945) recall: (test=0.985) total time=   9.5s\n",
      "[LightGBM] [Info] Number of positive: 13024, number of negative: 1235\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33241\n",
      "[LightGBM] [Info] Number of data points in the train set: 14259, number of used features: 1487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913388 -> initscore=2.355723\n",
      "[LightGBM] [Info] Start training from score 2.355723\n",
      "[CV 5/5] END class_weight={0: 1.0, 1: 1.0}, n_estimators=566; f1: (test=0.966) precision: (test=0.950) recall: (test=0.983) total time=   7.4s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_precision</th>\n",
       "      <th>split1_test_precision</th>\n",
       "      <th>split2_test_precision</th>\n",
       "      <th>...</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>rank_test_recall</th>\n",
       "      <th>split0_test_f1</th>\n",
       "      <th>split1_test_f1</th>\n",
       "      <th>split2_test_f1</th>\n",
       "      <th>split3_test_f1</th>\n",
       "      <th>split4_test_f1</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.699475</td>\n",
       "      <td>0.180762</td>\n",
       "      <td>0.066927</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>76</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.942122</td>\n",
       "      <td>0.942999</td>\n",
       "      <td>0.947816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965404</td>\n",
       "      <td>0.966302</td>\n",
       "      <td>0.969852</td>\n",
       "      <td>0.966951</td>\n",
       "      <td>0.967626</td>\n",
       "      <td>0.967227</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.416172</td>\n",
       "      <td>0.161599</td>\n",
       "      <td>0.104414</td>\n",
       "      <td>0.009275</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>144</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.943225</td>\n",
       "      <td>0.944282</td>\n",
       "      <td>0.950457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>3</td>\n",
       "      <td>0.965982</td>\n",
       "      <td>0.966097</td>\n",
       "      <td>0.969761</td>\n",
       "      <td>0.965559</td>\n",
       "      <td>0.967887</td>\n",
       "      <td>0.967057</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.097562</td>\n",
       "      <td>0.174045</td>\n",
       "      <td>0.149262</td>\n",
       "      <td>0.007325</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>250</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.946240</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.950931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>6</td>\n",
       "      <td>0.967267</td>\n",
       "      <td>0.965060</td>\n",
       "      <td>0.969122</td>\n",
       "      <td>0.966231</td>\n",
       "      <td>0.967383</td>\n",
       "      <td>0.967013</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.776887</td>\n",
       "      <td>0.644607</td>\n",
       "      <td>0.190811</td>\n",
       "      <td>0.018065</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>334</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.947012</td>\n",
       "      <td>0.947400</td>\n",
       "      <td>0.950650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>7</td>\n",
       "      <td>0.967083</td>\n",
       "      <td>0.965663</td>\n",
       "      <td>0.968976</td>\n",
       "      <td>0.965870</td>\n",
       "      <td>0.967198</td>\n",
       "      <td>0.966958</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.381330</td>\n",
       "      <td>0.234753</td>\n",
       "      <td>0.150377</td>\n",
       "      <td>0.017984</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>200</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.944591</td>\n",
       "      <td>0.946887</td>\n",
       "      <td>0.950723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>5</td>\n",
       "      <td>0.966552</td>\n",
       "      <td>0.965839</td>\n",
       "      <td>0.969752</td>\n",
       "      <td>0.965217</td>\n",
       "      <td>0.967393</td>\n",
       "      <td>0.966951</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.607064</td>\n",
       "      <td>0.232923</td>\n",
       "      <td>0.195960</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>321</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.947291</td>\n",
       "      <td>0.947664</td>\n",
       "      <td>0.950917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>8</td>\n",
       "      <td>0.967228</td>\n",
       "      <td>0.965652</td>\n",
       "      <td>0.968967</td>\n",
       "      <td>0.965725</td>\n",
       "      <td>0.966897</td>\n",
       "      <td>0.966894</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.753953</td>\n",
       "      <td>0.175939</td>\n",
       "      <td>0.123820</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>167</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.944070</td>\n",
       "      <td>0.945851</td>\n",
       "      <td>0.950442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>4</td>\n",
       "      <td>0.966572</td>\n",
       "      <td>0.966035</td>\n",
       "      <td>0.969606</td>\n",
       "      <td>0.964783</td>\n",
       "      <td>0.967422</td>\n",
       "      <td>0.966884</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.087639</td>\n",
       "      <td>0.136404</td>\n",
       "      <td>0.093727</td>\n",
       "      <td>0.009881</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>124</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.942088</td>\n",
       "      <td>0.944282</td>\n",
       "      <td>0.949367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>2</td>\n",
       "      <td>0.965094</td>\n",
       "      <td>0.966097</td>\n",
       "      <td>0.969487</td>\n",
       "      <td>0.965724</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.966829</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.759002</td>\n",
       "      <td>0.395646</td>\n",
       "      <td>0.216329</td>\n",
       "      <td>0.013375</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>397</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.947322</td>\n",
       "      <td>0.947322</td>\n",
       "      <td>0.951422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>9</td>\n",
       "      <td>0.967538</td>\n",
       "      <td>0.964883</td>\n",
       "      <td>0.968637</td>\n",
       "      <td>0.965704</td>\n",
       "      <td>0.966867</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.927969</td>\n",
       "      <td>0.259288</td>\n",
       "      <td>0.291329</td>\n",
       "      <td>0.030403</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>506</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.948098</td>\n",
       "      <td>0.947852</td>\n",
       "      <td>0.951422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>12</td>\n",
       "      <td>0.967354</td>\n",
       "      <td>0.964862</td>\n",
       "      <td>0.968637</td>\n",
       "      <td>0.965860</td>\n",
       "      <td>0.966204</td>\n",
       "      <td>0.966583</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.029583</td>\n",
       "      <td>0.378627</td>\n",
       "      <td>0.393761</td>\n",
       "      <td>0.072249</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>715</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.948627</td>\n",
       "      <td>0.946979</td>\n",
       "      <td>0.951479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>18</td>\n",
       "      <td>0.967334</td>\n",
       "      <td>0.964113</td>\n",
       "      <td>0.969259</td>\n",
       "      <td>0.965237</td>\n",
       "      <td>0.966631</td>\n",
       "      <td>0.966515</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.388265</td>\n",
       "      <td>4.117636</td>\n",
       "      <td>0.393725</td>\n",
       "      <td>0.078679</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>587</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.948128</td>\n",
       "      <td>0.947290</td>\n",
       "      <td>0.950902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>11</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>0.964571</td>\n",
       "      <td>0.968811</td>\n",
       "      <td>0.965092</td>\n",
       "      <td>0.966350</td>\n",
       "      <td>0.966498</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.550388</td>\n",
       "      <td>0.300657</td>\n",
       "      <td>0.258309</td>\n",
       "      <td>0.015854</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>485</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.947803</td>\n",
       "      <td>0.947587</td>\n",
       "      <td>0.950858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>10</td>\n",
       "      <td>0.967053</td>\n",
       "      <td>0.964873</td>\n",
       "      <td>0.968345</td>\n",
       "      <td>0.965714</td>\n",
       "      <td>0.966360</td>\n",
       "      <td>0.966469</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.148095</td>\n",
       "      <td>0.331638</td>\n",
       "      <td>0.271077</td>\n",
       "      <td>0.012973</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>495</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.947818</td>\n",
       "      <td>0.947571</td>\n",
       "      <td>0.951422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>12</td>\n",
       "      <td>0.967208</td>\n",
       "      <td>0.964717</td>\n",
       "      <td>0.968637</td>\n",
       "      <td>0.965714</td>\n",
       "      <td>0.966058</td>\n",
       "      <td>0.966467</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.103303</td>\n",
       "      <td>0.720893</td>\n",
       "      <td>0.355317</td>\n",
       "      <td>0.049041</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>566</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.948128</td>\n",
       "      <td>0.947290</td>\n",
       "      <td>0.951479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>14</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>0.964571</td>\n",
       "      <td>0.969259</td>\n",
       "      <td>0.964801</td>\n",
       "      <td>0.966038</td>\n",
       "      <td>0.966467</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.993068</td>\n",
       "      <td>0.250673</td>\n",
       "      <td>0.254256</td>\n",
       "      <td>0.003926</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>473</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.947803</td>\n",
       "      <td>0.947587</td>\n",
       "      <td>0.951126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>16</td>\n",
       "      <td>0.967053</td>\n",
       "      <td>0.964873</td>\n",
       "      <td>0.968335</td>\n",
       "      <td>0.965548</td>\n",
       "      <td>0.966214</td>\n",
       "      <td>0.966405</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.661269</td>\n",
       "      <td>0.259241</td>\n",
       "      <td>0.368911</td>\n",
       "      <td>0.047452</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>684</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.948408</td>\n",
       "      <td>0.947275</td>\n",
       "      <td>0.950592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>17</td>\n",
       "      <td>0.967810</td>\n",
       "      <td>0.964415</td>\n",
       "      <td>0.968354</td>\n",
       "      <td>0.965102</td>\n",
       "      <td>0.966340</td>\n",
       "      <td>0.966404</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.440078</td>\n",
       "      <td>0.196475</td>\n",
       "      <td>0.287438</td>\n",
       "      <td>0.007635</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>565</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.948128</td>\n",
       "      <td>0.947290</td>\n",
       "      <td>0.950902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>15</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>0.964571</td>\n",
       "      <td>0.968811</td>\n",
       "      <td>0.964801</td>\n",
       "      <td>0.966038</td>\n",
       "      <td>0.966377</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.880516</td>\n",
       "      <td>0.294736</td>\n",
       "      <td>0.387970</td>\n",
       "      <td>0.012140</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>802</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.948892</td>\n",
       "      <td>0.946106</td>\n",
       "      <td>0.951183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>20</td>\n",
       "      <td>0.967324</td>\n",
       "      <td>0.963365</td>\n",
       "      <td>0.968957</td>\n",
       "      <td>0.965247</td>\n",
       "      <td>0.965715</td>\n",
       "      <td>0.966122</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8.058729</td>\n",
       "      <td>0.215926</td>\n",
       "      <td>0.369865</td>\n",
       "      <td>0.007464</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>778</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.948332</td>\n",
       "      <td>0.946651</td>\n",
       "      <td>0.951155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>21</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.963499</td>\n",
       "      <td>0.968646</td>\n",
       "      <td>0.965247</td>\n",
       "      <td>0.966027</td>\n",
       "      <td>0.966091</td>\n",
       "      <td>0.001723</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.431266</td>\n",
       "      <td>0.491859</td>\n",
       "      <td>0.427991</td>\n",
       "      <td>0.037751</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>778</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.948332</td>\n",
       "      <td>0.946651</td>\n",
       "      <td>0.951155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>21</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.963499</td>\n",
       "      <td>0.968646</td>\n",
       "      <td>0.965247</td>\n",
       "      <td>0.966027</td>\n",
       "      <td>0.966091</td>\n",
       "      <td>0.001723</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.058358</td>\n",
       "      <td>0.142582</td>\n",
       "      <td>0.423618</td>\n",
       "      <td>0.016537</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>919</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.948083</td>\n",
       "      <td>0.946386</td>\n",
       "      <td>0.950015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>19</td>\n",
       "      <td>0.967198</td>\n",
       "      <td>0.963510</td>\n",
       "      <td>0.967907</td>\n",
       "      <td>0.965403</td>\n",
       "      <td>0.966017</td>\n",
       "      <td>0.966007</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9.506170</td>\n",
       "      <td>0.310538</td>\n",
       "      <td>0.447899</td>\n",
       "      <td>0.041781</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>932</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.948052</td>\n",
       "      <td>0.946090</td>\n",
       "      <td>0.949719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>23</td>\n",
       "      <td>0.966887</td>\n",
       "      <td>0.963209</td>\n",
       "      <td>0.967606</td>\n",
       "      <td>0.965258</td>\n",
       "      <td>0.966465</td>\n",
       "      <td>0.965885</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11.010930</td>\n",
       "      <td>1.768648</td>\n",
       "      <td>0.510778</td>\n",
       "      <td>0.065188</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>972</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.948052</td>\n",
       "      <td>0.946074</td>\n",
       "      <td>0.950577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>24</td>\n",
       "      <td>0.966887</td>\n",
       "      <td>0.963052</td>\n",
       "      <td>0.968199</td>\n",
       "      <td>0.965247</td>\n",
       "      <td>0.965538</td>\n",
       "      <td>0.965785</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.255647</td>\n",
       "      <td>0.571337</td>\n",
       "      <td>0.481648</td>\n",
       "      <td>0.059350</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>920</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.952453</td>\n",
       "      <td>0.953893</td>\n",
       "      <td>0.957898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003504</td>\n",
       "      <td>26</td>\n",
       "      <td>0.959134</td>\n",
       "      <td>0.956668</td>\n",
       "      <td>0.957604</td>\n",
       "      <td>0.958854</td>\n",
       "      <td>0.959571</td>\n",
       "      <td>0.958366</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.004831</td>\n",
       "      <td>0.450572</td>\n",
       "      <td>0.454781</td>\n",
       "      <td>0.008287</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>988</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.952208</td>\n",
       "      <td>0.953077</td>\n",
       "      <td>0.957552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>25</td>\n",
       "      <td>0.959464</td>\n",
       "      <td>0.956868</td>\n",
       "      <td>0.956816</td>\n",
       "      <td>0.959159</td>\n",
       "      <td>0.959437</td>\n",
       "      <td>0.958348</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>30.128286</td>\n",
       "      <td>36.270933</td>\n",
       "      <td>0.537506</td>\n",
       "      <td>0.179703</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>918</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.952150</td>\n",
       "      <td>0.953893</td>\n",
       "      <td>0.957898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>27</td>\n",
       "      <td>0.958829</td>\n",
       "      <td>0.956668</td>\n",
       "      <td>0.957604</td>\n",
       "      <td>0.958695</td>\n",
       "      <td>0.959571</td>\n",
       "      <td>0.958273</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.876095</td>\n",
       "      <td>0.261830</td>\n",
       "      <td>0.313975</td>\n",
       "      <td>0.015616</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>644</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.954878</td>\n",
       "      <td>0.955118</td>\n",
       "      <td>0.959222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>29</td>\n",
       "      <td>0.958384</td>\n",
       "      <td>0.954678</td>\n",
       "      <td>0.956415</td>\n",
       "      <td>0.958607</td>\n",
       "      <td>0.959262</td>\n",
       "      <td>0.957469</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8.256545</td>\n",
       "      <td>0.602532</td>\n",
       "      <td>0.362524</td>\n",
       "      <td>0.010433</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>746</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.953206</td>\n",
       "      <td>0.953474</td>\n",
       "      <td>0.958565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>28</td>\n",
       "      <td>0.958302</td>\n",
       "      <td>0.955082</td>\n",
       "      <td>0.955316</td>\n",
       "      <td>0.959561</td>\n",
       "      <td>0.959030</td>\n",
       "      <td>0.957458</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6.367509</td>\n",
       "      <td>0.185909</td>\n",
       "      <td>0.268694</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>561</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.954991</td>\n",
       "      <td>0.956535</td>\n",
       "      <td>0.960285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>30</td>\n",
       "      <td>0.956455</td>\n",
       "      <td>0.954769</td>\n",
       "      <td>0.955394</td>\n",
       "      <td>0.957893</td>\n",
       "      <td>0.957794</td>\n",
       "      <td>0.956461</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.668172</td>\n",
       "      <td>0.297146</td>\n",
       "      <td>0.239958</td>\n",
       "      <td>0.030111</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>448</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.956240</td>\n",
       "      <td>0.958879</td>\n",
       "      <td>0.961970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>31</td>\n",
       "      <td>0.954622</td>\n",
       "      <td>0.952057</td>\n",
       "      <td>0.954827</td>\n",
       "      <td>0.957070</td>\n",
       "      <td>0.957032</td>\n",
       "      <td>0.955122</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.336304</td>\n",
       "      <td>0.269833</td>\n",
       "      <td>0.244869</td>\n",
       "      <td>0.016218</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>436</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.955919</td>\n",
       "      <td>0.958541</td>\n",
       "      <td>0.963183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003668</td>\n",
       "      <td>32</td>\n",
       "      <td>0.954154</td>\n",
       "      <td>0.951423</td>\n",
       "      <td>0.955580</td>\n",
       "      <td>0.957218</td>\n",
       "      <td>0.956697</td>\n",
       "      <td>0.955014</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.842337</td>\n",
       "      <td>0.122179</td>\n",
       "      <td>0.192951</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>369</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.957440</td>\n",
       "      <td>0.959688</td>\n",
       "      <td>0.965149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003190</td>\n",
       "      <td>34</td>\n",
       "      <td>0.951969</td>\n",
       "      <td>0.951363</td>\n",
       "      <td>0.954510</td>\n",
       "      <td>0.956052</td>\n",
       "      <td>0.956616</td>\n",
       "      <td>0.954102</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5.536524</td>\n",
       "      <td>0.583180</td>\n",
       "      <td>0.213571</td>\n",
       "      <td>0.042581</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>346</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.957996</td>\n",
       "      <td>0.959075</td>\n",
       "      <td>0.965745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002780</td>\n",
       "      <td>35</td>\n",
       "      <td>0.951777</td>\n",
       "      <td>0.950906</td>\n",
       "      <td>0.954644</td>\n",
       "      <td>0.955851</td>\n",
       "      <td>0.955645</td>\n",
       "      <td>0.953765</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>12.240656</td>\n",
       "      <td>0.681623</td>\n",
       "      <td>0.491045</td>\n",
       "      <td>0.026456</td>\n",
       "      <td>balanced</td>\n",
       "      <td>997</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 997}</td>\n",
       "      <td>0.954012</td>\n",
       "      <td>0.954447</td>\n",
       "      <td>0.958489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>33</td>\n",
       "      <td>0.951663</td>\n",
       "      <td>0.950177</td>\n",
       "      <td>0.950774</td>\n",
       "      <td>0.953274</td>\n",
       "      <td>0.954046</td>\n",
       "      <td>0.951987</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.685159</td>\n",
       "      <td>0.477053</td>\n",
       "      <td>0.156808</td>\n",
       "      <td>0.014913</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>256</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.959523</td>\n",
       "      <td>0.959862</td>\n",
       "      <td>0.967947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>39</td>\n",
       "      <td>0.949247</td>\n",
       "      <td>0.949884</td>\n",
       "      <td>0.952084</td>\n",
       "      <td>0.953651</td>\n",
       "      <td>0.952826</td>\n",
       "      <td>0.951538</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>8.556572</td>\n",
       "      <td>0.214952</td>\n",
       "      <td>0.391532</td>\n",
       "      <td>0.009964</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>855</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...</td>\n",
       "      <td>0.954376</td>\n",
       "      <td>0.956157</td>\n",
       "      <td>0.959101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>36</td>\n",
       "      <td>0.949367</td>\n",
       "      <td>0.950247</td>\n",
       "      <td>0.951231</td>\n",
       "      <td>0.953026</td>\n",
       "      <td>0.952528</td>\n",
       "      <td>0.951280</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>9.178321</td>\n",
       "      <td>0.548067</td>\n",
       "      <td>0.373143</td>\n",
       "      <td>0.032954</td>\n",
       "      <td>balanced</td>\n",
       "      <td>765</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 765}</td>\n",
       "      <td>0.955549</td>\n",
       "      <td>0.956888</td>\n",
       "      <td>0.959574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>37</td>\n",
       "      <td>0.949791</td>\n",
       "      <td>0.948738</td>\n",
       "      <td>0.949899</td>\n",
       "      <td>0.953130</td>\n",
       "      <td>0.952912</td>\n",
       "      <td>0.950894</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>8.224736</td>\n",
       "      <td>0.667928</td>\n",
       "      <td>0.382321</td>\n",
       "      <td>0.051739</td>\n",
       "      <td>balanced</td>\n",
       "      <td>725</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 725}</td>\n",
       "      <td>0.955791</td>\n",
       "      <td>0.957733</td>\n",
       "      <td>0.959824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003402</td>\n",
       "      <td>38</td>\n",
       "      <td>0.949289</td>\n",
       "      <td>0.948527</td>\n",
       "      <td>0.949395</td>\n",
       "      <td>0.953381</td>\n",
       "      <td>0.952588</td>\n",
       "      <td>0.950636</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3.540189</td>\n",
       "      <td>0.247366</td>\n",
       "      <td>0.129443</td>\n",
       "      <td>0.006163</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>221</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.960655</td>\n",
       "      <td>0.961478</td>\n",
       "      <td>0.967845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>45</td>\n",
       "      <td>0.948857</td>\n",
       "      <td>0.948155</td>\n",
       "      <td>0.950446</td>\n",
       "      <td>0.952824</td>\n",
       "      <td>0.952217</td>\n",
       "      <td>0.950500</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>7.707880</td>\n",
       "      <td>1.076960</td>\n",
       "      <td>0.312287</td>\n",
       "      <td>0.022122</td>\n",
       "      <td>balanced</td>\n",
       "      <td>592</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 592}</td>\n",
       "      <td>0.957614</td>\n",
       "      <td>0.960440</td>\n",
       "      <td>0.961502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>42</td>\n",
       "      <td>0.947058</td>\n",
       "      <td>0.949853</td>\n",
       "      <td>0.948482</td>\n",
       "      <td>0.952455</td>\n",
       "      <td>0.952588</td>\n",
       "      <td>0.950087</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7.763043</td>\n",
       "      <td>0.903713</td>\n",
       "      <td>0.340966</td>\n",
       "      <td>0.025013</td>\n",
       "      <td>balanced</td>\n",
       "      <td>579</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 579}</td>\n",
       "      <td>0.957862</td>\n",
       "      <td>0.959824</td>\n",
       "      <td>0.961781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>44</td>\n",
       "      <td>0.946551</td>\n",
       "      <td>0.949395</td>\n",
       "      <td>0.948303</td>\n",
       "      <td>0.952292</td>\n",
       "      <td>0.951907</td>\n",
       "      <td>0.949689</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6.478772</td>\n",
       "      <td>0.243360</td>\n",
       "      <td>0.297442</td>\n",
       "      <td>0.026167</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>589</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...</td>\n",
       "      <td>0.956020</td>\n",
       "      <td>0.957627</td>\n",
       "      <td>0.961296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>41</td>\n",
       "      <td>0.948623</td>\n",
       "      <td>0.947221</td>\n",
       "      <td>0.949643</td>\n",
       "      <td>0.951408</td>\n",
       "      <td>0.950825</td>\n",
       "      <td>0.949544</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>7.437928</td>\n",
       "      <td>0.900784</td>\n",
       "      <td>0.299337</td>\n",
       "      <td>0.012187</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>609</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...</td>\n",
       "      <td>0.956277</td>\n",
       "      <td>0.957654</td>\n",
       "      <td>0.960655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>40</td>\n",
       "      <td>0.948281</td>\n",
       "      <td>0.947548</td>\n",
       "      <td>0.948857</td>\n",
       "      <td>0.951910</td>\n",
       "      <td>0.951019</td>\n",
       "      <td>0.949523</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>6.749089</td>\n",
       "      <td>0.373555</td>\n",
       "      <td>0.285617</td>\n",
       "      <td>0.011030</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>584</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...</td>\n",
       "      <td>0.956305</td>\n",
       "      <td>0.957300</td>\n",
       "      <td>0.961587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>43</td>\n",
       "      <td>0.948607</td>\n",
       "      <td>0.946747</td>\n",
       "      <td>0.949627</td>\n",
       "      <td>0.951540</td>\n",
       "      <td>0.950498</td>\n",
       "      <td>0.949404</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>7.169069</td>\n",
       "      <td>1.051897</td>\n",
       "      <td>0.279301</td>\n",
       "      <td>0.027196</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>495</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...</td>\n",
       "      <td>0.957667</td>\n",
       "      <td>0.959924</td>\n",
       "      <td>0.965113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>46</td>\n",
       "      <td>0.947711</td>\n",
       "      <td>0.946926</td>\n",
       "      <td>0.949602</td>\n",
       "      <td>0.951064</td>\n",
       "      <td>0.947796</td>\n",
       "      <td>0.948620</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>7.134660</td>\n",
       "      <td>0.172411</td>\n",
       "      <td>0.272900</td>\n",
       "      <td>0.023748</td>\n",
       "      <td>balanced</td>\n",
       "      <td>472</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 472}</td>\n",
       "      <td>0.959280</td>\n",
       "      <td>0.960809</td>\n",
       "      <td>0.965858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>47</td>\n",
       "      <td>0.946139</td>\n",
       "      <td>0.947040</td>\n",
       "      <td>0.947418</td>\n",
       "      <td>0.948957</td>\n",
       "      <td>0.949969</td>\n",
       "      <td>0.947905</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5.370031</td>\n",
       "      <td>0.195425</td>\n",
       "      <td>0.222596</td>\n",
       "      <td>0.015009</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>422</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...</td>\n",
       "      <td>0.958150</td>\n",
       "      <td>0.960697</td>\n",
       "      <td>0.967177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>48</td>\n",
       "      <td>0.946534</td>\n",
       "      <td>0.945562</td>\n",
       "      <td>0.949327</td>\n",
       "      <td>0.949174</td>\n",
       "      <td>0.946493</td>\n",
       "      <td>0.947418</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.121574</td>\n",
       "      <td>0.077818</td>\n",
       "      <td>0.208981</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>418</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...</td>\n",
       "      <td>0.958438</td>\n",
       "      <td>0.961282</td>\n",
       "      <td>0.967474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003058</td>\n",
       "      <td>49</td>\n",
       "      <td>0.946517</td>\n",
       "      <td>0.945528</td>\n",
       "      <td>0.949312</td>\n",
       "      <td>0.949158</td>\n",
       "      <td>0.946345</td>\n",
       "      <td>0.947372</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4.758875</td>\n",
       "      <td>0.143656</td>\n",
       "      <td>0.194569</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>377</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...</td>\n",
       "      <td>0.960114</td>\n",
       "      <td>0.961122</td>\n",
       "      <td>0.968910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>50</td>\n",
       "      <td>0.945596</td>\n",
       "      <td>0.943384</td>\n",
       "      <td>0.948243</td>\n",
       "      <td>0.947632</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.946160</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5.062231</td>\n",
       "      <td>0.613765</td>\n",
       "      <td>0.159859</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>278</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...</td>\n",
       "      <td>0.964022</td>\n",
       "      <td>0.963707</td>\n",
       "      <td>0.971363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003967</td>\n",
       "      <td>51</td>\n",
       "      <td>0.942377</td>\n",
       "      <td>0.937874</td>\n",
       "      <td>0.943277</td>\n",
       "      <td>0.944671</td>\n",
       "      <td>0.941735</td>\n",
       "      <td>0.941987</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2.329757</td>\n",
       "      <td>0.252141</td>\n",
       "      <td>0.060961</td>\n",
       "      <td>0.010334</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>74</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.969915</td>\n",
       "      <td>0.966428</td>\n",
       "      <td>0.974834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005818</td>\n",
       "      <td>53</td>\n",
       "      <td>0.939500</td>\n",
       "      <td>0.937698</td>\n",
       "      <td>0.938177</td>\n",
       "      <td>0.945810</td>\n",
       "      <td>0.940877</td>\n",
       "      <td>0.940412</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4.307721</td>\n",
       "      <td>0.489038</td>\n",
       "      <td>0.156253</td>\n",
       "      <td>0.023217</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>241</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...</td>\n",
       "      <td>0.965395</td>\n",
       "      <td>0.965101</td>\n",
       "      <td>0.973079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>52</td>\n",
       "      <td>0.940454</td>\n",
       "      <td>0.936096</td>\n",
       "      <td>0.940654</td>\n",
       "      <td>0.943141</td>\n",
       "      <td>0.940915</td>\n",
       "      <td>0.940252</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4.523507</td>\n",
       "      <td>0.284664</td>\n",
       "      <td>0.150213</td>\n",
       "      <td>0.008276</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>224</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...</td>\n",
       "      <td>0.966515</td>\n",
       "      <td>0.964941</td>\n",
       "      <td>0.973589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>54</td>\n",
       "      <td>0.939040</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.938425</td>\n",
       "      <td>0.942754</td>\n",
       "      <td>0.938724</td>\n",
       "      <td>0.938535</td>\n",
       "      <td>0.002868</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4.867814</td>\n",
       "      <td>0.738449</td>\n",
       "      <td>0.150036</td>\n",
       "      <td>0.013374</td>\n",
       "      <td>balanced</td>\n",
       "      <td>212</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 212}</td>\n",
       "      <td>0.967058</td>\n",
       "      <td>0.967181</td>\n",
       "      <td>0.973395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005511</td>\n",
       "      <td>55</td>\n",
       "      <td>0.937994</td>\n",
       "      <td>0.935110</td>\n",
       "      <td>0.934696</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.936523</td>\n",
       "      <td>0.937100</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2.832277</td>\n",
       "      <td>0.091758</td>\n",
       "      <td>0.107563</td>\n",
       "      <td>0.015550</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>151</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...</td>\n",
       "      <td>0.971372</td>\n",
       "      <td>0.967602</td>\n",
       "      <td>0.977770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>56</td>\n",
       "      <td>0.932268</td>\n",
       "      <td>0.927040</td>\n",
       "      <td>0.932691</td>\n",
       "      <td>0.936008</td>\n",
       "      <td>0.931084</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3.321998</td>\n",
       "      <td>0.125802</td>\n",
       "      <td>0.099018</td>\n",
       "      <td>0.012292</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>150</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...</td>\n",
       "      <td>0.971362</td>\n",
       "      <td>0.967602</td>\n",
       "      <td>0.977433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>57</td>\n",
       "      <td>0.932098</td>\n",
       "      <td>0.927040</td>\n",
       "      <td>0.932369</td>\n",
       "      <td>0.936326</td>\n",
       "      <td>0.931062</td>\n",
       "      <td>0.931779</td>\n",
       "      <td>0.002970</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2.748065</td>\n",
       "      <td>0.082083</td>\n",
       "      <td>0.091123</td>\n",
       "      <td>0.011866</td>\n",
       "      <td>balanced</td>\n",
       "      <td>140</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 140}</td>\n",
       "      <td>0.973163</td>\n",
       "      <td>0.969076</td>\n",
       "      <td>0.977265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006408</td>\n",
       "      <td>58</td>\n",
       "      <td>0.930255</td>\n",
       "      <td>0.925373</td>\n",
       "      <td>0.928583</td>\n",
       "      <td>0.934970</td>\n",
       "      <td>0.927892</td>\n",
       "      <td>0.929414</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2.836820</td>\n",
       "      <td>0.181259</td>\n",
       "      <td>0.067940</td>\n",
       "      <td>0.012788</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>75</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...</td>\n",
       "      <td>0.977755</td>\n",
       "      <td>0.974306</td>\n",
       "      <td>0.979649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006008</td>\n",
       "      <td>59</td>\n",
       "      <td>0.917332</td>\n",
       "      <td>0.914602</td>\n",
       "      <td>0.914510</td>\n",
       "      <td>0.923700</td>\n",
       "      <td>0.918787</td>\n",
       "      <td>0.917786</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2.286945</td>\n",
       "      <td>0.076688</td>\n",
       "      <td>0.056235</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>59</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...</td>\n",
       "      <td>0.979599</td>\n",
       "      <td>0.975149</td>\n",
       "      <td>0.979837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>60</td>\n",
       "      <td>0.913264</td>\n",
       "      <td>0.911500</td>\n",
       "      <td>0.910735</td>\n",
       "      <td>0.921549</td>\n",
       "      <td>0.915726</td>\n",
       "      <td>0.914555</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        2.699475      0.180762         0.066927        0.013200   \n",
       "1        3.416172      0.161599         0.104414        0.009275   \n",
       "2        4.097562      0.174045         0.149262        0.007325   \n",
       "3        5.776887      0.644607         0.190811        0.018065   \n",
       "4        4.381330      0.234753         0.150377        0.017984   \n",
       "5        5.607064      0.232923         0.195960        0.013245   \n",
       "6        3.753953      0.175939         0.123820        0.002306   \n",
       "7        3.087639      0.136404         0.093727        0.009881   \n",
       "8        5.759002      0.395646         0.216329        0.013375   \n",
       "9        7.927969      0.259288         0.291329        0.030403   \n",
       "10      10.029583      0.378627         0.393761        0.072249   \n",
       "11      12.388265      4.117636         0.393725        0.078679   \n",
       "12       7.550388      0.300657         0.258309        0.015854   \n",
       "13       7.148095      0.331638         0.271077        0.012973   \n",
       "14       8.103303      0.720893         0.355317        0.049041   \n",
       "15       6.993068      0.250673         0.254256        0.003926   \n",
       "16       7.661269      0.259241         0.368911        0.047452   \n",
       "17       6.440078      0.196475         0.287438        0.007635   \n",
       "18       8.880516      0.294736         0.387970        0.012140   \n",
       "19       8.058729      0.215926         0.369865        0.007464   \n",
       "20      10.431266      0.491859         0.427991        0.037751   \n",
       "21       9.058358      0.142582         0.423618        0.016537   \n",
       "22       9.506170      0.310538         0.447899        0.041781   \n",
       "23      11.010930      1.768648         0.510778        0.065188   \n",
       "24      10.255647      0.571337         0.481648        0.059350   \n",
       "25      10.004831      0.450572         0.454781        0.008287   \n",
       "26      30.128286     36.270933         0.537506        0.179703   \n",
       "27       6.876095      0.261830         0.313975        0.015616   \n",
       "28       8.256545      0.602532         0.362524        0.010433   \n",
       "29       6.367509      0.185909         0.268694        0.006247   \n",
       "30       5.668172      0.297146         0.239958        0.030111   \n",
       "31       6.336304      0.269833         0.244869        0.016218   \n",
       "32       4.842337      0.122179         0.192951        0.006845   \n",
       "33       5.536524      0.583180         0.213571        0.042581   \n",
       "34      12.240656      0.681623         0.491045        0.026456   \n",
       "35       4.685159      0.477053         0.156808        0.014913   \n",
       "36       8.556572      0.214952         0.391532        0.009964   \n",
       "37       9.178321      0.548067         0.373143        0.032954   \n",
       "38       8.224736      0.667928         0.382321        0.051739   \n",
       "39       3.540189      0.247366         0.129443        0.006163   \n",
       "40       7.707880      1.076960         0.312287        0.022122   \n",
       "41       7.763043      0.903713         0.340966        0.025013   \n",
       "42       6.478772      0.243360         0.297442        0.026167   \n",
       "43       7.437928      0.900784         0.299337        0.012187   \n",
       "44       6.749089      0.373555         0.285617        0.011030   \n",
       "45       7.169069      1.051897         0.279301        0.027196   \n",
       "46       7.134660      0.172411         0.272900        0.023748   \n",
       "47       5.370031      0.195425         0.222596        0.015009   \n",
       "48       5.121574      0.077818         0.208981        0.007250   \n",
       "49       4.758875      0.143656         0.194569        0.007117   \n",
       "50       5.062231      0.613765         0.159859        0.006070   \n",
       "51       2.329757      0.252141         0.060961        0.010334   \n",
       "52       4.307721      0.489038         0.156253        0.023217   \n",
       "53       4.523507      0.284664         0.150213        0.008276   \n",
       "54       4.867814      0.738449         0.150036        0.013374   \n",
       "55       2.832277      0.091758         0.107563        0.015550   \n",
       "56       3.321998      0.125802         0.099018        0.012292   \n",
       "57       2.748065      0.082083         0.091123        0.011866   \n",
       "58       2.836820      0.181259         0.067940        0.012788   \n",
       "59       2.286945      0.076688         0.056235        0.007652   \n",
       "\n",
       "   param_class_weight param_n_estimators  \\\n",
       "0    {0: 1.0, 1: 1.0}                 76   \n",
       "1    {0: 1.0, 1: 1.0}                144   \n",
       "2    {0: 1.0, 1: 1.0}                250   \n",
       "3    {0: 1.0, 1: 1.0}                334   \n",
       "4    {0: 1.0, 1: 1.0}                200   \n",
       "5    {0: 1.0, 1: 1.0}                321   \n",
       "6    {0: 1.0, 1: 1.0}                167   \n",
       "7    {0: 1.0, 1: 1.0}                124   \n",
       "8    {0: 1.0, 1: 1.0}                397   \n",
       "9    {0: 1.0, 1: 1.0}                506   \n",
       "10   {0: 1.0, 1: 1.0}                715   \n",
       "11   {0: 1.0, 1: 1.0}                587   \n",
       "12   {0: 1.0, 1: 1.0}                485   \n",
       "13   {0: 1.0, 1: 1.0}                495   \n",
       "14   {0: 1.0, 1: 1.0}                566   \n",
       "15   {0: 1.0, 1: 1.0}                473   \n",
       "16   {0: 1.0, 1: 1.0}                684   \n",
       "17   {0: 1.0, 1: 1.0}                565   \n",
       "18   {0: 1.0, 1: 1.0}                802   \n",
       "19   {0: 1.0, 1: 1.0}                778   \n",
       "20   {0: 1.0, 1: 1.0}                778   \n",
       "21   {0: 1.0, 1: 1.0}                919   \n",
       "22   {0: 1.0, 1: 1.0}                932   \n",
       "23   {0: 1.0, 1: 1.0}                972   \n",
       "24   {0: 5.0, 1: 1.0}                920   \n",
       "25   {0: 5.0, 1: 1.0}                988   \n",
       "26   {0: 5.0, 1: 1.0}                918   \n",
       "27   {0: 5.0, 1: 1.0}                644   \n",
       "28   {0: 5.0, 1: 1.0}                746   \n",
       "29   {0: 5.0, 1: 1.0}                561   \n",
       "30   {0: 5.0, 1: 1.0}                448   \n",
       "31   {0: 5.0, 1: 1.0}                436   \n",
       "32   {0: 5.0, 1: 1.0}                369   \n",
       "33   {0: 5.0, 1: 1.0}                346   \n",
       "34           balanced                997   \n",
       "35   {0: 5.0, 1: 1.0}                256   \n",
       "36  {0: 10.0, 1: 1.0}                855   \n",
       "37           balanced                765   \n",
       "38           balanced                725   \n",
       "39   {0: 5.0, 1: 1.0}                221   \n",
       "40           balanced                592   \n",
       "41           balanced                579   \n",
       "42  {0: 10.0, 1: 1.0}                589   \n",
       "43  {0: 10.0, 1: 1.0}                609   \n",
       "44  {0: 10.0, 1: 1.0}                584   \n",
       "45  {0: 10.0, 1: 1.0}                495   \n",
       "46           balanced                472   \n",
       "47  {0: 10.0, 1: 1.0}                422   \n",
       "48  {0: 10.0, 1: 1.0}                418   \n",
       "49  {0: 10.0, 1: 1.0}                377   \n",
       "50  {0: 10.0, 1: 1.0}                278   \n",
       "51   {0: 5.0, 1: 1.0}                 74   \n",
       "52  {0: 10.0, 1: 1.0}                241   \n",
       "53  {0: 10.0, 1: 1.0}                224   \n",
       "54           balanced                212   \n",
       "55  {0: 10.0, 1: 1.0}                151   \n",
       "56  {0: 10.0, 1: 1.0}                150   \n",
       "57           balanced                140   \n",
       "58  {0: 10.0, 1: 1.0}                 75   \n",
       "59  {0: 10.0, 1: 1.0}                 59   \n",
       "\n",
       "                                               params  split0_test_precision  \\\n",
       "0   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.942122   \n",
       "1   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.943225   \n",
       "2   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.946240   \n",
       "3   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.947012   \n",
       "4   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.944591   \n",
       "5   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.947291   \n",
       "6   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.944070   \n",
       "7   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.942088   \n",
       "8   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.947322   \n",
       "9   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.948098   \n",
       "10  {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.948627   \n",
       "11  {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.948128   \n",
       "12  {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.947803   \n",
       "13  {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.947818   \n",
       "14  {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.948128   \n",
       "15  {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.947803   \n",
       "16  {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.948408   \n",
       "17  {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.948128   \n",
       "18  {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.948892   \n",
       "19  {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.948332   \n",
       "20  {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.948332   \n",
       "21  {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.948083   \n",
       "22  {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.948052   \n",
       "23  {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...               0.948052   \n",
       "24  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.952453   \n",
       "25  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.952208   \n",
       "26  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.952150   \n",
       "27  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.954878   \n",
       "28  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.953206   \n",
       "29  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.954991   \n",
       "30  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.956240   \n",
       "31  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.955919   \n",
       "32  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.957440   \n",
       "33  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.957996   \n",
       "34  {'class_weight': 'balanced', 'n_estimators': 997}               0.954012   \n",
       "35  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.959523   \n",
       "36  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...               0.954376   \n",
       "37  {'class_weight': 'balanced', 'n_estimators': 765}               0.955549   \n",
       "38  {'class_weight': 'balanced', 'n_estimators': 725}               0.955791   \n",
       "39  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.960655   \n",
       "40  {'class_weight': 'balanced', 'n_estimators': 592}               0.957614   \n",
       "41  {'class_weight': 'balanced', 'n_estimators': 579}               0.957862   \n",
       "42  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...               0.956020   \n",
       "43  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...               0.956277   \n",
       "44  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...               0.956305   \n",
       "45  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...               0.957667   \n",
       "46  {'class_weight': 'balanced', 'n_estimators': 472}               0.959280   \n",
       "47  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...               0.958150   \n",
       "48  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...               0.958438   \n",
       "49  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...               0.960114   \n",
       "50  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...               0.964022   \n",
       "51  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.969915   \n",
       "52  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...               0.965395   \n",
       "53  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...               0.966515   \n",
       "54  {'class_weight': 'balanced', 'n_estimators': 212}               0.967058   \n",
       "55  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...               0.971372   \n",
       "56  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...               0.971362   \n",
       "57  {'class_weight': 'balanced', 'n_estimators': 140}               0.973163   \n",
       "58  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...               0.977755   \n",
       "59  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimat...               0.979599   \n",
       "\n",
       "    split1_test_precision  split2_test_precision  ...  std_test_recall  \\\n",
       "0                0.942999               0.947816  ...         0.001204   \n",
       "1                0.944282               0.950457  ...         0.000595   \n",
       "2                0.946809               0.950931  ...         0.001883   \n",
       "3                0.947400               0.950650  ...         0.001268   \n",
       "4                0.946887               0.950723  ...         0.001487   \n",
       "5                0.947664               0.950917  ...         0.001309   \n",
       "6                0.945851               0.950442  ...         0.001109   \n",
       "7                0.944282               0.949367  ...         0.000791   \n",
       "8                0.947322               0.951422  ...         0.001765   \n",
       "9                0.947852               0.951422  ...         0.001844   \n",
       "10               0.946979               0.951479  ...         0.002137   \n",
       "11               0.947290               0.950902  ...         0.002114   \n",
       "12               0.947587               0.950858  ...         0.001631   \n",
       "13               0.947571               0.951422  ...         0.001844   \n",
       "14               0.947290               0.951479  ...         0.002283   \n",
       "15               0.947587               0.951126  ...         0.001538   \n",
       "16               0.947275               0.950592  ...         0.002134   \n",
       "17               0.947290               0.950902  ...         0.002219   \n",
       "18               0.946106               0.951183  ...         0.002401   \n",
       "19               0.946651               0.951155  ...         0.002246   \n",
       "20               0.946651               0.951155  ...         0.002246   \n",
       "21               0.946386               0.950015  ...         0.002294   \n",
       "22               0.946090               0.949719  ...         0.002159   \n",
       "23               0.946074               0.950577  ...         0.002556   \n",
       "24               0.953893               0.957898  ...         0.003504   \n",
       "25               0.953077               0.957552  ...         0.003987   \n",
       "26               0.953893               0.957898  ...         0.003372   \n",
       "27               0.955118               0.959222  ...         0.004031   \n",
       "28               0.953474               0.958565  ...         0.004815   \n",
       "29               0.956535               0.960285  ...         0.003566   \n",
       "30               0.958879               0.961970  ...         0.003536   \n",
       "31               0.958541               0.963183  ...         0.003668   \n",
       "32               0.959688               0.965149  ...         0.003190   \n",
       "33               0.959075               0.965745  ...         0.002780   \n",
       "34               0.954447               0.958489  ...         0.003112   \n",
       "35               0.959862               0.967947  ...         0.002587   \n",
       "36               0.956157               0.959101  ...         0.002415   \n",
       "37               0.956888               0.959574  ...         0.003253   \n",
       "38               0.957733               0.959824  ...         0.003402   \n",
       "39               0.961478               0.967845  ...         0.003130   \n",
       "40               0.960440               0.961502  ...         0.003120   \n",
       "41               0.959824               0.961781  ...         0.003241   \n",
       "42               0.957627               0.961296  ...         0.002545   \n",
       "43               0.957654               0.960655  ...         0.002878   \n",
       "44               0.957300               0.961587  ...         0.002647   \n",
       "45               0.959924               0.965113  ...         0.003019   \n",
       "46               0.960809               0.965858  ...         0.002149   \n",
       "47               0.960697               0.967177  ...         0.003137   \n",
       "48               0.961282               0.967474  ...         0.003058   \n",
       "49               0.961122               0.968910  ...         0.002601   \n",
       "50               0.963707               0.971363  ...         0.003967   \n",
       "51               0.966428               0.974834  ...         0.005818   \n",
       "52               0.965101               0.973079  ...         0.004264   \n",
       "53               0.964941               0.973589  ...         0.005232   \n",
       "54               0.967181               0.973395  ...         0.005511   \n",
       "55               0.967602               0.977770  ...         0.004944   \n",
       "56               0.967602               0.977433  ...         0.005111   \n",
       "57               0.969076               0.977265  ...         0.006408   \n",
       "58               0.974306               0.979649  ...         0.006008   \n",
       "59               0.975149               0.979837  ...         0.006314   \n",
       "\n",
       "    rank_test_recall  split0_test_f1  split1_test_f1  split2_test_f1  \\\n",
       "0                  1        0.965404        0.966302        0.969852   \n",
       "1                  3        0.965982        0.966097        0.969761   \n",
       "2                  6        0.967267        0.965060        0.969122   \n",
       "3                  7        0.967083        0.965663        0.968976   \n",
       "4                  5        0.966552        0.965839        0.969752   \n",
       "5                  8        0.967228        0.965652        0.968967   \n",
       "6                  4        0.966572        0.966035        0.969606   \n",
       "7                  2        0.965094        0.966097        0.969487   \n",
       "8                  9        0.967538        0.964883        0.968637   \n",
       "9                 12        0.967354        0.964862        0.968637   \n",
       "10                18        0.967334        0.964113        0.969259   \n",
       "11                11        0.967664        0.964571        0.968811   \n",
       "12                10        0.967053        0.964873        0.968345   \n",
       "13                12        0.967208        0.964717        0.968637   \n",
       "14                14        0.967664        0.964571        0.969259   \n",
       "15                16        0.967053        0.964873        0.968335   \n",
       "16                17        0.967810        0.964415        0.968354   \n",
       "17                15        0.967664        0.964571        0.968811   \n",
       "18                20        0.967324        0.963365        0.968957   \n",
       "19                21        0.967033        0.963499        0.968646   \n",
       "20                21        0.967033        0.963499        0.968646   \n",
       "21                19        0.967198        0.963510        0.967907   \n",
       "22                23        0.966887        0.963209        0.967606   \n",
       "23                24        0.966887        0.963052        0.968199   \n",
       "24                26        0.959134        0.956668        0.957604   \n",
       "25                25        0.959464        0.956868        0.956816   \n",
       "26                27        0.958829        0.956668        0.957604   \n",
       "27                29        0.958384        0.954678        0.956415   \n",
       "28                28        0.958302        0.955082        0.955316   \n",
       "29                30        0.956455        0.954769        0.955394   \n",
       "30                31        0.954622        0.952057        0.954827   \n",
       "31                32        0.954154        0.951423        0.955580   \n",
       "32                34        0.951969        0.951363        0.954510   \n",
       "33                35        0.951777        0.950906        0.954644   \n",
       "34                33        0.951663        0.950177        0.950774   \n",
       "35                39        0.949247        0.949884        0.952084   \n",
       "36                36        0.949367        0.950247        0.951231   \n",
       "37                37        0.949791        0.948738        0.949899   \n",
       "38                38        0.949289        0.948527        0.949395   \n",
       "39                45        0.948857        0.948155        0.950446   \n",
       "40                42        0.947058        0.949853        0.948482   \n",
       "41                44        0.946551        0.949395        0.948303   \n",
       "42                41        0.948623        0.947221        0.949643   \n",
       "43                40        0.948281        0.947548        0.948857   \n",
       "44                43        0.948607        0.946747        0.949627   \n",
       "45                46        0.947711        0.946926        0.949602   \n",
       "46                47        0.946139        0.947040        0.947418   \n",
       "47                48        0.946534        0.945562        0.949327   \n",
       "48                49        0.946517        0.945528        0.949312   \n",
       "49                50        0.945596        0.943384        0.948243   \n",
       "50                51        0.942377        0.937874        0.943277   \n",
       "51                53        0.939500        0.937698        0.938177   \n",
       "52                52        0.940454        0.936096        0.940654   \n",
       "53                54        0.939040        0.933735        0.938425   \n",
       "54                55        0.937994        0.935110        0.934696   \n",
       "55                56        0.932268        0.927040        0.932691   \n",
       "56                57        0.932098        0.927040        0.932369   \n",
       "57                58        0.930255        0.925373        0.928583   \n",
       "58                59        0.917332        0.914602        0.914510   \n",
       "59                60        0.913264        0.911500        0.910735   \n",
       "\n",
       "    split3_test_f1  split4_test_f1  mean_test_f1  std_test_f1  rank_test_f1  \n",
       "0         0.966951        0.967626      0.967227     0.001504             1  \n",
       "1         0.965559        0.967887      0.967057     0.001570             2  \n",
       "2         0.966231        0.967383      0.967013     0.001347             3  \n",
       "3         0.965870        0.967198      0.966958     0.001184             4  \n",
       "4         0.965217        0.967393      0.966951     0.001577             5  \n",
       "5         0.965725        0.966897      0.966894     0.001210             6  \n",
       "6         0.964783        0.967422      0.966884     0.001608             7  \n",
       "7         0.965724        0.967742      0.966829     0.001592             8  \n",
       "8         0.965704        0.966867      0.966726     0.001325             9  \n",
       "9         0.965860        0.966204      0.966583     0.001299            10  \n",
       "10        0.965237        0.966631      0.966515     0.001767            11  \n",
       "11        0.965092        0.966350      0.966498     0.001576            12  \n",
       "12        0.965714        0.966360      0.966469     0.001182            13  \n",
       "13        0.965714        0.966058      0.966467     0.001346            14  \n",
       "14        0.964801        0.966038      0.966467     0.001776            15  \n",
       "15        0.965548        0.966214      0.966405     0.001205            16  \n",
       "16        0.965102        0.966340      0.966404     0.001512            17  \n",
       "17        0.964801        0.966038      0.966377     0.001640            18  \n",
       "18        0.965247        0.965715      0.966122     0.001898            19  \n",
       "19        0.965247        0.966027      0.966091     0.001723            20  \n",
       "20        0.965247        0.966027      0.966091     0.001723            20  \n",
       "21        0.965403        0.966017      0.966007     0.001525            22  \n",
       "22        0.965258        0.966465      0.965885     0.001540            23  \n",
       "23        0.965247        0.965538      0.965785     0.001724            24  \n",
       "24        0.958854        0.959571      0.958366     0.001072            25  \n",
       "25        0.959159        0.959437      0.958348     0.001235            26  \n",
       "26        0.958695        0.959571      0.958273     0.001019            27  \n",
       "27        0.958607        0.959262      0.957469     0.001688            28  \n",
       "28        0.959561        0.959030      0.957458     0.001889            29  \n",
       "29        0.957893        0.957794      0.956461     0.001251            30  \n",
       "30        0.957070        0.957032      0.955122     0.001854            31  \n",
       "31        0.957218        0.956697      0.955014     0.002080            32  \n",
       "32        0.956052        0.956616      0.954102     0.002114            33  \n",
       "33        0.955851        0.955645      0.953765     0.002039            34  \n",
       "34        0.953274        0.954046      0.951987     0.001466            35  \n",
       "35        0.953651        0.952826      0.951538     0.001697            36  \n",
       "36        0.953026        0.952528      0.951280     0.001366            37  \n",
       "37        0.953130        0.952912      0.950894     0.001785            38  \n",
       "38        0.953381        0.952588      0.950636     0.001957            39  \n",
       "39        0.952824        0.952217      0.950500     0.001819            40  \n",
       "40        0.952455        0.952588      0.950087     0.002176            41  \n",
       "41        0.952292        0.951907      0.949689     0.002170            42  \n",
       "42        0.951408        0.950825      0.949544     0.001508            43  \n",
       "43        0.951910        0.951019      0.949523     0.001662            44  \n",
       "44        0.951540        0.950498      0.949404     0.001643            45  \n",
       "45        0.951064        0.947796      0.948620     0.001504            46  \n",
       "46        0.948957        0.949969      0.947905     0.001376            47  \n",
       "47        0.949174        0.946493      0.947418     0.001537            48  \n",
       "48        0.949158        0.946345      0.947372     0.001558            49  \n",
       "49        0.947632        0.945946      0.946160     0.001707            50  \n",
       "50        0.944671        0.941735      0.941987     0.002280            51  \n",
       "51        0.945810        0.940877      0.940412     0.002917            52  \n",
       "52        0.943141        0.940915      0.940252     0.002292            53  \n",
       "53        0.942754        0.938724      0.938535     0.002868            54  \n",
       "54        0.941176        0.936523      0.937100     0.002345            55  \n",
       "55        0.936008        0.931084      0.931818     0.002894            56  \n",
       "56        0.936326        0.931062      0.931779     0.002970            57  \n",
       "57        0.934970        0.927892      0.929414     0.003191            58  \n",
       "58        0.923700        0.918787      0.917786     0.003378            59  \n",
       "59        0.921549        0.915726      0.914555     0.003896            60  \n",
       "\n",
       "[60 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>2.699475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.180762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.066927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.0132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_class_weight</th>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_n_estimators</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.942122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.942999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.947816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_precision</th>\n",
       "      <td>0.942291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_precision</th>\n",
       "      <td>0.944688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.943983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.00212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.989865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.990786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.992936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_recall</th>\n",
       "      <td>0.992936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_recall</th>\n",
       "      <td>0.991705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.991646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.001204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.965404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.966302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.969852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_f1</th>\n",
       "      <td>0.966951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_f1</th>\n",
       "      <td>0.967626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.967227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.001504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       0\n",
       "mean_fit_time                                                   2.699475\n",
       "std_fit_time                                                    0.180762\n",
       "mean_score_time                                                 0.066927\n",
       "std_score_time                                                    0.0132\n",
       "param_class_weight                                      {0: 1.0, 1: 1.0}\n",
       "param_n_estimators                                                    76\n",
       "params                 {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimato...\n",
       "split0_test_precision                                           0.942122\n",
       "split1_test_precision                                           0.942999\n",
       "split2_test_precision                                           0.947816\n",
       "split3_test_precision                                           0.942291\n",
       "split4_test_precision                                           0.944688\n",
       "mean_test_precision                                             0.943983\n",
       "std_test_precision                                               0.00212\n",
       "rank_test_precision                                                   60\n",
       "split0_test_recall                                              0.989865\n",
       "split1_test_recall                                              0.990786\n",
       "split2_test_recall                                              0.992936\n",
       "split3_test_recall                                              0.992936\n",
       "split4_test_recall                                              0.991705\n",
       "mean_test_recall                                                0.991646\n",
       "std_test_recall                                                 0.001204\n",
       "rank_test_recall                                                       1\n",
       "split0_test_f1                                                  0.965404\n",
       "split1_test_f1                                                  0.966302\n",
       "split2_test_f1                                                  0.969852\n",
       "split3_test_f1                                                  0.966951\n",
       "split4_test_f1                                                  0.967626\n",
       "mean_test_f1                                                    0.967227\n",
       "std_test_f1                                                     0.001504\n",
       "rank_test_f1                                                           1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                          0\n",
      "params               {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 76}\n",
      "mean_test_precision                                                0.943983\n",
      "mean_test_recall                                                   0.991646\n",
      "mean_test_f1                                                       0.967227\n"
     ]
    }
   ],
   "source": [
    "lgbm = lgb.LGBMClassifier(objective='binary')\n",
    "\n",
    "lgb_cv = RandomizedSearchCV(lgbm, {'class_weight':['balanced', {0: 1.0, 1: 1.0}, {0: 5.0, 1: 1.0}, {0: 1.0, 1: 1.0}, {0: 10.0, 1: 1.0}],\n",
    "                            'n_estimators': randint(50,1000)},\\\n",
    "                            n_iter=60, cv = 5, verbose=3, \\\n",
    "                            scoring = ['precision','recall','f1'], refit=False)\n",
    "\n",
    "lgb_cv.fit(X_tr_vec, y_tr)   \n",
    "\n",
    "lgbm_cvdf = pd.DataFrame(lgb_cv.cv_results_).sort_values(by='rank_test_f1').reset_index(drop=True)\n",
    "lgbm_best = pd.DataFrame(lgbm_cvdf.iloc[0,:])\n",
    "display(lgbm_cvdf)\n",
    "display(lgbm_best)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(lgbm_best.loc[['params', 'mean_test_precision','mean_test_recall','mean_test_f1'],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da3e917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_precision</th>\n",
       "      <th>split1_test_precision</th>\n",
       "      <th>split2_test_precision</th>\n",
       "      <th>split3_test_precision</th>\n",
       "      <th>split4_test_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>rank_test_precision</th>\n",
       "      <th>split0_test_recall</th>\n",
       "      <th>split1_test_recall</th>\n",
       "      <th>split2_test_recall</th>\n",
       "      <th>split3_test_recall</th>\n",
       "      <th>split4_test_recall</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>rank_test_recall</th>\n",
       "      <th>split0_test_f1</th>\n",
       "      <th>split1_test_f1</th>\n",
       "      <th>split2_test_f1</th>\n",
       "      <th>split3_test_f1</th>\n",
       "      <th>split4_test_f1</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.699475</td>\n",
       "      <td>0.180762</td>\n",
       "      <td>0.066927</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>76</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 76}</td>\n",
       "      <td>0.942122</td>\n",
       "      <td>0.942999</td>\n",
       "      <td>0.947816</td>\n",
       "      <td>0.942291</td>\n",
       "      <td>0.944688</td>\n",
       "      <td>0.943983</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>60</td>\n",
       "      <td>0.989865</td>\n",
       "      <td>0.990786</td>\n",
       "      <td>0.992936</td>\n",
       "      <td>0.992936</td>\n",
       "      <td>0.991705</td>\n",
       "      <td>0.991646</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965404</td>\n",
       "      <td>0.966302</td>\n",
       "      <td>0.969852</td>\n",
       "      <td>0.966951</td>\n",
       "      <td>0.967626</td>\n",
       "      <td>0.967227</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.416172</td>\n",
       "      <td>0.161599</td>\n",
       "      <td>0.104414</td>\n",
       "      <td>0.009275</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>144</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 144}</td>\n",
       "      <td>0.943225</td>\n",
       "      <td>0.944282</td>\n",
       "      <td>0.950457</td>\n",
       "      <td>0.942139</td>\n",
       "      <td>0.946025</td>\n",
       "      <td>0.945226</td>\n",
       "      <td>0.002913</td>\n",
       "      <td>58</td>\n",
       "      <td>0.989865</td>\n",
       "      <td>0.988943</td>\n",
       "      <td>0.989865</td>\n",
       "      <td>0.990172</td>\n",
       "      <td>0.990783</td>\n",
       "      <td>0.989926</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>3</td>\n",
       "      <td>0.965982</td>\n",
       "      <td>0.966097</td>\n",
       "      <td>0.969761</td>\n",
       "      <td>0.965559</td>\n",
       "      <td>0.967887</td>\n",
       "      <td>0.967057</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.097562</td>\n",
       "      <td>0.174045</td>\n",
       "      <td>0.149262</td>\n",
       "      <td>0.007325</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>250</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 250}</td>\n",
       "      <td>0.946240</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.950931</td>\n",
       "      <td>0.944819</td>\n",
       "      <td>0.947028</td>\n",
       "      <td>0.947165</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>55</td>\n",
       "      <td>0.989251</td>\n",
       "      <td>0.984029</td>\n",
       "      <td>0.988022</td>\n",
       "      <td>0.988636</td>\n",
       "      <td>0.988633</td>\n",
       "      <td>0.987714</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>6</td>\n",
       "      <td>0.967267</td>\n",
       "      <td>0.965060</td>\n",
       "      <td>0.969122</td>\n",
       "      <td>0.966231</td>\n",
       "      <td>0.967383</td>\n",
       "      <td>0.967013</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.776887</td>\n",
       "      <td>0.644607</td>\n",
       "      <td>0.190811</td>\n",
       "      <td>0.018065</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>334</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 334}</td>\n",
       "      <td>0.947012</td>\n",
       "      <td>0.947400</td>\n",
       "      <td>0.950650</td>\n",
       "      <td>0.946097</td>\n",
       "      <td>0.947803</td>\n",
       "      <td>0.947792</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>54</td>\n",
       "      <td>0.988022</td>\n",
       "      <td>0.984644</td>\n",
       "      <td>0.988022</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.987404</td>\n",
       "      <td>0.986916</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>7</td>\n",
       "      <td>0.967083</td>\n",
       "      <td>0.965663</td>\n",
       "      <td>0.968976</td>\n",
       "      <td>0.965870</td>\n",
       "      <td>0.967198</td>\n",
       "      <td>0.966958</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.381330</td>\n",
       "      <td>0.234753</td>\n",
       "      <td>0.150377</td>\n",
       "      <td>0.017984</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>200</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 200}</td>\n",
       "      <td>0.944591</td>\n",
       "      <td>0.946887</td>\n",
       "      <td>0.950723</td>\n",
       "      <td>0.942882</td>\n",
       "      <td>0.946765</td>\n",
       "      <td>0.946370</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>56</td>\n",
       "      <td>0.989558</td>\n",
       "      <td>0.985565</td>\n",
       "      <td>0.989558</td>\n",
       "      <td>0.988636</td>\n",
       "      <td>0.988940</td>\n",
       "      <td>0.988451</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>5</td>\n",
       "      <td>0.966552</td>\n",
       "      <td>0.965839</td>\n",
       "      <td>0.969752</td>\n",
       "      <td>0.965217</td>\n",
       "      <td>0.967393</td>\n",
       "      <td>0.966951</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.607064</td>\n",
       "      <td>0.232923</td>\n",
       "      <td>0.195960</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>321</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 321}</td>\n",
       "      <td>0.947291</td>\n",
       "      <td>0.947664</td>\n",
       "      <td>0.950917</td>\n",
       "      <td>0.945819</td>\n",
       "      <td>0.947508</td>\n",
       "      <td>0.947840</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>53</td>\n",
       "      <td>0.988022</td>\n",
       "      <td>0.984337</td>\n",
       "      <td>0.987715</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.987097</td>\n",
       "      <td>0.986731</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>8</td>\n",
       "      <td>0.967228</td>\n",
       "      <td>0.965652</td>\n",
       "      <td>0.968967</td>\n",
       "      <td>0.965725</td>\n",
       "      <td>0.966897</td>\n",
       "      <td>0.966894</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.753953</td>\n",
       "      <td>0.175939</td>\n",
       "      <td>0.123820</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>167</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 167}</td>\n",
       "      <td>0.944070</td>\n",
       "      <td>0.945851</td>\n",
       "      <td>0.950442</td>\n",
       "      <td>0.942054</td>\n",
       "      <td>0.945978</td>\n",
       "      <td>0.945679</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>57</td>\n",
       "      <td>0.990172</td>\n",
       "      <td>0.987101</td>\n",
       "      <td>0.989558</td>\n",
       "      <td>0.988636</td>\n",
       "      <td>0.989862</td>\n",
       "      <td>0.989066</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>4</td>\n",
       "      <td>0.966572</td>\n",
       "      <td>0.966035</td>\n",
       "      <td>0.969606</td>\n",
       "      <td>0.964783</td>\n",
       "      <td>0.967422</td>\n",
       "      <td>0.966884</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.087639</td>\n",
       "      <td>0.136404</td>\n",
       "      <td>0.093727</td>\n",
       "      <td>0.009881</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>124</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 124}</td>\n",
       "      <td>0.942088</td>\n",
       "      <td>0.944282</td>\n",
       "      <td>0.949367</td>\n",
       "      <td>0.941898</td>\n",
       "      <td>0.945748</td>\n",
       "      <td>0.944677</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>59</td>\n",
       "      <td>0.989251</td>\n",
       "      <td>0.988943</td>\n",
       "      <td>0.990479</td>\n",
       "      <td>0.990786</td>\n",
       "      <td>0.990783</td>\n",
       "      <td>0.990049</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>2</td>\n",
       "      <td>0.965094</td>\n",
       "      <td>0.966097</td>\n",
       "      <td>0.969487</td>\n",
       "      <td>0.965724</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.966829</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.759002</td>\n",
       "      <td>0.395646</td>\n",
       "      <td>0.216329</td>\n",
       "      <td>0.013375</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>397</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 397}</td>\n",
       "      <td>0.947322</td>\n",
       "      <td>0.947322</td>\n",
       "      <td>0.951422</td>\n",
       "      <td>0.946344</td>\n",
       "      <td>0.948301</td>\n",
       "      <td>0.948142</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>50</td>\n",
       "      <td>0.988636</td>\n",
       "      <td>0.983108</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.985872</td>\n",
       "      <td>0.986175</td>\n",
       "      <td>0.986056</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>9</td>\n",
       "      <td>0.967538</td>\n",
       "      <td>0.964883</td>\n",
       "      <td>0.968637</td>\n",
       "      <td>0.965704</td>\n",
       "      <td>0.966867</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.927969</td>\n",
       "      <td>0.259288</td>\n",
       "      <td>0.291329</td>\n",
       "      <td>0.030403</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>506</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 506}</td>\n",
       "      <td>0.948098</td>\n",
       "      <td>0.947852</td>\n",
       "      <td>0.951422</td>\n",
       "      <td>0.946360</td>\n",
       "      <td>0.949303</td>\n",
       "      <td>0.948607</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>38</td>\n",
       "      <td>0.987408</td>\n",
       "      <td>0.982494</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.986179</td>\n",
       "      <td>0.983717</td>\n",
       "      <td>0.985257</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>12</td>\n",
       "      <td>0.967354</td>\n",
       "      <td>0.964862</td>\n",
       "      <td>0.968637</td>\n",
       "      <td>0.965860</td>\n",
       "      <td>0.966204</td>\n",
       "      <td>0.966583</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.029583</td>\n",
       "      <td>0.378627</td>\n",
       "      <td>0.393761</td>\n",
       "      <td>0.072249</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>715</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 715}</td>\n",
       "      <td>0.948627</td>\n",
       "      <td>0.946979</td>\n",
       "      <td>0.951479</td>\n",
       "      <td>0.946297</td>\n",
       "      <td>0.950416</td>\n",
       "      <td>0.948760</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>37</td>\n",
       "      <td>0.986794</td>\n",
       "      <td>0.981880</td>\n",
       "      <td>0.987715</td>\n",
       "      <td>0.984951</td>\n",
       "      <td>0.983410</td>\n",
       "      <td>0.984950</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>18</td>\n",
       "      <td>0.967334</td>\n",
       "      <td>0.964113</td>\n",
       "      <td>0.969259</td>\n",
       "      <td>0.965237</td>\n",
       "      <td>0.966631</td>\n",
       "      <td>0.966515</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.388265</td>\n",
       "      <td>4.117636</td>\n",
       "      <td>0.393725</td>\n",
       "      <td>0.078679</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>587</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 587}</td>\n",
       "      <td>0.948128</td>\n",
       "      <td>0.947290</td>\n",
       "      <td>0.950902</td>\n",
       "      <td>0.946018</td>\n",
       "      <td>0.949585</td>\n",
       "      <td>0.948385</td>\n",
       "      <td>0.001711</td>\n",
       "      <td>42</td>\n",
       "      <td>0.988022</td>\n",
       "      <td>0.982494</td>\n",
       "      <td>0.987408</td>\n",
       "      <td>0.984951</td>\n",
       "      <td>0.983717</td>\n",
       "      <td>0.985318</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>11</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>0.964571</td>\n",
       "      <td>0.968811</td>\n",
       "      <td>0.965092</td>\n",
       "      <td>0.966350</td>\n",
       "      <td>0.966498</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.550388</td>\n",
       "      <td>0.300657</td>\n",
       "      <td>0.258309</td>\n",
       "      <td>0.015854</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>485</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 485}</td>\n",
       "      <td>0.947803</td>\n",
       "      <td>0.947587</td>\n",
       "      <td>0.950858</td>\n",
       "      <td>0.946081</td>\n",
       "      <td>0.949318</td>\n",
       "      <td>0.948330</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>47</td>\n",
       "      <td>0.987101</td>\n",
       "      <td>0.982801</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.986179</td>\n",
       "      <td>0.984025</td>\n",
       "      <td>0.985318</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>10</td>\n",
       "      <td>0.967053</td>\n",
       "      <td>0.964873</td>\n",
       "      <td>0.968345</td>\n",
       "      <td>0.965714</td>\n",
       "      <td>0.966360</td>\n",
       "      <td>0.966469</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.148095</td>\n",
       "      <td>0.331638</td>\n",
       "      <td>0.271077</td>\n",
       "      <td>0.012973</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>495</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 495}</td>\n",
       "      <td>0.947818</td>\n",
       "      <td>0.947571</td>\n",
       "      <td>0.951422</td>\n",
       "      <td>0.946081</td>\n",
       "      <td>0.949022</td>\n",
       "      <td>0.948383</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>43</td>\n",
       "      <td>0.987408</td>\n",
       "      <td>0.982494</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.986179</td>\n",
       "      <td>0.983717</td>\n",
       "      <td>0.985257</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>12</td>\n",
       "      <td>0.967208</td>\n",
       "      <td>0.964717</td>\n",
       "      <td>0.968637</td>\n",
       "      <td>0.965714</td>\n",
       "      <td>0.966058</td>\n",
       "      <td>0.966467</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.103303</td>\n",
       "      <td>0.720893</td>\n",
       "      <td>0.355317</td>\n",
       "      <td>0.049041</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>566</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 566}</td>\n",
       "      <td>0.948128</td>\n",
       "      <td>0.947290</td>\n",
       "      <td>0.951479</td>\n",
       "      <td>0.945460</td>\n",
       "      <td>0.949555</td>\n",
       "      <td>0.948383</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>44</td>\n",
       "      <td>0.988022</td>\n",
       "      <td>0.982494</td>\n",
       "      <td>0.987715</td>\n",
       "      <td>0.984951</td>\n",
       "      <td>0.983103</td>\n",
       "      <td>0.985257</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>14</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>0.964571</td>\n",
       "      <td>0.969259</td>\n",
       "      <td>0.964801</td>\n",
       "      <td>0.966038</td>\n",
       "      <td>0.966467</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.993068</td>\n",
       "      <td>0.250673</td>\n",
       "      <td>0.254256</td>\n",
       "      <td>0.003926</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>473</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 473}</td>\n",
       "      <td>0.947803</td>\n",
       "      <td>0.947587</td>\n",
       "      <td>0.951126</td>\n",
       "      <td>0.946329</td>\n",
       "      <td>0.949037</td>\n",
       "      <td>0.948376</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>46</td>\n",
       "      <td>0.987101</td>\n",
       "      <td>0.982801</td>\n",
       "      <td>0.986179</td>\n",
       "      <td>0.985565</td>\n",
       "      <td>0.984025</td>\n",
       "      <td>0.985134</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>16</td>\n",
       "      <td>0.967053</td>\n",
       "      <td>0.964873</td>\n",
       "      <td>0.968335</td>\n",
       "      <td>0.965548</td>\n",
       "      <td>0.966214</td>\n",
       "      <td>0.966405</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.661269</td>\n",
       "      <td>0.259241</td>\n",
       "      <td>0.368911</td>\n",
       "      <td>0.047452</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>684</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 684}</td>\n",
       "      <td>0.948408</td>\n",
       "      <td>0.947275</td>\n",
       "      <td>0.950592</td>\n",
       "      <td>0.945755</td>\n",
       "      <td>0.949852</td>\n",
       "      <td>0.948376</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>45</td>\n",
       "      <td>0.988022</td>\n",
       "      <td>0.982187</td>\n",
       "      <td>0.986794</td>\n",
       "      <td>0.985258</td>\n",
       "      <td>0.983410</td>\n",
       "      <td>0.985134</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>17</td>\n",
       "      <td>0.967810</td>\n",
       "      <td>0.964415</td>\n",
       "      <td>0.968354</td>\n",
       "      <td>0.965102</td>\n",
       "      <td>0.966340</td>\n",
       "      <td>0.966404</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.440078</td>\n",
       "      <td>0.196475</td>\n",
       "      <td>0.287438</td>\n",
       "      <td>0.007635</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>565</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 565}</td>\n",
       "      <td>0.948128</td>\n",
       "      <td>0.947290</td>\n",
       "      <td>0.950902</td>\n",
       "      <td>0.945460</td>\n",
       "      <td>0.949555</td>\n",
       "      <td>0.948267</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>48</td>\n",
       "      <td>0.988022</td>\n",
       "      <td>0.982494</td>\n",
       "      <td>0.987408</td>\n",
       "      <td>0.984951</td>\n",
       "      <td>0.983103</td>\n",
       "      <td>0.985196</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>15</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>0.964571</td>\n",
       "      <td>0.968811</td>\n",
       "      <td>0.964801</td>\n",
       "      <td>0.966038</td>\n",
       "      <td>0.966377</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.880516</td>\n",
       "      <td>0.294736</td>\n",
       "      <td>0.387970</td>\n",
       "      <td>0.012140</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>802</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 802}</td>\n",
       "      <td>0.948892</td>\n",
       "      <td>0.946106</td>\n",
       "      <td>0.951183</td>\n",
       "      <td>0.946034</td>\n",
       "      <td>0.949792</td>\n",
       "      <td>0.948401</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>39</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.981265</td>\n",
       "      <td>0.987408</td>\n",
       "      <td>0.985258</td>\n",
       "      <td>0.982181</td>\n",
       "      <td>0.984520</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>20</td>\n",
       "      <td>0.967324</td>\n",
       "      <td>0.963365</td>\n",
       "      <td>0.968957</td>\n",
       "      <td>0.965247</td>\n",
       "      <td>0.965715</td>\n",
       "      <td>0.966122</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.431266</td>\n",
       "      <td>0.491859</td>\n",
       "      <td>0.427991</td>\n",
       "      <td>0.037751</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>778</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 778}</td>\n",
       "      <td>0.948332</td>\n",
       "      <td>0.946651</td>\n",
       "      <td>0.951155</td>\n",
       "      <td>0.946034</td>\n",
       "      <td>0.949822</td>\n",
       "      <td>0.948399</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>40</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.980958</td>\n",
       "      <td>0.986794</td>\n",
       "      <td>0.985258</td>\n",
       "      <td>0.982796</td>\n",
       "      <td>0.984458</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>21</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.963499</td>\n",
       "      <td>0.968646</td>\n",
       "      <td>0.965247</td>\n",
       "      <td>0.966027</td>\n",
       "      <td>0.966091</td>\n",
       "      <td>0.001723</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.058729</td>\n",
       "      <td>0.215926</td>\n",
       "      <td>0.369865</td>\n",
       "      <td>0.007464</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>778</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 778}</td>\n",
       "      <td>0.948332</td>\n",
       "      <td>0.946651</td>\n",
       "      <td>0.951155</td>\n",
       "      <td>0.946034</td>\n",
       "      <td>0.949822</td>\n",
       "      <td>0.948399</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>40</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.980958</td>\n",
       "      <td>0.986794</td>\n",
       "      <td>0.985258</td>\n",
       "      <td>0.982796</td>\n",
       "      <td>0.984458</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>21</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.963499</td>\n",
       "      <td>0.968646</td>\n",
       "      <td>0.965247</td>\n",
       "      <td>0.966027</td>\n",
       "      <td>0.966091</td>\n",
       "      <td>0.001723</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.058358</td>\n",
       "      <td>0.142582</td>\n",
       "      <td>0.423618</td>\n",
       "      <td>0.016537</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>919</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 919}</td>\n",
       "      <td>0.948083</td>\n",
       "      <td>0.946386</td>\n",
       "      <td>0.950015</td>\n",
       "      <td>0.946050</td>\n",
       "      <td>0.950089</td>\n",
       "      <td>0.948124</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>51</td>\n",
       "      <td>0.987101</td>\n",
       "      <td>0.981265</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.985565</td>\n",
       "      <td>0.982488</td>\n",
       "      <td>0.984581</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>19</td>\n",
       "      <td>0.967198</td>\n",
       "      <td>0.963510</td>\n",
       "      <td>0.967907</td>\n",
       "      <td>0.965403</td>\n",
       "      <td>0.966017</td>\n",
       "      <td>0.966007</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9.506170</td>\n",
       "      <td>0.310538</td>\n",
       "      <td>0.447899</td>\n",
       "      <td>0.041781</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>932</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 932}</td>\n",
       "      <td>0.948052</td>\n",
       "      <td>0.946090</td>\n",
       "      <td>0.949719</td>\n",
       "      <td>0.945771</td>\n",
       "      <td>0.950669</td>\n",
       "      <td>0.948060</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>52</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.980958</td>\n",
       "      <td>0.986179</td>\n",
       "      <td>0.985565</td>\n",
       "      <td>0.982796</td>\n",
       "      <td>0.984397</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>23</td>\n",
       "      <td>0.966887</td>\n",
       "      <td>0.963209</td>\n",
       "      <td>0.967606</td>\n",
       "      <td>0.965258</td>\n",
       "      <td>0.966465</td>\n",
       "      <td>0.965885</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11.010930</td>\n",
       "      <td>1.768648</td>\n",
       "      <td>0.510778</td>\n",
       "      <td>0.065188</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>972</td>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 972}</td>\n",
       "      <td>0.948052</td>\n",
       "      <td>0.946074</td>\n",
       "      <td>0.950577</td>\n",
       "      <td>0.946034</td>\n",
       "      <td>0.950312</td>\n",
       "      <td>0.948210</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>49</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.980651</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.985258</td>\n",
       "      <td>0.981260</td>\n",
       "      <td>0.984028</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>24</td>\n",
       "      <td>0.966887</td>\n",
       "      <td>0.963052</td>\n",
       "      <td>0.968199</td>\n",
       "      <td>0.965247</td>\n",
       "      <td>0.965538</td>\n",
       "      <td>0.965785</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.255647</td>\n",
       "      <td>0.571337</td>\n",
       "      <td>0.481648</td>\n",
       "      <td>0.059350</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>920</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 920}</td>\n",
       "      <td>0.952453</td>\n",
       "      <td>0.953893</td>\n",
       "      <td>0.957898</td>\n",
       "      <td>0.951603</td>\n",
       "      <td>0.956641</td>\n",
       "      <td>0.954498</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>34</td>\n",
       "      <td>0.965909</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.957310</td>\n",
       "      <td>0.966216</td>\n",
       "      <td>0.962519</td>\n",
       "      <td>0.962283</td>\n",
       "      <td>0.003504</td>\n",
       "      <td>26</td>\n",
       "      <td>0.959134</td>\n",
       "      <td>0.956668</td>\n",
       "      <td>0.957604</td>\n",
       "      <td>0.958854</td>\n",
       "      <td>0.959571</td>\n",
       "      <td>0.958366</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.004831</td>\n",
       "      <td>0.450572</td>\n",
       "      <td>0.454781</td>\n",
       "      <td>0.008287</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>988</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 988}</td>\n",
       "      <td>0.952208</td>\n",
       "      <td>0.953077</td>\n",
       "      <td>0.957552</td>\n",
       "      <td>0.951906</td>\n",
       "      <td>0.956071</td>\n",
       "      <td>0.954163</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>36</td>\n",
       "      <td>0.966830</td>\n",
       "      <td>0.960688</td>\n",
       "      <td>0.956081</td>\n",
       "      <td>0.966523</td>\n",
       "      <td>0.962826</td>\n",
       "      <td>0.962590</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>25</td>\n",
       "      <td>0.959464</td>\n",
       "      <td>0.956868</td>\n",
       "      <td>0.956816</td>\n",
       "      <td>0.959159</td>\n",
       "      <td>0.959437</td>\n",
       "      <td>0.958348</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>30.128286</td>\n",
       "      <td>36.270933</td>\n",
       "      <td>0.537506</td>\n",
       "      <td>0.179703</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>918</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 918}</td>\n",
       "      <td>0.952150</td>\n",
       "      <td>0.953893</td>\n",
       "      <td>0.957898</td>\n",
       "      <td>0.951589</td>\n",
       "      <td>0.956641</td>\n",
       "      <td>0.954434</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>35</td>\n",
       "      <td>0.965602</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.957310</td>\n",
       "      <td>0.965909</td>\n",
       "      <td>0.962519</td>\n",
       "      <td>0.962160</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>27</td>\n",
       "      <td>0.958829</td>\n",
       "      <td>0.956668</td>\n",
       "      <td>0.957604</td>\n",
       "      <td>0.958695</td>\n",
       "      <td>0.959571</td>\n",
       "      <td>0.958273</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.876095</td>\n",
       "      <td>0.261830</td>\n",
       "      <td>0.313975</td>\n",
       "      <td>0.015616</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>644</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 644}</td>\n",
       "      <td>0.954878</td>\n",
       "      <td>0.955118</td>\n",
       "      <td>0.959222</td>\n",
       "      <td>0.953510</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.956545</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>31</td>\n",
       "      <td>0.961916</td>\n",
       "      <td>0.954238</td>\n",
       "      <td>0.953624</td>\n",
       "      <td>0.963759</td>\n",
       "      <td>0.958525</td>\n",
       "      <td>0.958413</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>29</td>\n",
       "      <td>0.958384</td>\n",
       "      <td>0.954678</td>\n",
       "      <td>0.956415</td>\n",
       "      <td>0.958607</td>\n",
       "      <td>0.959262</td>\n",
       "      <td>0.957469</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8.256545</td>\n",
       "      <td>0.602532</td>\n",
       "      <td>0.362524</td>\n",
       "      <td>0.010433</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>746</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 746}</td>\n",
       "      <td>0.953206</td>\n",
       "      <td>0.953474</td>\n",
       "      <td>0.958565</td>\n",
       "      <td>0.953594</td>\n",
       "      <td>0.958001</td>\n",
       "      <td>0.955368</td>\n",
       "      <td>0.002390</td>\n",
       "      <td>33</td>\n",
       "      <td>0.963452</td>\n",
       "      <td>0.956695</td>\n",
       "      <td>0.952088</td>\n",
       "      <td>0.965602</td>\n",
       "      <td>0.960061</td>\n",
       "      <td>0.959580</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>28</td>\n",
       "      <td>0.958302</td>\n",
       "      <td>0.955082</td>\n",
       "      <td>0.955316</td>\n",
       "      <td>0.959561</td>\n",
       "      <td>0.959030</td>\n",
       "      <td>0.957458</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6.367509</td>\n",
       "      <td>0.185909</td>\n",
       "      <td>0.268694</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>561</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 561}</td>\n",
       "      <td>0.954991</td>\n",
       "      <td>0.956535</td>\n",
       "      <td>0.960285</td>\n",
       "      <td>0.955115</td>\n",
       "      <td>0.960457</td>\n",
       "      <td>0.957477</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>29</td>\n",
       "      <td>0.957924</td>\n",
       "      <td>0.953010</td>\n",
       "      <td>0.950553</td>\n",
       "      <td>0.960688</td>\n",
       "      <td>0.955146</td>\n",
       "      <td>0.955464</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>30</td>\n",
       "      <td>0.956455</td>\n",
       "      <td>0.954769</td>\n",
       "      <td>0.955394</td>\n",
       "      <td>0.957893</td>\n",
       "      <td>0.957794</td>\n",
       "      <td>0.956461</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.668172</td>\n",
       "      <td>0.297146</td>\n",
       "      <td>0.239958</td>\n",
       "      <td>0.030111</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>448</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 448}</td>\n",
       "      <td>0.956240</td>\n",
       "      <td>0.958879</td>\n",
       "      <td>0.961970</td>\n",
       "      <td>0.958989</td>\n",
       "      <td>0.962986</td>\n",
       "      <td>0.959813</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>23</td>\n",
       "      <td>0.953010</td>\n",
       "      <td>0.945332</td>\n",
       "      <td>0.947789</td>\n",
       "      <td>0.955160</td>\n",
       "      <td>0.951152</td>\n",
       "      <td>0.950488</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>31</td>\n",
       "      <td>0.954622</td>\n",
       "      <td>0.952057</td>\n",
       "      <td>0.954827</td>\n",
       "      <td>0.957070</td>\n",
       "      <td>0.957032</td>\n",
       "      <td>0.955122</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.336304</td>\n",
       "      <td>0.269833</td>\n",
       "      <td>0.244869</td>\n",
       "      <td>0.016218</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>436</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 436}</td>\n",
       "      <td>0.955919</td>\n",
       "      <td>0.958541</td>\n",
       "      <td>0.963183</td>\n",
       "      <td>0.959284</td>\n",
       "      <td>0.963251</td>\n",
       "      <td>0.960036</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>22</td>\n",
       "      <td>0.952396</td>\n",
       "      <td>0.944410</td>\n",
       "      <td>0.948096</td>\n",
       "      <td>0.955160</td>\n",
       "      <td>0.950230</td>\n",
       "      <td>0.950058</td>\n",
       "      <td>0.003668</td>\n",
       "      <td>32</td>\n",
       "      <td>0.954154</td>\n",
       "      <td>0.951423</td>\n",
       "      <td>0.955580</td>\n",
       "      <td>0.957218</td>\n",
       "      <td>0.956697</td>\n",
       "      <td>0.955014</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.842337</td>\n",
       "      <td>0.122179</td>\n",
       "      <td>0.192951</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>369</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 369}</td>\n",
       "      <td>0.957440</td>\n",
       "      <td>0.959688</td>\n",
       "      <td>0.965149</td>\n",
       "      <td>0.960050</td>\n",
       "      <td>0.964989</td>\n",
       "      <td>0.961463</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>19</td>\n",
       "      <td>0.946560</td>\n",
       "      <td>0.943182</td>\n",
       "      <td>0.944103</td>\n",
       "      <td>0.952088</td>\n",
       "      <td>0.948387</td>\n",
       "      <td>0.946864</td>\n",
       "      <td>0.003190</td>\n",
       "      <td>34</td>\n",
       "      <td>0.951969</td>\n",
       "      <td>0.951363</td>\n",
       "      <td>0.954510</td>\n",
       "      <td>0.956052</td>\n",
       "      <td>0.956616</td>\n",
       "      <td>0.954102</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5.536524</td>\n",
       "      <td>0.583180</td>\n",
       "      <td>0.213571</td>\n",
       "      <td>0.042581</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>346</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 346}</td>\n",
       "      <td>0.957996</td>\n",
       "      <td>0.959075</td>\n",
       "      <td>0.965745</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.964923</td>\n",
       "      <td>0.961727</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>18</td>\n",
       "      <td>0.945639</td>\n",
       "      <td>0.942875</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.950860</td>\n",
       "      <td>0.946544</td>\n",
       "      <td>0.945943</td>\n",
       "      <td>0.002780</td>\n",
       "      <td>35</td>\n",
       "      <td>0.951777</td>\n",
       "      <td>0.950906</td>\n",
       "      <td>0.954644</td>\n",
       "      <td>0.955851</td>\n",
       "      <td>0.955645</td>\n",
       "      <td>0.953765</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>12.240656</td>\n",
       "      <td>0.681623</td>\n",
       "      <td>0.491045</td>\n",
       "      <td>0.026456</td>\n",
       "      <td>balanced</td>\n",
       "      <td>997</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 997}</td>\n",
       "      <td>0.954012</td>\n",
       "      <td>0.954447</td>\n",
       "      <td>0.958489</td>\n",
       "      <td>0.954154</td>\n",
       "      <td>0.961035</td>\n",
       "      <td>0.956427</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>32</td>\n",
       "      <td>0.949324</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.943182</td>\n",
       "      <td>0.952396</td>\n",
       "      <td>0.947158</td>\n",
       "      <td>0.947601</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>33</td>\n",
       "      <td>0.951663</td>\n",
       "      <td>0.950177</td>\n",
       "      <td>0.950774</td>\n",
       "      <td>0.953274</td>\n",
       "      <td>0.954046</td>\n",
       "      <td>0.951987</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.685159</td>\n",
       "      <td>0.477053</td>\n",
       "      <td>0.156808</td>\n",
       "      <td>0.014913</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>256</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 256}</td>\n",
       "      <td>0.959523</td>\n",
       "      <td>0.959862</td>\n",
       "      <td>0.967947</td>\n",
       "      <td>0.962754</td>\n",
       "      <td>0.965909</td>\n",
       "      <td>0.963199</td>\n",
       "      <td>0.003308</td>\n",
       "      <td>15</td>\n",
       "      <td>0.939189</td>\n",
       "      <td>0.940111</td>\n",
       "      <td>0.936732</td>\n",
       "      <td>0.944717</td>\n",
       "      <td>0.940092</td>\n",
       "      <td>0.940168</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>39</td>\n",
       "      <td>0.949247</td>\n",
       "      <td>0.949884</td>\n",
       "      <td>0.952084</td>\n",
       "      <td>0.953651</td>\n",
       "      <td>0.952826</td>\n",
       "      <td>0.951538</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>8.556572</td>\n",
       "      <td>0.214952</td>\n",
       "      <td>0.391532</td>\n",
       "      <td>0.009964</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>855</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 855}</td>\n",
       "      <td>0.954376</td>\n",
       "      <td>0.956157</td>\n",
       "      <td>0.959101</td>\n",
       "      <td>0.955823</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.956872</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>30</td>\n",
       "      <td>0.944410</td>\n",
       "      <td>0.944410</td>\n",
       "      <td>0.943489</td>\n",
       "      <td>0.950246</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>0.945758</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>36</td>\n",
       "      <td>0.949367</td>\n",
       "      <td>0.950247</td>\n",
       "      <td>0.951231</td>\n",
       "      <td>0.953026</td>\n",
       "      <td>0.952528</td>\n",
       "      <td>0.951280</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>9.178321</td>\n",
       "      <td>0.548067</td>\n",
       "      <td>0.373143</td>\n",
       "      <td>0.032954</td>\n",
       "      <td>balanced</td>\n",
       "      <td>765</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 765}</td>\n",
       "      <td>0.955549</td>\n",
       "      <td>0.956888</td>\n",
       "      <td>0.959574</td>\n",
       "      <td>0.956966</td>\n",
       "      <td>0.960950</td>\n",
       "      <td>0.957985</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>28</td>\n",
       "      <td>0.944103</td>\n",
       "      <td>0.940725</td>\n",
       "      <td>0.940418</td>\n",
       "      <td>0.949324</td>\n",
       "      <td>0.945008</td>\n",
       "      <td>0.943916</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>37</td>\n",
       "      <td>0.949791</td>\n",
       "      <td>0.948738</td>\n",
       "      <td>0.949899</td>\n",
       "      <td>0.953130</td>\n",
       "      <td>0.952912</td>\n",
       "      <td>0.950894</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>8.224736</td>\n",
       "      <td>0.667928</td>\n",
       "      <td>0.382321</td>\n",
       "      <td>0.051739</td>\n",
       "      <td>balanced</td>\n",
       "      <td>725</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 725}</td>\n",
       "      <td>0.955791</td>\n",
       "      <td>0.957733</td>\n",
       "      <td>0.959824</td>\n",
       "      <td>0.958411</td>\n",
       "      <td>0.960925</td>\n",
       "      <td>0.958537</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>27</td>\n",
       "      <td>0.942875</td>\n",
       "      <td>0.939496</td>\n",
       "      <td>0.939189</td>\n",
       "      <td>0.948403</td>\n",
       "      <td>0.944393</td>\n",
       "      <td>0.942871</td>\n",
       "      <td>0.003402</td>\n",
       "      <td>38</td>\n",
       "      <td>0.949289</td>\n",
       "      <td>0.948527</td>\n",
       "      <td>0.949395</td>\n",
       "      <td>0.953381</td>\n",
       "      <td>0.952588</td>\n",
       "      <td>0.950636</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3.540189</td>\n",
       "      <td>0.247366</td>\n",
       "      <td>0.129443</td>\n",
       "      <td>0.006163</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>221</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 221}</td>\n",
       "      <td>0.960655</td>\n",
       "      <td>0.961478</td>\n",
       "      <td>0.967845</td>\n",
       "      <td>0.962986</td>\n",
       "      <td>0.968244</td>\n",
       "      <td>0.964241</td>\n",
       "      <td>0.003196</td>\n",
       "      <td>12</td>\n",
       "      <td>0.937346</td>\n",
       "      <td>0.935197</td>\n",
       "      <td>0.933661</td>\n",
       "      <td>0.942875</td>\n",
       "      <td>0.936713</td>\n",
       "      <td>0.937158</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>45</td>\n",
       "      <td>0.948857</td>\n",
       "      <td>0.948155</td>\n",
       "      <td>0.950446</td>\n",
       "      <td>0.952824</td>\n",
       "      <td>0.952217</td>\n",
       "      <td>0.950500</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>7.707880</td>\n",
       "      <td>1.076960</td>\n",
       "      <td>0.312287</td>\n",
       "      <td>0.022122</td>\n",
       "      <td>balanced</td>\n",
       "      <td>592</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 592}</td>\n",
       "      <td>0.957614</td>\n",
       "      <td>0.960440</td>\n",
       "      <td>0.961502</td>\n",
       "      <td>0.960637</td>\n",
       "      <td>0.964128</td>\n",
       "      <td>0.960864</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>21</td>\n",
       "      <td>0.936732</td>\n",
       "      <td>0.939496</td>\n",
       "      <td>0.935811</td>\n",
       "      <td>0.944410</td>\n",
       "      <td>0.941321</td>\n",
       "      <td>0.939554</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>42</td>\n",
       "      <td>0.947058</td>\n",
       "      <td>0.949853</td>\n",
       "      <td>0.948482</td>\n",
       "      <td>0.952455</td>\n",
       "      <td>0.952588</td>\n",
       "      <td>0.950087</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7.763043</td>\n",
       "      <td>0.903713</td>\n",
       "      <td>0.340966</td>\n",
       "      <td>0.025013</td>\n",
       "      <td>balanced</td>\n",
       "      <td>579</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 579}</td>\n",
       "      <td>0.957862</td>\n",
       "      <td>0.959824</td>\n",
       "      <td>0.961781</td>\n",
       "      <td>0.960625</td>\n",
       "      <td>0.964669</td>\n",
       "      <td>0.960952</td>\n",
       "      <td>0.002255</td>\n",
       "      <td>20</td>\n",
       "      <td>0.935504</td>\n",
       "      <td>0.939189</td>\n",
       "      <td>0.935197</td>\n",
       "      <td>0.944103</td>\n",
       "      <td>0.939478</td>\n",
       "      <td>0.938694</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>44</td>\n",
       "      <td>0.946551</td>\n",
       "      <td>0.949395</td>\n",
       "      <td>0.948303</td>\n",
       "      <td>0.952292</td>\n",
       "      <td>0.951907</td>\n",
       "      <td>0.949689</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6.478772</td>\n",
       "      <td>0.243360</td>\n",
       "      <td>0.297442</td>\n",
       "      <td>0.026167</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>589</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 589}</td>\n",
       "      <td>0.956020</td>\n",
       "      <td>0.957627</td>\n",
       "      <td>0.961296</td>\n",
       "      <td>0.958827</td>\n",
       "      <td>0.963418</td>\n",
       "      <td>0.959438</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>25</td>\n",
       "      <td>0.941339</td>\n",
       "      <td>0.937039</td>\n",
       "      <td>0.938268</td>\n",
       "      <td>0.944103</td>\n",
       "      <td>0.938556</td>\n",
       "      <td>0.939861</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>41</td>\n",
       "      <td>0.948623</td>\n",
       "      <td>0.947221</td>\n",
       "      <td>0.949643</td>\n",
       "      <td>0.951408</td>\n",
       "      <td>0.950825</td>\n",
       "      <td>0.949544</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>7.437928</td>\n",
       "      <td>0.900784</td>\n",
       "      <td>0.299337</td>\n",
       "      <td>0.012187</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>609</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 609}</td>\n",
       "      <td>0.956277</td>\n",
       "      <td>0.957654</td>\n",
       "      <td>0.960655</td>\n",
       "      <td>0.958580</td>\n",
       "      <td>0.962846</td>\n",
       "      <td>0.959202</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>26</td>\n",
       "      <td>0.940418</td>\n",
       "      <td>0.937654</td>\n",
       "      <td>0.937346</td>\n",
       "      <td>0.945332</td>\n",
       "      <td>0.939478</td>\n",
       "      <td>0.940045</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>40</td>\n",
       "      <td>0.948281</td>\n",
       "      <td>0.947548</td>\n",
       "      <td>0.948857</td>\n",
       "      <td>0.951910</td>\n",
       "      <td>0.951019</td>\n",
       "      <td>0.949523</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>6.749089</td>\n",
       "      <td>0.373555</td>\n",
       "      <td>0.285617</td>\n",
       "      <td>0.011030</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>584</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 584}</td>\n",
       "      <td>0.956305</td>\n",
       "      <td>0.957300</td>\n",
       "      <td>0.961587</td>\n",
       "      <td>0.959413</td>\n",
       "      <td>0.963395</td>\n",
       "      <td>0.959600</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>24</td>\n",
       "      <td>0.941032</td>\n",
       "      <td>0.936425</td>\n",
       "      <td>0.937961</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.937942</td>\n",
       "      <td>0.939431</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>43</td>\n",
       "      <td>0.948607</td>\n",
       "      <td>0.946747</td>\n",
       "      <td>0.949627</td>\n",
       "      <td>0.951540</td>\n",
       "      <td>0.950498</td>\n",
       "      <td>0.949404</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>7.169069</td>\n",
       "      <td>1.051897</td>\n",
       "      <td>0.279301</td>\n",
       "      <td>0.027196</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>495</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 495}</td>\n",
       "      <td>0.957667</td>\n",
       "      <td>0.959924</td>\n",
       "      <td>0.965113</td>\n",
       "      <td>0.962276</td>\n",
       "      <td>0.964683</td>\n",
       "      <td>0.961933</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>17</td>\n",
       "      <td>0.937961</td>\n",
       "      <td>0.934275</td>\n",
       "      <td>0.934582</td>\n",
       "      <td>0.940111</td>\n",
       "      <td>0.931490</td>\n",
       "      <td>0.935684</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>46</td>\n",
       "      <td>0.947711</td>\n",
       "      <td>0.946926</td>\n",
       "      <td>0.949602</td>\n",
       "      <td>0.951064</td>\n",
       "      <td>0.947796</td>\n",
       "      <td>0.948620</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>7.134660</td>\n",
       "      <td>0.172411</td>\n",
       "      <td>0.272900</td>\n",
       "      <td>0.023748</td>\n",
       "      <td>balanced</td>\n",
       "      <td>472</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 472}</td>\n",
       "      <td>0.959280</td>\n",
       "      <td>0.960809</td>\n",
       "      <td>0.965858</td>\n",
       "      <td>0.961830</td>\n",
       "      <td>0.967208</td>\n",
       "      <td>0.962997</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>16</td>\n",
       "      <td>0.933354</td>\n",
       "      <td>0.933661</td>\n",
       "      <td>0.929668</td>\n",
       "      <td>0.936425</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933288</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>47</td>\n",
       "      <td>0.946139</td>\n",
       "      <td>0.947040</td>\n",
       "      <td>0.947418</td>\n",
       "      <td>0.948957</td>\n",
       "      <td>0.949969</td>\n",
       "      <td>0.947905</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5.370031</td>\n",
       "      <td>0.195425</td>\n",
       "      <td>0.222596</td>\n",
       "      <td>0.015009</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>422</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 422}</td>\n",
       "      <td>0.958150</td>\n",
       "      <td>0.960697</td>\n",
       "      <td>0.967177</td>\n",
       "      <td>0.963901</td>\n",
       "      <td>0.967287</td>\n",
       "      <td>0.963442</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>14</td>\n",
       "      <td>0.935197</td>\n",
       "      <td>0.930897</td>\n",
       "      <td>0.932125</td>\n",
       "      <td>0.934889</td>\n",
       "      <td>0.926575</td>\n",
       "      <td>0.931937</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>48</td>\n",
       "      <td>0.946534</td>\n",
       "      <td>0.945562</td>\n",
       "      <td>0.949327</td>\n",
       "      <td>0.949174</td>\n",
       "      <td>0.946493</td>\n",
       "      <td>0.947418</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.121574</td>\n",
       "      <td>0.077818</td>\n",
       "      <td>0.208981</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>418</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 418}</td>\n",
       "      <td>0.958438</td>\n",
       "      <td>0.961282</td>\n",
       "      <td>0.967474</td>\n",
       "      <td>0.964195</td>\n",
       "      <td>0.966977</td>\n",
       "      <td>0.963673</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>13</td>\n",
       "      <td>0.934889</td>\n",
       "      <td>0.930283</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.934582</td>\n",
       "      <td>0.926575</td>\n",
       "      <td>0.931629</td>\n",
       "      <td>0.003058</td>\n",
       "      <td>49</td>\n",
       "      <td>0.946517</td>\n",
       "      <td>0.945528</td>\n",
       "      <td>0.949312</td>\n",
       "      <td>0.949158</td>\n",
       "      <td>0.946345</td>\n",
       "      <td>0.947372</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4.758875</td>\n",
       "      <td>0.143656</td>\n",
       "      <td>0.194569</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>377</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 377}</td>\n",
       "      <td>0.960114</td>\n",
       "      <td>0.961122</td>\n",
       "      <td>0.968910</td>\n",
       "      <td>0.964979</td>\n",
       "      <td>0.968157</td>\n",
       "      <td>0.964656</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>11</td>\n",
       "      <td>0.931511</td>\n",
       "      <td>0.926290</td>\n",
       "      <td>0.928440</td>\n",
       "      <td>0.930897</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.928374</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>50</td>\n",
       "      <td>0.945596</td>\n",
       "      <td>0.943384</td>\n",
       "      <td>0.948243</td>\n",
       "      <td>0.947632</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.946160</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5.062231</td>\n",
       "      <td>0.613765</td>\n",
       "      <td>0.159859</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>278</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 278}</td>\n",
       "      <td>0.964022</td>\n",
       "      <td>0.963707</td>\n",
       "      <td>0.971363</td>\n",
       "      <td>0.967482</td>\n",
       "      <td>0.971578</td>\n",
       "      <td>0.967631</td>\n",
       "      <td>0.003405</td>\n",
       "      <td>10</td>\n",
       "      <td>0.921683</td>\n",
       "      <td>0.913391</td>\n",
       "      <td>0.916769</td>\n",
       "      <td>0.922912</td>\n",
       "      <td>0.913671</td>\n",
       "      <td>0.917685</td>\n",
       "      <td>0.003967</td>\n",
       "      <td>51</td>\n",
       "      <td>0.942377</td>\n",
       "      <td>0.937874</td>\n",
       "      <td>0.943277</td>\n",
       "      <td>0.944671</td>\n",
       "      <td>0.941735</td>\n",
       "      <td>0.941987</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2.329757</td>\n",
       "      <td>0.252141</td>\n",
       "      <td>0.060961</td>\n",
       "      <td>0.010334</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>74</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 74}</td>\n",
       "      <td>0.969915</td>\n",
       "      <td>0.966428</td>\n",
       "      <td>0.974834</td>\n",
       "      <td>0.970893</td>\n",
       "      <td>0.974646</td>\n",
       "      <td>0.971343</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>6</td>\n",
       "      <td>0.910934</td>\n",
       "      <td>0.910627</td>\n",
       "      <td>0.904177</td>\n",
       "      <td>0.921990</td>\n",
       "      <td>0.909370</td>\n",
       "      <td>0.911419</td>\n",
       "      <td>0.005818</td>\n",
       "      <td>53</td>\n",
       "      <td>0.939500</td>\n",
       "      <td>0.937698</td>\n",
       "      <td>0.938177</td>\n",
       "      <td>0.945810</td>\n",
       "      <td>0.940877</td>\n",
       "      <td>0.940412</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4.307721</td>\n",
       "      <td>0.489038</td>\n",
       "      <td>0.156253</td>\n",
       "      <td>0.023217</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>241</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 241}</td>\n",
       "      <td>0.965395</td>\n",
       "      <td>0.965101</td>\n",
       "      <td>0.973079</td>\n",
       "      <td>0.967992</td>\n",
       "      <td>0.974022</td>\n",
       "      <td>0.969118</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>9</td>\n",
       "      <td>0.916769</td>\n",
       "      <td>0.908784</td>\n",
       "      <td>0.910319</td>\n",
       "      <td>0.919533</td>\n",
       "      <td>0.909985</td>\n",
       "      <td>0.913078</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>52</td>\n",
       "      <td>0.940454</td>\n",
       "      <td>0.936096</td>\n",
       "      <td>0.940654</td>\n",
       "      <td>0.943141</td>\n",
       "      <td>0.940915</td>\n",
       "      <td>0.940252</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4.523507</td>\n",
       "      <td>0.284664</td>\n",
       "      <td>0.150213</td>\n",
       "      <td>0.008276</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>224</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 224}</td>\n",
       "      <td>0.966515</td>\n",
       "      <td>0.964941</td>\n",
       "      <td>0.973589</td>\n",
       "      <td>0.968882</td>\n",
       "      <td>0.973910</td>\n",
       "      <td>0.969567</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>8</td>\n",
       "      <td>0.913084</td>\n",
       "      <td>0.904484</td>\n",
       "      <td>0.905713</td>\n",
       "      <td>0.917998</td>\n",
       "      <td>0.905991</td>\n",
       "      <td>0.909454</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>54</td>\n",
       "      <td>0.939040</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.938425</td>\n",
       "      <td>0.942754</td>\n",
       "      <td>0.938724</td>\n",
       "      <td>0.938535</td>\n",
       "      <td>0.002868</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4.867814</td>\n",
       "      <td>0.738449</td>\n",
       "      <td>0.150036</td>\n",
       "      <td>0.013374</td>\n",
       "      <td>balanced</td>\n",
       "      <td>212</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 212}</td>\n",
       "      <td>0.967058</td>\n",
       "      <td>0.967181</td>\n",
       "      <td>0.973395</td>\n",
       "      <td>0.970013</td>\n",
       "      <td>0.973798</td>\n",
       "      <td>0.970289</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>7</td>\n",
       "      <td>0.910627</td>\n",
       "      <td>0.905098</td>\n",
       "      <td>0.898956</td>\n",
       "      <td>0.914005</td>\n",
       "      <td>0.901997</td>\n",
       "      <td>0.906136</td>\n",
       "      <td>0.005511</td>\n",
       "      <td>55</td>\n",
       "      <td>0.937994</td>\n",
       "      <td>0.935110</td>\n",
       "      <td>0.934696</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.936523</td>\n",
       "      <td>0.937100</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2.832277</td>\n",
       "      <td>0.091758</td>\n",
       "      <td>0.107563</td>\n",
       "      <td>0.015550</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>151</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 151}</td>\n",
       "      <td>0.971372</td>\n",
       "      <td>0.967602</td>\n",
       "      <td>0.977770</td>\n",
       "      <td>0.971580</td>\n",
       "      <td>0.975758</td>\n",
       "      <td>0.972816</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>5</td>\n",
       "      <td>0.896192</td>\n",
       "      <td>0.889742</td>\n",
       "      <td>0.891585</td>\n",
       "      <td>0.902948</td>\n",
       "      <td>0.890323</td>\n",
       "      <td>0.894158</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>56</td>\n",
       "      <td>0.932268</td>\n",
       "      <td>0.927040</td>\n",
       "      <td>0.932691</td>\n",
       "      <td>0.936008</td>\n",
       "      <td>0.931084</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3.321998</td>\n",
       "      <td>0.125802</td>\n",
       "      <td>0.099018</td>\n",
       "      <td>0.012292</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>150</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 150}</td>\n",
       "      <td>0.971362</td>\n",
       "      <td>0.967602</td>\n",
       "      <td>0.977433</td>\n",
       "      <td>0.971910</td>\n",
       "      <td>0.976078</td>\n",
       "      <td>0.972877</td>\n",
       "      <td>0.003523</td>\n",
       "      <td>4</td>\n",
       "      <td>0.895885</td>\n",
       "      <td>0.889742</td>\n",
       "      <td>0.891278</td>\n",
       "      <td>0.903256</td>\n",
       "      <td>0.890015</td>\n",
       "      <td>0.894035</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>57</td>\n",
       "      <td>0.932098</td>\n",
       "      <td>0.927040</td>\n",
       "      <td>0.932369</td>\n",
       "      <td>0.936326</td>\n",
       "      <td>0.931062</td>\n",
       "      <td>0.931779</td>\n",
       "      <td>0.002970</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2.748065</td>\n",
       "      <td>0.082083</td>\n",
       "      <td>0.091123</td>\n",
       "      <td>0.011866</td>\n",
       "      <td>balanced</td>\n",
       "      <td>140</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 140}</td>\n",
       "      <td>0.973163</td>\n",
       "      <td>0.969076</td>\n",
       "      <td>0.977265</td>\n",
       "      <td>0.971836</td>\n",
       "      <td>0.976902</td>\n",
       "      <td>0.973648</td>\n",
       "      <td>0.003102</td>\n",
       "      <td>3</td>\n",
       "      <td>0.890971</td>\n",
       "      <td>0.885442</td>\n",
       "      <td>0.884521</td>\n",
       "      <td>0.900799</td>\n",
       "      <td>0.883564</td>\n",
       "      <td>0.889059</td>\n",
       "      <td>0.006408</td>\n",
       "      <td>58</td>\n",
       "      <td>0.930255</td>\n",
       "      <td>0.925373</td>\n",
       "      <td>0.928583</td>\n",
       "      <td>0.934970</td>\n",
       "      <td>0.927892</td>\n",
       "      <td>0.929414</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2.836820</td>\n",
       "      <td>0.181259</td>\n",
       "      <td>0.067940</td>\n",
       "      <td>0.012788</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>75</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 75}</td>\n",
       "      <td>0.977755</td>\n",
       "      <td>0.974306</td>\n",
       "      <td>0.979649</td>\n",
       "      <td>0.977374</td>\n",
       "      <td>0.979145</td>\n",
       "      <td>0.977646</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>2</td>\n",
       "      <td>0.863943</td>\n",
       "      <td>0.861794</td>\n",
       "      <td>0.857494</td>\n",
       "      <td>0.875614</td>\n",
       "      <td>0.865438</td>\n",
       "      <td>0.864857</td>\n",
       "      <td>0.006008</td>\n",
       "      <td>59</td>\n",
       "      <td>0.917332</td>\n",
       "      <td>0.914602</td>\n",
       "      <td>0.914510</td>\n",
       "      <td>0.923700</td>\n",
       "      <td>0.918787</td>\n",
       "      <td>0.917786</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2.286945</td>\n",
       "      <td>0.076688</td>\n",
       "      <td>0.056235</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>59</td>\n",
       "      <td>{'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 59}</td>\n",
       "      <td>0.979599</td>\n",
       "      <td>0.975149</td>\n",
       "      <td>0.979837</td>\n",
       "      <td>0.980263</td>\n",
       "      <td>0.979692</td>\n",
       "      <td>0.978908</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>1</td>\n",
       "      <td>0.855344</td>\n",
       "      <td>0.855651</td>\n",
       "      <td>0.850737</td>\n",
       "      <td>0.869472</td>\n",
       "      <td>0.859601</td>\n",
       "      <td>0.858161</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>60</td>\n",
       "      <td>0.913264</td>\n",
       "      <td>0.911500</td>\n",
       "      <td>0.910735</td>\n",
       "      <td>0.921549</td>\n",
       "      <td>0.915726</td>\n",
       "      <td>0.914555</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        2.699475      0.180762         0.066927        0.013200   \n",
       "1        3.416172      0.161599         0.104414        0.009275   \n",
       "2        4.097562      0.174045         0.149262        0.007325   \n",
       "3        5.776887      0.644607         0.190811        0.018065   \n",
       "4        4.381330      0.234753         0.150377        0.017984   \n",
       "5        5.607064      0.232923         0.195960        0.013245   \n",
       "6        3.753953      0.175939         0.123820        0.002306   \n",
       "7        3.087639      0.136404         0.093727        0.009881   \n",
       "8        5.759002      0.395646         0.216329        0.013375   \n",
       "9        7.927969      0.259288         0.291329        0.030403   \n",
       "10      10.029583      0.378627         0.393761        0.072249   \n",
       "11      12.388265      4.117636         0.393725        0.078679   \n",
       "12       7.550388      0.300657         0.258309        0.015854   \n",
       "13       7.148095      0.331638         0.271077        0.012973   \n",
       "14       8.103303      0.720893         0.355317        0.049041   \n",
       "15       6.993068      0.250673         0.254256        0.003926   \n",
       "16       7.661269      0.259241         0.368911        0.047452   \n",
       "17       6.440078      0.196475         0.287438        0.007635   \n",
       "18       8.880516      0.294736         0.387970        0.012140   \n",
       "19      10.431266      0.491859         0.427991        0.037751   \n",
       "20       8.058729      0.215926         0.369865        0.007464   \n",
       "21       9.058358      0.142582         0.423618        0.016537   \n",
       "22       9.506170      0.310538         0.447899        0.041781   \n",
       "23      11.010930      1.768648         0.510778        0.065188   \n",
       "24      10.255647      0.571337         0.481648        0.059350   \n",
       "25      10.004831      0.450572         0.454781        0.008287   \n",
       "26      30.128286     36.270933         0.537506        0.179703   \n",
       "27       6.876095      0.261830         0.313975        0.015616   \n",
       "28       8.256545      0.602532         0.362524        0.010433   \n",
       "29       6.367509      0.185909         0.268694        0.006247   \n",
       "30       5.668172      0.297146         0.239958        0.030111   \n",
       "31       6.336304      0.269833         0.244869        0.016218   \n",
       "32       4.842337      0.122179         0.192951        0.006845   \n",
       "33       5.536524      0.583180         0.213571        0.042581   \n",
       "34      12.240656      0.681623         0.491045        0.026456   \n",
       "35       4.685159      0.477053         0.156808        0.014913   \n",
       "36       8.556572      0.214952         0.391532        0.009964   \n",
       "37       9.178321      0.548067         0.373143        0.032954   \n",
       "38       8.224736      0.667928         0.382321        0.051739   \n",
       "39       3.540189      0.247366         0.129443        0.006163   \n",
       "40       7.707880      1.076960         0.312287        0.022122   \n",
       "41       7.763043      0.903713         0.340966        0.025013   \n",
       "42       6.478772      0.243360         0.297442        0.026167   \n",
       "43       7.437928      0.900784         0.299337        0.012187   \n",
       "44       6.749089      0.373555         0.285617        0.011030   \n",
       "45       7.169069      1.051897         0.279301        0.027196   \n",
       "46       7.134660      0.172411         0.272900        0.023748   \n",
       "47       5.370031      0.195425         0.222596        0.015009   \n",
       "48       5.121574      0.077818         0.208981        0.007250   \n",
       "49       4.758875      0.143656         0.194569        0.007117   \n",
       "50       5.062231      0.613765         0.159859        0.006070   \n",
       "51       2.329757      0.252141         0.060961        0.010334   \n",
       "52       4.307721      0.489038         0.156253        0.023217   \n",
       "53       4.523507      0.284664         0.150213        0.008276   \n",
       "54       4.867814      0.738449         0.150036        0.013374   \n",
       "55       2.832277      0.091758         0.107563        0.015550   \n",
       "56       3.321998      0.125802         0.099018        0.012292   \n",
       "57       2.748065      0.082083         0.091123        0.011866   \n",
       "58       2.836820      0.181259         0.067940        0.012788   \n",
       "59       2.286945      0.076688         0.056235        0.007652   \n",
       "\n",
       "   param_class_weight param_n_estimators  \\\n",
       "0    {0: 1.0, 1: 1.0}                 76   \n",
       "1    {0: 1.0, 1: 1.0}                144   \n",
       "2    {0: 1.0, 1: 1.0}                250   \n",
       "3    {0: 1.0, 1: 1.0}                334   \n",
       "4    {0: 1.0, 1: 1.0}                200   \n",
       "5    {0: 1.0, 1: 1.0}                321   \n",
       "6    {0: 1.0, 1: 1.0}                167   \n",
       "7    {0: 1.0, 1: 1.0}                124   \n",
       "8    {0: 1.0, 1: 1.0}                397   \n",
       "9    {0: 1.0, 1: 1.0}                506   \n",
       "10   {0: 1.0, 1: 1.0}                715   \n",
       "11   {0: 1.0, 1: 1.0}                587   \n",
       "12   {0: 1.0, 1: 1.0}                485   \n",
       "13   {0: 1.0, 1: 1.0}                495   \n",
       "14   {0: 1.0, 1: 1.0}                566   \n",
       "15   {0: 1.0, 1: 1.0}                473   \n",
       "16   {0: 1.0, 1: 1.0}                684   \n",
       "17   {0: 1.0, 1: 1.0}                565   \n",
       "18   {0: 1.0, 1: 1.0}                802   \n",
       "19   {0: 1.0, 1: 1.0}                778   \n",
       "20   {0: 1.0, 1: 1.0}                778   \n",
       "21   {0: 1.0, 1: 1.0}                919   \n",
       "22   {0: 1.0, 1: 1.0}                932   \n",
       "23   {0: 1.0, 1: 1.0}                972   \n",
       "24   {0: 5.0, 1: 1.0}                920   \n",
       "25   {0: 5.0, 1: 1.0}                988   \n",
       "26   {0: 5.0, 1: 1.0}                918   \n",
       "27   {0: 5.0, 1: 1.0}                644   \n",
       "28   {0: 5.0, 1: 1.0}                746   \n",
       "29   {0: 5.0, 1: 1.0}                561   \n",
       "30   {0: 5.0, 1: 1.0}                448   \n",
       "31   {0: 5.0, 1: 1.0}                436   \n",
       "32   {0: 5.0, 1: 1.0}                369   \n",
       "33   {0: 5.0, 1: 1.0}                346   \n",
       "34           balanced                997   \n",
       "35   {0: 5.0, 1: 1.0}                256   \n",
       "36  {0: 10.0, 1: 1.0}                855   \n",
       "37           balanced                765   \n",
       "38           balanced                725   \n",
       "39   {0: 5.0, 1: 1.0}                221   \n",
       "40           balanced                592   \n",
       "41           balanced                579   \n",
       "42  {0: 10.0, 1: 1.0}                589   \n",
       "43  {0: 10.0, 1: 1.0}                609   \n",
       "44  {0: 10.0, 1: 1.0}                584   \n",
       "45  {0: 10.0, 1: 1.0}                495   \n",
       "46           balanced                472   \n",
       "47  {0: 10.0, 1: 1.0}                422   \n",
       "48  {0: 10.0, 1: 1.0}                418   \n",
       "49  {0: 10.0, 1: 1.0}                377   \n",
       "50  {0: 10.0, 1: 1.0}                278   \n",
       "51   {0: 5.0, 1: 1.0}                 74   \n",
       "52  {0: 10.0, 1: 1.0}                241   \n",
       "53  {0: 10.0, 1: 1.0}                224   \n",
       "54           balanced                212   \n",
       "55  {0: 10.0, 1: 1.0}                151   \n",
       "56  {0: 10.0, 1: 1.0}                150   \n",
       "57           balanced                140   \n",
       "58  {0: 10.0, 1: 1.0}                 75   \n",
       "59  {0: 10.0, 1: 1.0}                 59   \n",
       "\n",
       "                                                      params  \\\n",
       "0     {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 76}   \n",
       "1    {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 144}   \n",
       "2    {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 250}   \n",
       "3    {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 334}   \n",
       "4    {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 200}   \n",
       "5    {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 321}   \n",
       "6    {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 167}   \n",
       "7    {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 124}   \n",
       "8    {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 397}   \n",
       "9    {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 506}   \n",
       "10   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 715}   \n",
       "11   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 587}   \n",
       "12   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 485}   \n",
       "13   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 495}   \n",
       "14   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 566}   \n",
       "15   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 473}   \n",
       "16   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 684}   \n",
       "17   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 565}   \n",
       "18   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 802}   \n",
       "19   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 778}   \n",
       "20   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 778}   \n",
       "21   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 919}   \n",
       "22   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 932}   \n",
       "23   {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 972}   \n",
       "24   {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 920}   \n",
       "25   {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 988}   \n",
       "26   {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 918}   \n",
       "27   {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 644}   \n",
       "28   {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 746}   \n",
       "29   {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 561}   \n",
       "30   {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 448}   \n",
       "31   {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 436}   \n",
       "32   {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 369}   \n",
       "33   {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 346}   \n",
       "34         {'class_weight': 'balanced', 'n_estimators': 997}   \n",
       "35   {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 256}   \n",
       "36  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 855}   \n",
       "37         {'class_weight': 'balanced', 'n_estimators': 765}   \n",
       "38         {'class_weight': 'balanced', 'n_estimators': 725}   \n",
       "39   {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 221}   \n",
       "40         {'class_weight': 'balanced', 'n_estimators': 592}   \n",
       "41         {'class_weight': 'balanced', 'n_estimators': 579}   \n",
       "42  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 589}   \n",
       "43  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 609}   \n",
       "44  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 584}   \n",
       "45  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 495}   \n",
       "46         {'class_weight': 'balanced', 'n_estimators': 472}   \n",
       "47  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 422}   \n",
       "48  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 418}   \n",
       "49  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 377}   \n",
       "50  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 278}   \n",
       "51    {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 74}   \n",
       "52  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 241}   \n",
       "53  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 224}   \n",
       "54         {'class_weight': 'balanced', 'n_estimators': 212}   \n",
       "55  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 151}   \n",
       "56  {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 150}   \n",
       "57         {'class_weight': 'balanced', 'n_estimators': 140}   \n",
       "58   {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 75}   \n",
       "59   {'class_weight': {0: 10.0, 1: 1.0}, 'n_estimators': 59}   \n",
       "\n",
       "    split0_test_precision  split1_test_precision  split2_test_precision  \\\n",
       "0                0.942122               0.942999               0.947816   \n",
       "1                0.943225               0.944282               0.950457   \n",
       "2                0.946240               0.946809               0.950931   \n",
       "3                0.947012               0.947400               0.950650   \n",
       "4                0.944591               0.946887               0.950723   \n",
       "5                0.947291               0.947664               0.950917   \n",
       "6                0.944070               0.945851               0.950442   \n",
       "7                0.942088               0.944282               0.949367   \n",
       "8                0.947322               0.947322               0.951422   \n",
       "9                0.948098               0.947852               0.951422   \n",
       "10               0.948627               0.946979               0.951479   \n",
       "11               0.948128               0.947290               0.950902   \n",
       "12               0.947803               0.947587               0.950858   \n",
       "13               0.947818               0.947571               0.951422   \n",
       "14               0.948128               0.947290               0.951479   \n",
       "15               0.947803               0.947587               0.951126   \n",
       "16               0.948408               0.947275               0.950592   \n",
       "17               0.948128               0.947290               0.950902   \n",
       "18               0.948892               0.946106               0.951183   \n",
       "19               0.948332               0.946651               0.951155   \n",
       "20               0.948332               0.946651               0.951155   \n",
       "21               0.948083               0.946386               0.950015   \n",
       "22               0.948052               0.946090               0.949719   \n",
       "23               0.948052               0.946074               0.950577   \n",
       "24               0.952453               0.953893               0.957898   \n",
       "25               0.952208               0.953077               0.957552   \n",
       "26               0.952150               0.953893               0.957898   \n",
       "27               0.954878               0.955118               0.959222   \n",
       "28               0.953206               0.953474               0.958565   \n",
       "29               0.954991               0.956535               0.960285   \n",
       "30               0.956240               0.958879               0.961970   \n",
       "31               0.955919               0.958541               0.963183   \n",
       "32               0.957440               0.959688               0.965149   \n",
       "33               0.957996               0.959075               0.965745   \n",
       "34               0.954012               0.954447               0.958489   \n",
       "35               0.959523               0.959862               0.967947   \n",
       "36               0.954376               0.956157               0.959101   \n",
       "37               0.955549               0.956888               0.959574   \n",
       "38               0.955791               0.957733               0.959824   \n",
       "39               0.960655               0.961478               0.967845   \n",
       "40               0.957614               0.960440               0.961502   \n",
       "41               0.957862               0.959824               0.961781   \n",
       "42               0.956020               0.957627               0.961296   \n",
       "43               0.956277               0.957654               0.960655   \n",
       "44               0.956305               0.957300               0.961587   \n",
       "45               0.957667               0.959924               0.965113   \n",
       "46               0.959280               0.960809               0.965858   \n",
       "47               0.958150               0.960697               0.967177   \n",
       "48               0.958438               0.961282               0.967474   \n",
       "49               0.960114               0.961122               0.968910   \n",
       "50               0.964022               0.963707               0.971363   \n",
       "51               0.969915               0.966428               0.974834   \n",
       "52               0.965395               0.965101               0.973079   \n",
       "53               0.966515               0.964941               0.973589   \n",
       "54               0.967058               0.967181               0.973395   \n",
       "55               0.971372               0.967602               0.977770   \n",
       "56               0.971362               0.967602               0.977433   \n",
       "57               0.973163               0.969076               0.977265   \n",
       "58               0.977755               0.974306               0.979649   \n",
       "59               0.979599               0.975149               0.979837   \n",
       "\n",
       "    split3_test_precision  split4_test_precision  mean_test_precision  \\\n",
       "0                0.942291               0.944688             0.943983   \n",
       "1                0.942139               0.946025             0.945226   \n",
       "2                0.944819               0.947028             0.947165   \n",
       "3                0.946097               0.947803             0.947792   \n",
       "4                0.942882               0.946765             0.946370   \n",
       "5                0.945819               0.947508             0.947840   \n",
       "6                0.942054               0.945978             0.945679   \n",
       "7                0.941898               0.945748             0.944677   \n",
       "8                0.946344               0.948301             0.948142   \n",
       "9                0.946360               0.949303             0.948607   \n",
       "10               0.946297               0.950416             0.948760   \n",
       "11               0.946018               0.949585             0.948385   \n",
       "12               0.946081               0.949318             0.948330   \n",
       "13               0.946081               0.949022             0.948383   \n",
       "14               0.945460               0.949555             0.948383   \n",
       "15               0.946329               0.949037             0.948376   \n",
       "16               0.945755               0.949852             0.948376   \n",
       "17               0.945460               0.949555             0.948267   \n",
       "18               0.946034               0.949792             0.948401   \n",
       "19               0.946034               0.949822             0.948399   \n",
       "20               0.946034               0.949822             0.948399   \n",
       "21               0.946050               0.950089             0.948124   \n",
       "22               0.945771               0.950669             0.948060   \n",
       "23               0.946034               0.950312             0.948210   \n",
       "24               0.951603               0.956641             0.954498   \n",
       "25               0.951906               0.956071             0.954163   \n",
       "26               0.951589               0.956641             0.954434   \n",
       "27               0.953510               0.960000             0.956545   \n",
       "28               0.953594               0.958001             0.955368   \n",
       "29               0.955115               0.960457             0.957477   \n",
       "30               0.958989               0.962986             0.959813   \n",
       "31               0.959284               0.963251             0.960036   \n",
       "32               0.960050               0.964989             0.961463   \n",
       "33               0.960894               0.964923             0.961727   \n",
       "34               0.954154               0.961035             0.956427   \n",
       "35               0.962754               0.965909             0.963199   \n",
       "36               0.955823               0.958904             0.956872   \n",
       "37               0.956966               0.960950             0.957985   \n",
       "38               0.958411               0.960925             0.958537   \n",
       "39               0.962986               0.968244             0.964241   \n",
       "40               0.960637               0.964128             0.960864   \n",
       "41               0.960625               0.964669             0.960952   \n",
       "42               0.958827               0.963418             0.959438   \n",
       "43               0.958580               0.962846             0.959202   \n",
       "44               0.959413               0.963395             0.959600   \n",
       "45               0.962276               0.964683             0.961933   \n",
       "46               0.961830               0.967208             0.962997   \n",
       "47               0.963901               0.967287             0.963442   \n",
       "48               0.964195               0.966977             0.963673   \n",
       "49               0.964979               0.968157             0.964656   \n",
       "50               0.967482               0.971578             0.967631   \n",
       "51               0.970893               0.974646             0.971343   \n",
       "52               0.967992               0.974022             0.969118   \n",
       "53               0.968882               0.973910             0.969567   \n",
       "54               0.970013               0.973798             0.970289   \n",
       "55               0.971580               0.975758             0.972816   \n",
       "56               0.971910               0.976078             0.972877   \n",
       "57               0.971836               0.976902             0.973648   \n",
       "58               0.977374               0.979145             0.977646   \n",
       "59               0.980263               0.979692             0.978908   \n",
       "\n",
       "    std_test_precision  rank_test_precision  split0_test_recall  \\\n",
       "0             0.002120                   60            0.989865   \n",
       "1             0.002913                   58            0.989865   \n",
       "2             0.002034                   55            0.989251   \n",
       "3             0.001536                   54            0.988022   \n",
       "4             0.002634                   56            0.989558   \n",
       "5             0.001673                   53            0.988022   \n",
       "6             0.002776                   57            0.990172   \n",
       "7             0.002747                   59            0.989251   \n",
       "8             0.001753                   50            0.988636   \n",
       "9             0.001690                   38            0.987408   \n",
       "10            0.001969                   37            0.986794   \n",
       "11            0.001711                   42            0.988022   \n",
       "12            0.001628                   47            0.987101   \n",
       "13            0.001784                   43            0.987408   \n",
       "14            0.002038                   44            0.988022   \n",
       "15            0.001621                   46            0.987101   \n",
       "16            0.001742                   45            0.988022   \n",
       "17            0.001868                   48            0.988022   \n",
       "18            0.002039                   39            0.986486   \n",
       "19            0.001912                   40            0.986486   \n",
       "20            0.001912                   40            0.986486   \n",
       "21            0.001718                   51            0.987101   \n",
       "22            0.001933                   52            0.986486   \n",
       "23            0.001967                   49            0.986486   \n",
       "24            0.002412                   34            0.965909   \n",
       "25            0.002246                   36            0.966830   \n",
       "26            0.002469                   35            0.965602   \n",
       "27            0.002574                   31            0.961916   \n",
       "28            0.002390                   33            0.963452   \n",
       "29            0.002426                   29            0.957924   \n",
       "30            0.002410                   23            0.953010   \n",
       "31            0.002828                   22            0.952396   \n",
       "32            0.003077                   19            0.946560   \n",
       "33            0.003098                   18            0.945639   \n",
       "34            0.002843                   32            0.949324   \n",
       "35            0.003308                   15            0.939189   \n",
       "36            0.001841                   30            0.944410   \n",
       "37            0.001974                   28            0.944103   \n",
       "38            0.001764                   27            0.942875   \n",
       "39            0.003196                   12            0.937346   \n",
       "40            0.002091                   21            0.936732   \n",
       "41            0.002255                   20            0.935504   \n",
       "42            0.002632                   25            0.941339   \n",
       "43            0.002312                   26            0.940418   \n",
       "44            0.002628                   24            0.941032   \n",
       "45            0.002829                   17            0.937961   \n",
       "46            0.003029                   16            0.933354   \n",
       "47            0.003591                   14            0.935197   \n",
       "48            0.003428                   13            0.934889   \n",
       "49            0.003566                   11            0.931511   \n",
       "50            0.003405                   10            0.921683   \n",
       "51            0.003147                    6            0.910934   \n",
       "52            0.003768                    9            0.916769   \n",
       "53            0.003639                    8            0.913084   \n",
       "54            0.002903                    7            0.910627   \n",
       "55            0.003578                    5            0.896192   \n",
       "56            0.003523                    4            0.895885   \n",
       "57            0.003102                    3            0.890971   \n",
       "58            0.001871                    2            0.863943   \n",
       "59            0.001893                    1            0.855344   \n",
       "\n",
       "    split1_test_recall  split2_test_recall  split3_test_recall  \\\n",
       "0             0.990786            0.992936            0.992936   \n",
       "1             0.988943            0.989865            0.990172   \n",
       "2             0.984029            0.988022            0.988636   \n",
       "3             0.984644            0.988022            0.986486   \n",
       "4             0.985565            0.989558            0.988636   \n",
       "5             0.984337            0.987715            0.986486   \n",
       "6             0.987101            0.989558            0.988636   \n",
       "7             0.988943            0.990479            0.990786   \n",
       "8             0.983108            0.986486            0.985872   \n",
       "9             0.982494            0.986486            0.986179   \n",
       "10            0.981880            0.987715            0.984951   \n",
       "11            0.982494            0.987408            0.984951   \n",
       "12            0.982801            0.986486            0.986179   \n",
       "13            0.982494            0.986486            0.986179   \n",
       "14            0.982494            0.987715            0.984951   \n",
       "15            0.982801            0.986179            0.985565   \n",
       "16            0.982187            0.986794            0.985258   \n",
       "17            0.982494            0.987408            0.984951   \n",
       "18            0.981265            0.987408            0.985258   \n",
       "19            0.980958            0.986794            0.985258   \n",
       "20            0.980958            0.986794            0.985258   \n",
       "21            0.981265            0.986486            0.985565   \n",
       "22            0.980958            0.986179            0.985565   \n",
       "23            0.980651            0.986486            0.985258   \n",
       "24            0.959459            0.957310            0.966216   \n",
       "25            0.960688            0.956081            0.966523   \n",
       "26            0.959459            0.957310            0.965909   \n",
       "27            0.954238            0.953624            0.963759   \n",
       "28            0.956695            0.952088            0.965602   \n",
       "29            0.953010            0.950553            0.960688   \n",
       "30            0.945332            0.947789            0.955160   \n",
       "31            0.944410            0.948096            0.955160   \n",
       "32            0.943182            0.944103            0.952088   \n",
       "33            0.942875            0.943796            0.950860   \n",
       "34            0.945946            0.943182            0.952396   \n",
       "35            0.940111            0.936732            0.944717   \n",
       "36            0.944410            0.943489            0.950246   \n",
       "37            0.940725            0.940418            0.949324   \n",
       "38            0.939496            0.939189            0.948403   \n",
       "39            0.935197            0.933661            0.942875   \n",
       "40            0.939496            0.935811            0.944410   \n",
       "41            0.939189            0.935197            0.944103   \n",
       "42            0.937039            0.938268            0.944103   \n",
       "43            0.937654            0.937346            0.945332   \n",
       "44            0.936425            0.937961            0.943796   \n",
       "45            0.934275            0.934582            0.940111   \n",
       "46            0.933661            0.929668            0.936425   \n",
       "47            0.930897            0.932125            0.934889   \n",
       "48            0.930283            0.931818            0.934582   \n",
       "49            0.926290            0.928440            0.930897   \n",
       "50            0.913391            0.916769            0.922912   \n",
       "51            0.910627            0.904177            0.921990   \n",
       "52            0.908784            0.910319            0.919533   \n",
       "53            0.904484            0.905713            0.917998   \n",
       "54            0.905098            0.898956            0.914005   \n",
       "55            0.889742            0.891585            0.902948   \n",
       "56            0.889742            0.891278            0.903256   \n",
       "57            0.885442            0.884521            0.900799   \n",
       "58            0.861794            0.857494            0.875614   \n",
       "59            0.855651            0.850737            0.869472   \n",
       "\n",
       "    split4_test_recall  mean_test_recall  std_test_recall  rank_test_recall  \\\n",
       "0             0.991705          0.991646         0.001204                 1   \n",
       "1             0.990783          0.989926         0.000595                 3   \n",
       "2             0.988633          0.987714         0.001883                 6   \n",
       "3             0.987404          0.986916         0.001268                 7   \n",
       "4             0.988940          0.988451         0.001487                 5   \n",
       "5             0.987097          0.986731         0.001309                 8   \n",
       "6             0.989862          0.989066         0.001109                 4   \n",
       "7             0.990783          0.990049         0.000791                 2   \n",
       "8             0.986175          0.986056         0.001765                 9   \n",
       "9             0.983717          0.985257         0.001844                12   \n",
       "10            0.983410          0.984950         0.002137                18   \n",
       "11            0.983717          0.985318         0.002114                11   \n",
       "12            0.984025          0.985318         0.001631                10   \n",
       "13            0.983717          0.985257         0.001844                12   \n",
       "14            0.983103          0.985257         0.002283                14   \n",
       "15            0.984025          0.985134         0.001538                16   \n",
       "16            0.983410          0.985134         0.002134                17   \n",
       "17            0.983103          0.985196         0.002219                15   \n",
       "18            0.982181          0.984520         0.002401                20   \n",
       "19            0.982796          0.984458         0.002246                21   \n",
       "20            0.982796          0.984458         0.002246                21   \n",
       "21            0.982488          0.984581         0.002294                19   \n",
       "22            0.982796          0.984397         0.002159                23   \n",
       "23            0.981260          0.984028         0.002556                24   \n",
       "24            0.962519          0.962283         0.003504                26   \n",
       "25            0.962826          0.962590         0.003987                25   \n",
       "26            0.962519          0.962160         0.003372                27   \n",
       "27            0.958525          0.958413         0.004031                29   \n",
       "28            0.960061          0.959580         0.004815                28   \n",
       "29            0.955146          0.955464         0.003566                30   \n",
       "30            0.951152          0.950488         0.003536                31   \n",
       "31            0.950230          0.950058         0.003668                32   \n",
       "32            0.948387          0.946864         0.003190                34   \n",
       "33            0.946544          0.945943         0.002780                35   \n",
       "34            0.947158          0.947601         0.003112                33   \n",
       "35            0.940092          0.940168         0.002587                39   \n",
       "36            0.946237          0.945758         0.002415                36   \n",
       "37            0.945008          0.943916         0.003253                37   \n",
       "38            0.944393          0.942871         0.003402                38   \n",
       "39            0.936713          0.937158         0.003130                45   \n",
       "40            0.941321          0.939554         0.003120                42   \n",
       "41            0.939478          0.938694         0.003241                44   \n",
       "42            0.938556          0.939861         0.002545                41   \n",
       "43            0.939478          0.940045         0.002878                40   \n",
       "44            0.937942          0.939431         0.002647                43   \n",
       "45            0.931490          0.935684         0.003019                46   \n",
       "46            0.933333          0.933288         0.002149                47   \n",
       "47            0.926575          0.931937         0.003137                48   \n",
       "48            0.926575          0.931629         0.003058                49   \n",
       "49            0.924731          0.928374         0.002601                50   \n",
       "50            0.913671          0.917685         0.003967                51   \n",
       "51            0.909370          0.911419         0.005818                53   \n",
       "52            0.909985          0.913078         0.004264                52   \n",
       "53            0.905991          0.909454         0.005232                54   \n",
       "54            0.901997          0.906136         0.005511                55   \n",
       "55            0.890323          0.894158         0.004944                56   \n",
       "56            0.890015          0.894035         0.005111                57   \n",
       "57            0.883564          0.889059         0.006408                58   \n",
       "58            0.865438          0.864857         0.006008                59   \n",
       "59            0.859601          0.858161         0.006314                60   \n",
       "\n",
       "    split0_test_f1  split1_test_f1  split2_test_f1  split3_test_f1  \\\n",
       "0         0.965404        0.966302        0.969852        0.966951   \n",
       "1         0.965982        0.966097        0.969761        0.965559   \n",
       "2         0.967267        0.965060        0.969122        0.966231   \n",
       "3         0.967083        0.965663        0.968976        0.965870   \n",
       "4         0.966552        0.965839        0.969752        0.965217   \n",
       "5         0.967228        0.965652        0.968967        0.965725   \n",
       "6         0.966572        0.966035        0.969606        0.964783   \n",
       "7         0.965094        0.966097        0.969487        0.965724   \n",
       "8         0.967538        0.964883        0.968637        0.965704   \n",
       "9         0.967354        0.964862        0.968637        0.965860   \n",
       "10        0.967334        0.964113        0.969259        0.965237   \n",
       "11        0.967664        0.964571        0.968811        0.965092   \n",
       "12        0.967053        0.964873        0.968345        0.965714   \n",
       "13        0.967208        0.964717        0.968637        0.965714   \n",
       "14        0.967664        0.964571        0.969259        0.964801   \n",
       "15        0.967053        0.964873        0.968335        0.965548   \n",
       "16        0.967810        0.964415        0.968354        0.965102   \n",
       "17        0.967664        0.964571        0.968811        0.964801   \n",
       "18        0.967324        0.963365        0.968957        0.965247   \n",
       "19        0.967033        0.963499        0.968646        0.965247   \n",
       "20        0.967033        0.963499        0.968646        0.965247   \n",
       "21        0.967198        0.963510        0.967907        0.965403   \n",
       "22        0.966887        0.963209        0.967606        0.965258   \n",
       "23        0.966887        0.963052        0.968199        0.965247   \n",
       "24        0.959134        0.956668        0.957604        0.958854   \n",
       "25        0.959464        0.956868        0.956816        0.959159   \n",
       "26        0.958829        0.956668        0.957604        0.958695   \n",
       "27        0.958384        0.954678        0.956415        0.958607   \n",
       "28        0.958302        0.955082        0.955316        0.959561   \n",
       "29        0.956455        0.954769        0.955394        0.957893   \n",
       "30        0.954622        0.952057        0.954827        0.957070   \n",
       "31        0.954154        0.951423        0.955580        0.957218   \n",
       "32        0.951969        0.951363        0.954510        0.956052   \n",
       "33        0.951777        0.950906        0.954644        0.955851   \n",
       "34        0.951663        0.950177        0.950774        0.953274   \n",
       "35        0.949247        0.949884        0.952084        0.953651   \n",
       "36        0.949367        0.950247        0.951231        0.953026   \n",
       "37        0.949791        0.948738        0.949899        0.953130   \n",
       "38        0.949289        0.948527        0.949395        0.953381   \n",
       "39        0.948857        0.948155        0.950446        0.952824   \n",
       "40        0.947058        0.949853        0.948482        0.952455   \n",
       "41        0.946551        0.949395        0.948303        0.952292   \n",
       "42        0.948623        0.947221        0.949643        0.951408   \n",
       "43        0.948281        0.947548        0.948857        0.951910   \n",
       "44        0.948607        0.946747        0.949627        0.951540   \n",
       "45        0.947711        0.946926        0.949602        0.951064   \n",
       "46        0.946139        0.947040        0.947418        0.948957   \n",
       "47        0.946534        0.945562        0.949327        0.949174   \n",
       "48        0.946517        0.945528        0.949312        0.949158   \n",
       "49        0.945596        0.943384        0.948243        0.947632   \n",
       "50        0.942377        0.937874        0.943277        0.944671   \n",
       "51        0.939500        0.937698        0.938177        0.945810   \n",
       "52        0.940454        0.936096        0.940654        0.943141   \n",
       "53        0.939040        0.933735        0.938425        0.942754   \n",
       "54        0.937994        0.935110        0.934696        0.941176   \n",
       "55        0.932268        0.927040        0.932691        0.936008   \n",
       "56        0.932098        0.927040        0.932369        0.936326   \n",
       "57        0.930255        0.925373        0.928583        0.934970   \n",
       "58        0.917332        0.914602        0.914510        0.923700   \n",
       "59        0.913264        0.911500        0.910735        0.921549   \n",
       "\n",
       "    split4_test_f1  mean_test_f1  std_test_f1  rank_test_f1  \n",
       "0         0.967626      0.967227     0.001504             1  \n",
       "1         0.967887      0.967057     0.001570             2  \n",
       "2         0.967383      0.967013     0.001347             3  \n",
       "3         0.967198      0.966958     0.001184             4  \n",
       "4         0.967393      0.966951     0.001577             5  \n",
       "5         0.966897      0.966894     0.001210             6  \n",
       "6         0.967422      0.966884     0.001608             7  \n",
       "7         0.967742      0.966829     0.001592             8  \n",
       "8         0.966867      0.966726     0.001325             9  \n",
       "9         0.966204      0.966583     0.001299            10  \n",
       "10        0.966631      0.966515     0.001767            11  \n",
       "11        0.966350      0.966498     0.001576            12  \n",
       "12        0.966360      0.966469     0.001182            13  \n",
       "13        0.966058      0.966467     0.001346            14  \n",
       "14        0.966038      0.966467     0.001776            15  \n",
       "15        0.966214      0.966405     0.001205            16  \n",
       "16        0.966340      0.966404     0.001512            17  \n",
       "17        0.966038      0.966377     0.001640            18  \n",
       "18        0.965715      0.966122     0.001898            19  \n",
       "19        0.966027      0.966091     0.001723            20  \n",
       "20        0.966027      0.966091     0.001723            20  \n",
       "21        0.966017      0.966007     0.001525            22  \n",
       "22        0.966465      0.965885     0.001540            23  \n",
       "23        0.965538      0.965785     0.001724            24  \n",
       "24        0.959571      0.958366     0.001072            25  \n",
       "25        0.959437      0.958348     0.001235            26  \n",
       "26        0.959571      0.958273     0.001019            27  \n",
       "27        0.959262      0.957469     0.001688            28  \n",
       "28        0.959030      0.957458     0.001889            29  \n",
       "29        0.957794      0.956461     0.001251            30  \n",
       "30        0.957032      0.955122     0.001854            31  \n",
       "31        0.956697      0.955014     0.002080            32  \n",
       "32        0.956616      0.954102     0.002114            33  \n",
       "33        0.955645      0.953765     0.002039            34  \n",
       "34        0.954046      0.951987     0.001466            35  \n",
       "35        0.952826      0.951538     0.001697            36  \n",
       "36        0.952528      0.951280     0.001366            37  \n",
       "37        0.952912      0.950894     0.001785            38  \n",
       "38        0.952588      0.950636     0.001957            39  \n",
       "39        0.952217      0.950500     0.001819            40  \n",
       "40        0.952588      0.950087     0.002176            41  \n",
       "41        0.951907      0.949689     0.002170            42  \n",
       "42        0.950825      0.949544     0.001508            43  \n",
       "43        0.951019      0.949523     0.001662            44  \n",
       "44        0.950498      0.949404     0.001643            45  \n",
       "45        0.947796      0.948620     0.001504            46  \n",
       "46        0.949969      0.947905     0.001376            47  \n",
       "47        0.946493      0.947418     0.001537            48  \n",
       "48        0.946345      0.947372     0.001558            49  \n",
       "49        0.945946      0.946160     0.001707            50  \n",
       "50        0.941735      0.941987     0.002280            51  \n",
       "51        0.940877      0.940412     0.002917            52  \n",
       "52        0.940915      0.940252     0.002292            53  \n",
       "53        0.938724      0.938535     0.002868            54  \n",
       "54        0.936523      0.937100     0.002345            55  \n",
       "55        0.931084      0.931818     0.002894            56  \n",
       "56        0.931062      0.931779     0.002970            57  \n",
       "57        0.927892      0.929414     0.003191            58  \n",
       "58        0.918787      0.917786     0.003378            59  \n",
       "59        0.915726      0.914555     0.003896            60  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>2.699475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.180762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.066927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.0132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_class_weight</th>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_n_estimators</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 76}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.942122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.942999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.947816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_precision</th>\n",
       "      <td>0.942291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_precision</th>\n",
       "      <td>0.944688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.943983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.00212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.989865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.990786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.992936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_recall</th>\n",
       "      <td>0.992936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_recall</th>\n",
       "      <td>0.991705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.991646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.001204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.965404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.966302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.969852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_f1</th>\n",
       "      <td>0.966951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_f1</th>\n",
       "      <td>0.967626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.967227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.001504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            0\n",
       "mean_fit_time                                                        2.699475\n",
       "std_fit_time                                                         0.180762\n",
       "mean_score_time                                                      0.066927\n",
       "std_score_time                                                         0.0132\n",
       "param_class_weight                                           {0: 1.0, 1: 1.0}\n",
       "param_n_estimators                                                         76\n",
       "params                 {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 76}\n",
       "split0_test_precision                                                0.942122\n",
       "split1_test_precision                                                0.942999\n",
       "split2_test_precision                                                0.947816\n",
       "split3_test_precision                                                0.942291\n",
       "split4_test_precision                                                0.944688\n",
       "mean_test_precision                                                  0.943983\n",
       "std_test_precision                                                    0.00212\n",
       "rank_test_precision                                                        60\n",
       "split0_test_recall                                                   0.989865\n",
       "split1_test_recall                                                   0.990786\n",
       "split2_test_recall                                                   0.992936\n",
       "split3_test_recall                                                   0.992936\n",
       "split4_test_recall                                                   0.991705\n",
       "mean_test_recall                                                     0.991646\n",
       "std_test_recall                                                      0.001204\n",
       "rank_test_recall                                                            1\n",
       "split0_test_f1                                                       0.965404\n",
       "split1_test_f1                                                       0.966302\n",
       "split2_test_f1                                                       0.969852\n",
       "split3_test_f1                                                       0.966951\n",
       "split4_test_f1                                                       0.967626\n",
       "mean_test_f1                                                         0.967227\n",
       "std_test_f1                                                          0.001504\n",
       "rank_test_f1                                                                1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                          0\n",
      "params               {'class_weight': {0: 1.0, 1: 1.0}, 'n_estimators': 76}\n",
      "mean_test_precision                                                0.943983\n",
      "mean_test_recall                                                   0.991646\n",
      "mean_test_f1                                                       0.967227\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "lgbm_cvdf = pd.DataFrame(lgb_cv.cv_results_).sort_values(by=['rank_test_f1', 'rank_test_recall','rank_test_precision']).reset_index(drop=True)\n",
    "lgbm_best = pd.DataFrame(lgbm_cvdf.iloc[0,:])\n",
    "display(lgbm_cvdf)\n",
    "display(lgbm_best)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(lgbm_best.loc[['params', 'mean_test_precision','mean_test_recall','mean_test_f1'],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc83fec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=4688; f1: (test=0.946) precision: (test=0.947) recall: (test=0.945) total time=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=4688; f1: (test=0.950) precision: (test=0.949) recall: (test=0.950) total time=   6.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=4688; f1: (test=0.951) precision: (test=0.949) recall: (test=0.953) total time=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=4688; f1: (test=0.954) precision: (test=0.949) recall: (test=0.959) total time=   8.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=4688; f1: (test=0.945) precision: (test=0.949) recall: (test=0.941) total time=   9.2s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=3677; f1: (test=0.965) precision: (test=0.935) recall: (test=0.998) total time=   0.1s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=3677; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.2s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=3677; f1: (test=0.966) precision: (test=0.936) recall: (test=0.998) total time=   0.2s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=3677; f1: (test=0.965) precision: (test=0.933) recall: (test=0.999) total time=   0.1s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=3677; f1: (test=0.964) precision: (test=0.934) recall: (test=0.997) total time=   0.2s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=3962; f1: (test=0.954) precision: (test=0.955) recall: (test=0.953) total time=   0.5s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=3962; f1: (test=0.959) precision: (test=0.957) recall: (test=0.961) total time=   0.7s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=3962; f1: (test=0.955) precision: (test=0.960) recall: (test=0.951) total time=   0.9s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=3962; f1: (test=0.957) precision: (test=0.956) recall: (test=0.958) total time=   0.9s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=3962; f1: (test=0.953) precision: (test=0.957) recall: (test=0.948) total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=3362; f1: (test=0.947) precision: (test=0.947) recall: (test=0.946) total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=3362; f1: (test=0.950) precision: (test=0.949) recall: (test=0.951) total time=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=3362; f1: (test=0.951) precision: (test=0.949) recall: (test=0.953) total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=3362; f1: (test=0.954) precision: (test=0.949) recall: (test=0.959) total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=3362; f1: (test=0.945) precision: (test=0.949) recall: (test=0.940) total time=   9.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2807; f1: (test=0.962) precision: (test=0.942) recall: (test=0.984) total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2807; f1: (test=0.962) precision: (test=0.943) recall: (test=0.982) total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2807; f1: (test=0.964) precision: (test=0.945) recall: (test=0.984) total time=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2807; f1: (test=0.963) precision: (test=0.943) recall: (test=0.983) total time=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2807; f1: (test=0.959) precision: (test=0.944) recall: (test=0.975) total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=3688; f1: (test=0.953) precision: (test=0.946) recall: (test=0.959) total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=3688; f1: (test=0.955) precision: (test=0.949) recall: (test=0.962) total time=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=3688; f1: (test=0.956) precision: (test=0.950) recall: (test=0.962) total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=3688; f1: (test=0.957) precision: (test=0.948) recall: (test=0.966) total time=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=3688; f1: (test=0.951) precision: (test=0.947) recall: (test=0.955) total time=   7.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1494; f1: (test=0.949) precision: (test=0.947) recall: (test=0.950) total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1494; f1: (test=0.950) precision: (test=0.949) recall: (test=0.951) total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1494; f1: (test=0.952) precision: (test=0.949) recall: (test=0.954) total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1494; f1: (test=0.953) precision: (test=0.949) recall: (test=0.958) total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1494; f1: (test=0.946) precision: (test=0.948) recall: (test=0.944) total time=   2.8s\n",
      "[CV 1/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=3227; f1: (test=0.953) precision: (test=0.950) recall: (test=0.956) total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=3227; f1: (test=0.955) precision: (test=0.951) recall: (test=0.958) total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=3227; f1: (test=0.955) precision: (test=0.952) recall: (test=0.957) total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=3227; f1: (test=0.952) precision: (test=0.950) recall: (test=0.954) total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=3227; f1: (test=0.948) precision: (test=0.950) recall: (test=0.946) total time=   6.1s\n",
      "[CV 1/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=4544; f1: (test=0.953) precision: (test=0.950) recall: (test=0.956) total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=4544; f1: (test=0.955) precision: (test=0.951) recall: (test=0.958) total time=   8.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=4544; f1: (test=0.955) precision: (test=0.952) recall: (test=0.957) total time=   7.9s\n",
      "[CV 4/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=4544; f1: (test=0.952) precision: (test=0.950) recall: (test=0.954) total time=   7.7s\n",
      "[CV 5/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=4544; f1: (test=0.948) precision: (test=0.950) recall: (test=0.946) total time=   8.0s\n",
      "[CV 1/5] END C=0.1, class_weight=balanced, max_iter=3053; f1: (test=0.913) precision: (test=0.974) recall: (test=0.859) total time=   0.7s\n",
      "[CV 2/5] END C=0.1, class_weight=balanced, max_iter=3053; f1: (test=0.915) precision: (test=0.971) recall: (test=0.865) total time=   0.9s\n",
      "[CV 3/5] END C=0.1, class_weight=balanced, max_iter=3053; f1: (test=0.904) precision: (test=0.974) recall: (test=0.844) total time=   0.8s\n",
      "[CV 4/5] END C=0.1, class_weight=balanced, max_iter=3053; f1: (test=0.921) precision: (test=0.976) recall: (test=0.871) total time=   0.7s\n",
      "[CV 5/5] END C=0.1, class_weight=balanced, max_iter=3053; f1: (test=0.918) precision: (test=0.977) recall: (test=0.866) total time=   0.8s\n",
      "[CV 1/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=4048; f1: (test=0.966) precision: (test=0.940) recall: (test=0.994) total time=   0.8s\n",
      "[CV 2/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=4048; f1: (test=0.965) precision: (test=0.943) recall: (test=0.988) total time=   1.2s\n",
      "[CV 3/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=4048; f1: (test=0.967) precision: (test=0.943) recall: (test=0.992) total time=   1.0s\n",
      "[CV 4/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=4048; f1: (test=0.966) precision: (test=0.941) recall: (test=0.991) total time=   1.3s\n",
      "[CV 5/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=4048; f1: (test=0.966) precision: (test=0.943) recall: (test=0.989) total time=   1.0s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=1277; f1: (test=0.936) precision: (test=0.965) recall: (test=0.908) total time=   0.8s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=1277; f1: (test=0.937) precision: (test=0.965) recall: (test=0.910) total time=   1.2s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=1277; f1: (test=0.929) precision: (test=0.967) recall: (test=0.893) total time=   1.2s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=1277; f1: (test=0.937) precision: (test=0.969) recall: (test=0.907) total time=   1.2s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=1277; f1: (test=0.935) precision: (test=0.969) recall: (test=0.904) total time=   1.2s\n",
      "[CV 1/5] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=3906; f1: (test=0.957) precision: (test=0.947) recall: (test=0.968) total time=   1.7s\n",
      "[CV 2/5] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=3906; f1: (test=0.959) precision: (test=0.949) recall: (test=0.969) total time=   5.6s\n",
      "[CV 3/5] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=3906; f1: (test=0.960) precision: (test=0.951) recall: (test=0.969) total time=   4.1s\n",
      "[CV 4/5] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=3906; f1: (test=0.958) precision: (test=0.948) recall: (test=0.969) total time=   4.6s\n",
      "[CV 5/5] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=3906; f1: (test=0.954) precision: (test=0.949) recall: (test=0.960) total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2798; f1: (test=0.962) precision: (test=0.942) recall: (test=0.984) total time=   4.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2798; f1: (test=0.962) precision: (test=0.943) recall: (test=0.982) total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2798; f1: (test=0.964) precision: (test=0.945) recall: (test=0.984) total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2798; f1: (test=0.963) precision: (test=0.943) recall: (test=0.983) total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2798; f1: (test=0.959) precision: (test=0.944) recall: (test=0.975) total time=   3.9s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1093; f1: (test=0.965) precision: (test=0.935) recall: (test=0.998) total time=   0.1s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1093; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.1s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1093; f1: (test=0.966) precision: (test=0.936) recall: (test=0.998) total time=   0.1s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1093; f1: (test=0.965) precision: (test=0.933) recall: (test=0.999) total time=   0.1s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1093; f1: (test=0.964) precision: (test=0.934) recall: (test=0.997) total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1030; f1: (test=0.953) precision: (test=0.950) recall: (test=0.956) total time=   1.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1030; f1: (test=0.955) precision: (test=0.951) recall: (test=0.958) total time=   1.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1030; f1: (test=0.955) precision: (test=0.952) recall: (test=0.957) total time=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1030; f1: (test=0.952) precision: (test=0.950) recall: (test=0.954) total time=   1.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1030; f1: (test=0.948) precision: (test=0.950) recall: (test=0.946) total time=   2.0s\n",
      "[CV 1/5] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2930; f1: (test=0.957) precision: (test=0.947) recall: (test=0.968) total time=   1.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2930; f1: (test=0.959) precision: (test=0.949) recall: (test=0.969) total time=   5.1s\n",
      "[CV 3/5] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2930; f1: (test=0.960) precision: (test=0.951) recall: (test=0.969) total time=   4.2s\n",
      "[CV 4/5] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2930; f1: (test=0.958) precision: (test=0.948) recall: (test=0.969) total time=   4.5s\n",
      "[CV 5/5] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2930; f1: (test=0.954) precision: (test=0.949) recall: (test=0.960) total time=   4.7s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=4354; f1: (test=0.954) precision: (test=0.955) recall: (test=0.953) total time=   0.4s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=4354; f1: (test=0.959) precision: (test=0.957) recall: (test=0.961) total time=   0.6s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=4354; f1: (test=0.955) precision: (test=0.960) recall: (test=0.951) total time=   0.7s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=4354; f1: (test=0.957) precision: (test=0.956) recall: (test=0.958) total time=   0.6s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=4354; f1: (test=0.953) precision: (test=0.957) recall: (test=0.948) total time=   0.6s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=3610; f1: (test=0.936) precision: (test=0.965) recall: (test=0.908) total time=   0.7s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=3610; f1: (test=0.937) precision: (test=0.965) recall: (test=0.910) total time=   1.2s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=3610; f1: (test=0.929) precision: (test=0.967) recall: (test=0.893) total time=   1.2s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=3610; f1: (test=0.937) precision: (test=0.969) recall: (test=0.907) total time=   1.2s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=3610; f1: (test=0.935) precision: (test=0.969) recall: (test=0.904) total time=   1.2s\n",
      "[CV 1/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=4598; f1: (test=0.962) precision: (test=0.942) recall: (test=0.984) total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=4598; f1: (test=0.962) precision: (test=0.943) recall: (test=0.982) total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=4598; f1: (test=0.964) precision: (test=0.945) recall: (test=0.984) total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=4598; f1: (test=0.963) precision: (test=0.943) recall: (test=0.983) total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=4598; f1: (test=0.959) precision: (test=0.944) recall: (test=0.975) total time=   7.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight=balanced, max_iter=1312; f1: (test=0.950) precision: (test=0.948) recall: (test=0.952) total time=   2.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight=balanced, max_iter=1312; f1: (test=0.952) precision: (test=0.948) recall: (test=0.956) total time=   2.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight=balanced, max_iter=1312; f1: (test=0.953) precision: (test=0.950) recall: (test=0.956) total time=   2.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight=balanced, max_iter=1312; f1: (test=0.954) precision: (test=0.948) recall: (test=0.959) total time=   2.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight=balanced, max_iter=1312; f1: (test=0.948) precision: (test=0.949) recall: (test=0.948) total time=   2.3s\n",
      "[CV 1/5] END C=0.1, class_weight=balanced, max_iter=4452; f1: (test=0.913) precision: (test=0.974) recall: (test=0.859) total time=   0.7s\n",
      "[CV 2/5] END C=0.1, class_weight=balanced, max_iter=4452; f1: (test=0.915) precision: (test=0.971) recall: (test=0.865) total time=   0.7s\n",
      "[CV 3/5] END C=0.1, class_weight=balanced, max_iter=4452; f1: (test=0.904) precision: (test=0.974) recall: (test=0.844) total time=   0.7s\n",
      "[CV 4/5] END C=0.1, class_weight=balanced, max_iter=4452; f1: (test=0.921) precision: (test=0.976) recall: (test=0.871) total time=   0.7s\n",
      "[CV 5/5] END C=0.1, class_weight=balanced, max_iter=4452; f1: (test=0.918) precision: (test=0.977) recall: (test=0.866) total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1650; f1: (test=0.953) precision: (test=0.950) recall: (test=0.956) total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1650; f1: (test=0.955) precision: (test=0.951) recall: (test=0.958) total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1650; f1: (test=0.955) precision: (test=0.952) recall: (test=0.958) total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1650; f1: (test=0.952) precision: (test=0.950) recall: (test=0.954) total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1650; f1: (test=0.948) precision: (test=0.950) recall: (test=0.946) total time=   3.4s\n",
      "[CV 1/5] END C=0.1, class_weight=balanced, max_iter=3711; f1: (test=0.913) precision: (test=0.974) recall: (test=0.859) total time=   0.7s\n",
      "[CV 2/5] END C=0.1, class_weight=balanced, max_iter=3711; f1: (test=0.915) precision: (test=0.971) recall: (test=0.865) total time=   1.3s\n",
      "[CV 3/5] END C=0.1, class_weight=balanced, max_iter=3711; f1: (test=0.904) precision: (test=0.974) recall: (test=0.844) total time=   0.8s\n",
      "[CV 4/5] END C=0.1, class_weight=balanced, max_iter=3711; f1: (test=0.921) precision: (test=0.976) recall: (test=0.871) total time=   0.7s\n",
      "[CV 5/5] END C=0.1, class_weight=balanced, max_iter=3711; f1: (test=0.918) precision: (test=0.977) recall: (test=0.866) total time=   0.8s\n",
      "[CV 1/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1928; f1: (test=0.966) precision: (test=0.940) recall: (test=0.994) total time=   0.7s\n",
      "[CV 2/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1928; f1: (test=0.965) precision: (test=0.943) recall: (test=0.988) total time=   1.3s\n",
      "[CV 3/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1928; f1: (test=0.967) precision: (test=0.943) recall: (test=0.992) total time=   1.2s\n",
      "[CV 4/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1928; f1: (test=0.966) precision: (test=0.941) recall: (test=0.991) total time=   1.2s\n",
      "[CV 5/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1928; f1: (test=0.966) precision: (test=0.943) recall: (test=0.989) total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2642; f1: (test=0.953) precision: (test=0.946) recall: (test=0.959) total time=   4.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2642; f1: (test=0.955) precision: (test=0.949) recall: (test=0.962) total time=   4.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2642; f1: (test=0.956) precision: (test=0.950) recall: (test=0.962) total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2642; f1: (test=0.957) precision: (test=0.948) recall: (test=0.966) total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2642; f1: (test=0.951) precision: (test=0.947) recall: (test=0.955) total time=   4.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2205; f1: (test=0.953) precision: (test=0.946) recall: (test=0.959) total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2205; f1: (test=0.955) precision: (test=0.949) recall: (test=0.962) total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2205; f1: (test=0.956) precision: (test=0.950) recall: (test=0.962) total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2205; f1: (test=0.957) precision: (test=0.948) recall: (test=0.967) total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2205; f1: (test=0.951) precision: (test=0.947) recall: (test=0.955) total time=   3.4s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2386; f1: (test=0.965) precision: (test=0.935) recall: (test=0.998) total time=   0.1s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2386; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.1s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2386; f1: (test=0.966) precision: (test=0.936) recall: (test=0.998) total time=   0.1s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2386; f1: (test=0.965) precision: (test=0.933) recall: (test=0.999) total time=   0.1s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2386; f1: (test=0.964) precision: (test=0.934) recall: (test=0.997) total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2850; f1: (test=0.962) precision: (test=0.942) recall: (test=0.984) total time=   4.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2850; f1: (test=0.962) precision: (test=0.943) recall: (test=0.982) total time=   4.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2850; f1: (test=0.964) precision: (test=0.945) recall: (test=0.984) total time=   4.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2850; f1: (test=0.963) precision: (test=0.943) recall: (test=0.983) total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2850; f1: (test=0.959) precision: (test=0.944) recall: (test=0.975) total time=   4.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=4395; f1: (test=0.953) precision: (test=0.946) recall: (test=0.959) total time=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=4395; f1: (test=0.955) precision: (test=0.949) recall: (test=0.961) total time=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=4395; f1: (test=0.956) precision: (test=0.949) recall: (test=0.963) total time=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=4395; f1: (test=0.957) precision: (test=0.948) recall: (test=0.966) total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=4395; f1: (test=0.951) precision: (test=0.947) recall: (test=0.955) total time=   6.5s\n",
      "[CV 1/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=4162; f1: (test=0.962) precision: (test=0.942) recall: (test=0.984) total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=4162; f1: (test=0.962) precision: (test=0.943) recall: (test=0.982) total time=   5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=4162; f1: (test=0.964) precision: (test=0.945) recall: (test=0.984) total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=4162; f1: (test=0.963) precision: (test=0.943) recall: (test=0.983) total time=   5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=4162; f1: (test=0.959) precision: (test=0.944) recall: (test=0.975) total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight=balanced, max_iter=4409; f1: (test=0.949) precision: (test=0.947) recall: (test=0.951) total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight=balanced, max_iter=4409; f1: (test=0.951) precision: (test=0.948) recall: (test=0.953) total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight=balanced, max_iter=4409; f1: (test=0.952) precision: (test=0.950) recall: (test=0.954) total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight=balanced, max_iter=4409; f1: (test=0.953) precision: (test=0.948) recall: (test=0.959) total time=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight=balanced, max_iter=4409; f1: (test=0.948) precision: (test=0.949) recall: (test=0.948) total time=   6.6s\n",
      "[CV 1/5] END C=1, class_weight=balanced, max_iter=3289; f1: (test=0.951) precision: (test=0.951) recall: (test=0.951) total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=1, class_weight=balanced, max_iter=3289; f1: (test=0.954) precision: (test=0.954) recall: (test=0.955) total time=   6.6s\n",
      "[CV 3/5] END C=1, class_weight=balanced, max_iter=3289; f1: (test=0.952) precision: (test=0.952) recall: (test=0.952) total time=   5.8s\n",
      "[CV 4/5] END C=1, class_weight=balanced, max_iter=3289; f1: (test=0.951) precision: (test=0.952) recall: (test=0.950) total time=   5.5s\n",
      "[CV 5/5] END C=1, class_weight=balanced, max_iter=3289; f1: (test=0.948) precision: (test=0.953) recall: (test=0.943) total time=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1664; f1: (test=0.946) precision: (test=0.948) recall: (test=0.943) total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1664; f1: (test=0.951) precision: (test=0.949) recall: (test=0.952) total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1664; f1: (test=0.951) precision: (test=0.949) recall: (test=0.954) total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1664; f1: (test=0.954) precision: (test=0.949) recall: (test=0.959) total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1664; f1: (test=0.946) precision: (test=0.948) recall: (test=0.943) total time=   3.0s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=3319; f1: (test=0.936) precision: (test=0.965) recall: (test=0.908) total time=   0.8s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=3319; f1: (test=0.937) precision: (test=0.965) recall: (test=0.910) total time=   1.1s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=3319; f1: (test=0.929) precision: (test=0.967) recall: (test=0.893) total time=   1.2s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=3319; f1: (test=0.937) precision: (test=0.969) recall: (test=0.907) total time=   1.2s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=3319; f1: (test=0.935) precision: (test=0.969) recall: (test=0.904) total time=   1.4s\n",
      "[CV 1/5] END C=1, class_weight=balanced, max_iter=4134; f1: (test=0.951) precision: (test=0.951) recall: (test=0.951) total time=   1.8s\n",
      "[CV 2/5] END C=1, class_weight=balanced, max_iter=4134; f1: (test=0.954) precision: (test=0.954) recall: (test=0.955) total time=   7.0s\n",
      "[CV 3/5] END C=1, class_weight=balanced, max_iter=4134; f1: (test=0.952) precision: (test=0.952) recall: (test=0.952) total time=   6.4s\n",
      "[CV 4/5] END C=1, class_weight=balanced, max_iter=4134; f1: (test=0.951) precision: (test=0.952) recall: (test=0.950) total time=   5.5s\n",
      "[CV 5/5] END C=1, class_weight=balanced, max_iter=4134; f1: (test=0.948) precision: (test=0.953) recall: (test=0.943) total time=   5.6s\n",
      "[CV 1/5] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2160; f1: (test=0.957) precision: (test=0.947) recall: (test=0.968) total time=   1.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2160; f1: (test=0.959) precision: (test=0.949) recall: (test=0.968) total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2160; f1: (test=0.960) precision: (test=0.951) recall: (test=0.969) total time=   4.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2160; f1: (test=0.958) precision: (test=0.948) recall: (test=0.969) total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2160; f1: (test=0.954) precision: (test=0.949) recall: (test=0.960) total time=   4.0s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=4454; f1: (test=0.965) precision: (test=0.935) recall: (test=0.998) total time=   0.1s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=4454; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.1s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=4454; f1: (test=0.966) precision: (test=0.936) recall: (test=0.998) total time=   0.2s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=4454; f1: (test=0.965) precision: (test=0.933) recall: (test=0.999) total time=   0.1s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=4454; f1: (test=0.964) precision: (test=0.934) recall: (test=0.997) total time=   0.1s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=3911; f1: (test=0.936) precision: (test=0.965) recall: (test=0.908) total time=   0.8s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=3911; f1: (test=0.937) precision: (test=0.965) recall: (test=0.910) total time=   1.2s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=3911; f1: (test=0.929) precision: (test=0.967) recall: (test=0.893) total time=   1.2s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=3911; f1: (test=0.937) precision: (test=0.969) recall: (test=0.907) total time=   1.2s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=3911; f1: (test=0.935) precision: (test=0.969) recall: (test=0.904) total time=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight=balanced, max_iter=3895; f1: (test=0.949) precision: (test=0.947) recall: (test=0.951) total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight=balanced, max_iter=3895; f1: (test=0.951) precision: (test=0.948) recall: (test=0.954) total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight=balanced, max_iter=3895; f1: (test=0.952) precision: (test=0.950) recall: (test=0.954) total time=   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight=balanced, max_iter=3895; f1: (test=0.953) precision: (test=0.948) recall: (test=0.959) total time=   5.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight=balanced, max_iter=3895; f1: (test=0.948) precision: (test=0.949) recall: (test=0.948) total time=   5.8s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2270; f1: (test=0.936) precision: (test=0.965) recall: (test=0.908) total time=   0.7s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2270; f1: (test=0.937) precision: (test=0.965) recall: (test=0.910) total time=   1.2s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2270; f1: (test=0.929) precision: (test=0.967) recall: (test=0.893) total time=   1.2s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2270; f1: (test=0.937) precision: (test=0.969) recall: (test=0.907) total time=   1.2s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2270; f1: (test=0.935) precision: (test=0.969) recall: (test=0.904) total time=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight=balanced, max_iter=2383; f1: (test=0.949) precision: (test=0.947) recall: (test=0.951) total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight=balanced, max_iter=2383; f1: (test=0.951) precision: (test=0.948) recall: (test=0.954) total time=   3.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight=balanced, max_iter=2383; f1: (test=0.952) precision: (test=0.949) recall: (test=0.954) total time=   3.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight=balanced, max_iter=2383; f1: (test=0.954) precision: (test=0.948) recall: (test=0.959) total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight=balanced, max_iter=2383; f1: (test=0.948) precision: (test=0.949) recall: (test=0.948) total time=   3.8s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2424; f1: (test=0.965) precision: (test=0.935) recall: (test=0.998) total time=   0.1s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2424; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.1s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2424; f1: (test=0.966) precision: (test=0.936) recall: (test=0.998) total time=   0.1s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2424; f1: (test=0.965) precision: (test=0.933) recall: (test=0.999) total time=   0.1s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2424; f1: (test=0.964) precision: (test=0.934) recall: (test=0.997) total time=   0.1s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=4946; f1: (test=0.954) precision: (test=0.955) recall: (test=0.953) total time=   0.5s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=4946; f1: (test=0.959) precision: (test=0.957) recall: (test=0.961) total time=   0.6s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=4946; f1: (test=0.955) precision: (test=0.960) recall: (test=0.951) total time=   0.6s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=4946; f1: (test=0.957) precision: (test=0.956) recall: (test=0.958) total time=   0.7s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=4946; f1: (test=0.953) precision: (test=0.957) recall: (test=0.948) total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=4392; f1: (test=0.953) precision: (test=0.946) recall: (test=0.959) total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=4392; f1: (test=0.955) precision: (test=0.949) recall: (test=0.961) total time=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=4392; f1: (test=0.956) precision: (test=0.950) recall: (test=0.962) total time=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=4392; f1: (test=0.957) precision: (test=0.948) recall: (test=0.966) total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=4392; f1: (test=0.951) precision: (test=0.947) recall: (test=0.955) total time=   6.4s\n",
      "[CV 1/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=3996; f1: (test=0.962) precision: (test=0.942) recall: (test=0.984) total time=   5.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=3996; f1: (test=0.962) precision: (test=0.943) recall: (test=0.982) total time=   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=3996; f1: (test=0.964) precision: (test=0.945) recall: (test=0.984) total time=   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=3996; f1: (test=0.963) precision: (test=0.943) recall: (test=0.983) total time=   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=3996; f1: (test=0.959) precision: (test=0.944) recall: (test=0.975) total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=3571; f1: (test=0.953) precision: (test=0.946) recall: (test=0.959) total time=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=3571; f1: (test=0.955) precision: (test=0.949) recall: (test=0.961) total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=3571; f1: (test=0.956) precision: (test=0.950) recall: (test=0.962) total time=   5.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=3571; f1: (test=0.957) precision: (test=0.948) recall: (test=0.966) total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=3571; f1: (test=0.951) precision: (test=0.947) recall: (test=0.955) total time=   5.3s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=2652; f1: (test=0.954) precision: (test=0.955) recall: (test=0.953) total time=   0.5s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=2652; f1: (test=0.959) precision: (test=0.957) recall: (test=0.961) total time=   0.6s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=2652; f1: (test=0.955) precision: (test=0.960) recall: (test=0.951) total time=   0.7s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=2652; f1: (test=0.957) precision: (test=0.956) recall: (test=0.958) total time=   0.7s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=2652; f1: (test=0.953) precision: (test=0.957) recall: (test=0.948) total time=   0.6s\n",
      "[CV 1/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2513; f1: (test=0.966) precision: (test=0.940) recall: (test=0.994) total time=   0.7s\n",
      "[CV 2/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2513; f1: (test=0.965) precision: (test=0.943) recall: (test=0.988) total time=   1.2s\n",
      "[CV 3/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2513; f1: (test=0.967) precision: (test=0.943) recall: (test=0.992) total time=   1.0s\n",
      "[CV 4/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2513; f1: (test=0.966) precision: (test=0.941) recall: (test=0.991) total time=   1.0s\n",
      "[CV 5/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2513; f1: (test=0.966) precision: (test=0.943) recall: (test=0.989) total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=4439; f1: (test=0.946) precision: (test=0.947) recall: (test=0.945) total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=4439; f1: (test=0.950) precision: (test=0.949) recall: (test=0.950) total time=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=4439; f1: (test=0.951) precision: (test=0.948) recall: (test=0.954) total time=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=4439; f1: (test=0.954) precision: (test=0.949) recall: (test=0.959) total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=4439; f1: (test=0.945) precision: (test=0.949) recall: (test=0.941) total time=   6.5s\n",
      "[CV 1/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=4264; f1: (test=0.966) precision: (test=0.940) recall: (test=0.994) total time=   0.7s\n",
      "[CV 2/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=4264; f1: (test=0.965) precision: (test=0.943) recall: (test=0.988) total time=   1.2s\n",
      "[CV 3/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=4264; f1: (test=0.967) precision: (test=0.943) recall: (test=0.992) total time=   1.0s\n",
      "[CV 4/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=4264; f1: (test=0.966) precision: (test=0.941) recall: (test=0.991) total time=   1.1s\n",
      "[CV 5/5] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=4264; f1: (test=0.966) precision: (test=0.943) recall: (test=0.989) total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1424; f1: (test=0.949) precision: (test=0.947) recall: (test=0.951) total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1424; f1: (test=0.950) precision: (test=0.949) recall: (test=0.951) total time=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1424; f1: (test=0.952) precision: (test=0.949) recall: (test=0.955) total time=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1424; f1: (test=0.954) precision: (test=0.949) recall: (test=0.959) total time=   2.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1424; f1: (test=0.948) precision: (test=0.948) recall: (test=0.947) total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=1596; f1: (test=0.953) precision: (test=0.946) recall: (test=0.959) total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=1596; f1: (test=0.956) precision: (test=0.949) recall: (test=0.962) total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=1596; f1: (test=0.956) precision: (test=0.950) recall: (test=0.963) total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=1596; f1: (test=0.957) precision: (test=0.948) recall: (test=0.966) total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=1596; f1: (test=0.951) precision: (test=0.947) recall: (test=0.955) total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2016; f1: (test=0.962) precision: (test=0.942) recall: (test=0.984) total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2016; f1: (test=0.962) precision: (test=0.943) recall: (test=0.982) total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2016; f1: (test=0.964) precision: (test=0.945) recall: (test=0.984) total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2016; f1: (test=0.963) precision: (test=0.943) recall: (test=0.983) total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2016; f1: (test=0.959) precision: (test=0.944) recall: (test=0.975) total time=   2.9s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=4039; f1: (test=0.954) precision: (test=0.955) recall: (test=0.953) total time=   0.5s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=4039; f1: (test=0.959) precision: (test=0.957) recall: (test=0.961) total time=   0.6s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=4039; f1: (test=0.955) precision: (test=0.960) recall: (test=0.951) total time=   0.7s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=4039; f1: (test=0.957) precision: (test=0.956) recall: (test=0.958) total time=   0.7s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=4039; f1: (test=0.953) precision: (test=0.957) recall: (test=0.948) total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight=balanced, max_iter=1779; f1: (test=0.949) precision: (test=0.947) recall: (test=0.951) total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight=balanced, max_iter=1779; f1: (test=0.952) precision: (test=0.948) recall: (test=0.955) total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight=balanced, max_iter=1779; f1: (test=0.952) precision: (test=0.949) recall: (test=0.955) total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight=balanced, max_iter=1779; f1: (test=0.954) precision: (test=0.948) recall: (test=0.959) total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight=balanced, max_iter=1779; f1: (test=0.949) precision: (test=0.949) recall: (test=0.949) total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2497; f1: (test=0.962) precision: (test=0.942) recall: (test=0.984) total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2497; f1: (test=0.962) precision: (test=0.943) recall: (test=0.982) total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2497; f1: (test=0.964) precision: (test=0.945) recall: (test=0.984) total time=   3.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2497; f1: (test=0.963) precision: (test=0.943) recall: (test=0.983) total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2497; f1: (test=0.959) precision: (test=0.944) recall: (test=0.975) total time=   3.6s\n",
      "[CV 1/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2319; f1: (test=0.936) precision: (test=0.965) recall: (test=0.908) total time=   0.7s\n",
      "[CV 2/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2319; f1: (test=0.937) precision: (test=0.965) recall: (test=0.910) total time=   1.2s\n",
      "[CV 3/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2319; f1: (test=0.929) precision: (test=0.967) recall: (test=0.893) total time=   1.2s\n",
      "[CV 4/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2319; f1: (test=0.937) precision: (test=0.969) recall: (test=0.907) total time=   1.2s\n",
      "[CV 5/5] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2319; f1: (test=0.935) precision: (test=0.969) recall: (test=0.904) total time=   1.2s\n",
      "[CV 1/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=3599; f1: (test=0.962) precision: (test=0.942) recall: (test=0.984) total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=3599; f1: (test=0.962) precision: (test=0.943) recall: (test=0.982) total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=3599; f1: (test=0.964) precision: (test=0.945) recall: (test=0.984) total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=3599; f1: (test=0.963) precision: (test=0.943) recall: (test=0.983) total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=3599; f1: (test=0.959) precision: (test=0.944) recall: (test=0.975) total time=   5.1s\n",
      "[CV 1/5] END C=1, class_weight=balanced, max_iter=4810; f1: (test=0.951) precision: (test=0.951) recall: (test=0.951) total time=   1.8s\n",
      "[CV 2/5] END C=1, class_weight=balanced, max_iter=4810; f1: (test=0.954) precision: (test=0.954) recall: (test=0.955) total time=   6.7s\n",
      "[CV 3/5] END C=1, class_weight=balanced, max_iter=4810; f1: (test=0.952) precision: (test=0.952) recall: (test=0.952) total time=   6.0s\n",
      "[CV 4/5] END C=1, class_weight=balanced, max_iter=4810; f1: (test=0.951) precision: (test=0.952) recall: (test=0.950) total time=   5.5s\n",
      "[CV 5/5] END C=1, class_weight=balanced, max_iter=4810; f1: (test=0.948) precision: (test=0.953) recall: (test=0.943) total time=   5.7s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_precision</th>\n",
       "      <th>split1_test_precision</th>\n",
       "      <th>split2_test_precision</th>\n",
       "      <th>split3_test_precision</th>\n",
       "      <th>split4_test_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>rank_test_precision</th>\n",
       "      <th>split0_test_recall</th>\n",
       "      <th>split1_test_recall</th>\n",
       "      <th>split2_test_recall</th>\n",
       "      <th>split3_test_recall</th>\n",
       "      <th>split4_test_recall</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>rank_test_recall</th>\n",
       "      <th>split0_test_f1</th>\n",
       "      <th>split1_test_f1</th>\n",
       "      <th>split2_test_f1</th>\n",
       "      <th>split3_test_f1</th>\n",
       "      <th>split4_test_f1</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.067382</td>\n",
       "      <td>0.184177</td>\n",
       "      <td>0.009385</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>4264</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 4264}</td>\n",
       "      <td>0.939878</td>\n",
       "      <td>0.943418</td>\n",
       "      <td>0.943049</td>\n",
       "      <td>0.941382</td>\n",
       "      <td>0.943175</td>\n",
       "      <td>0.942181</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>52</td>\n",
       "      <td>0.993857</td>\n",
       "      <td>0.988329</td>\n",
       "      <td>0.991708</td>\n",
       "      <td>0.991400</td>\n",
       "      <td>0.989247</td>\n",
       "      <td>0.990908</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>6</td>\n",
       "      <td>0.966114</td>\n",
       "      <td>0.965352</td>\n",
       "      <td>0.966766</td>\n",
       "      <td>0.965744</td>\n",
       "      <td>0.965662</td>\n",
       "      <td>0.965928</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.129413</td>\n",
       "      <td>0.210010</td>\n",
       "      <td>0.012499</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>1928</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1928}</td>\n",
       "      <td>0.939878</td>\n",
       "      <td>0.943418</td>\n",
       "      <td>0.943049</td>\n",
       "      <td>0.941382</td>\n",
       "      <td>0.943175</td>\n",
       "      <td>0.942181</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>52</td>\n",
       "      <td>0.993857</td>\n",
       "      <td>0.988329</td>\n",
       "      <td>0.991708</td>\n",
       "      <td>0.991400</td>\n",
       "      <td>0.989247</td>\n",
       "      <td>0.990908</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>6</td>\n",
       "      <td>0.966114</td>\n",
       "      <td>0.965352</td>\n",
       "      <td>0.966766</td>\n",
       "      <td>0.965744</td>\n",
       "      <td>0.965662</td>\n",
       "      <td>0.965928</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.061435</td>\n",
       "      <td>0.170488</td>\n",
       "      <td>0.009374</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2513</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2513}</td>\n",
       "      <td>0.939878</td>\n",
       "      <td>0.943418</td>\n",
       "      <td>0.943049</td>\n",
       "      <td>0.941382</td>\n",
       "      <td>0.943175</td>\n",
       "      <td>0.942181</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>52</td>\n",
       "      <td>0.993857</td>\n",
       "      <td>0.988329</td>\n",
       "      <td>0.991708</td>\n",
       "      <td>0.991400</td>\n",
       "      <td>0.989247</td>\n",
       "      <td>0.990908</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>6</td>\n",
       "      <td>0.966114</td>\n",
       "      <td>0.965352</td>\n",
       "      <td>0.966766</td>\n",
       "      <td>0.965744</td>\n",
       "      <td>0.965662</td>\n",
       "      <td>0.965928</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.152884</td>\n",
       "      <td>0.189215</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>0.006262</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>4048</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 4048}</td>\n",
       "      <td>0.939878</td>\n",
       "      <td>0.943418</td>\n",
       "      <td>0.943049</td>\n",
       "      <td>0.941382</td>\n",
       "      <td>0.943175</td>\n",
       "      <td>0.942181</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>52</td>\n",
       "      <td>0.993857</td>\n",
       "      <td>0.988329</td>\n",
       "      <td>0.991708</td>\n",
       "      <td>0.991400</td>\n",
       "      <td>0.989247</td>\n",
       "      <td>0.990908</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>6</td>\n",
       "      <td>0.966114</td>\n",
       "      <td>0.965352</td>\n",
       "      <td>0.966766</td>\n",
       "      <td>0.965744</td>\n",
       "      <td>0.965662</td>\n",
       "      <td>0.965928</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.165045</td>\n",
       "      <td>0.012192</td>\n",
       "      <td>0.012508</td>\n",
       "      <td>0.006254</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2424</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2424}</td>\n",
       "      <td>0.934695</td>\n",
       "      <td>0.935159</td>\n",
       "      <td>0.935502</td>\n",
       "      <td>0.932894</td>\n",
       "      <td>0.933563</td>\n",
       "      <td>0.934363</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>56</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.996622</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.997235</td>\n",
       "      <td>0.997727</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965241</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.965671</td>\n",
       "      <td>0.964852</td>\n",
       "      <td>0.964349</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.174107</td>\n",
       "      <td>0.010383</td>\n",
       "      <td>0.013951</td>\n",
       "      <td>0.007517</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2386</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2386}</td>\n",
       "      <td>0.934695</td>\n",
       "      <td>0.935159</td>\n",
       "      <td>0.935502</td>\n",
       "      <td>0.932894</td>\n",
       "      <td>0.933563</td>\n",
       "      <td>0.934363</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>56</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.996622</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.997235</td>\n",
       "      <td>0.997727</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965241</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.965671</td>\n",
       "      <td>0.964852</td>\n",
       "      <td>0.964349</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.209974</td>\n",
       "      <td>0.052688</td>\n",
       "      <td>0.013097</td>\n",
       "      <td>0.011501</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>1093</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1093}</td>\n",
       "      <td>0.934695</td>\n",
       "      <td>0.935159</td>\n",
       "      <td>0.935502</td>\n",
       "      <td>0.932894</td>\n",
       "      <td>0.933563</td>\n",
       "      <td>0.934363</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>56</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.996622</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.997235</td>\n",
       "      <td>0.997727</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965241</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.965671</td>\n",
       "      <td>0.964852</td>\n",
       "      <td>0.964349</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.175538</td>\n",
       "      <td>0.015315</td>\n",
       "      <td>0.019844</td>\n",
       "      <td>0.017344</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>4454</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 4454}</td>\n",
       "      <td>0.934695</td>\n",
       "      <td>0.935159</td>\n",
       "      <td>0.935502</td>\n",
       "      <td>0.932894</td>\n",
       "      <td>0.933563</td>\n",
       "      <td>0.934363</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>56</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.996622</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.997235</td>\n",
       "      <td>0.997727</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965241</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.965671</td>\n",
       "      <td>0.964852</td>\n",
       "      <td>0.964349</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.244732</td>\n",
       "      <td>0.042463</td>\n",
       "      <td>0.006738</td>\n",
       "      <td>0.005211</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>3677</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 3677}</td>\n",
       "      <td>0.934695</td>\n",
       "      <td>0.935159</td>\n",
       "      <td>0.935502</td>\n",
       "      <td>0.932894</td>\n",
       "      <td>0.933563</td>\n",
       "      <td>0.934363</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>56</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.996622</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.997235</td>\n",
       "      <td>0.997727</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965241</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.965671</td>\n",
       "      <td>0.964852</td>\n",
       "      <td>0.964349</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.618168</td>\n",
       "      <td>0.092111</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>0.008315</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2497</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2497}</td>\n",
       "      <td>0.941782</td>\n",
       "      <td>0.943346</td>\n",
       "      <td>0.945396</td>\n",
       "      <td>0.943430</td>\n",
       "      <td>0.943536</td>\n",
       "      <td>0.943498</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>43</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.981880</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.983415</td>\n",
       "      <td>0.975422</td>\n",
       "      <td>0.981632</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>10</td>\n",
       "      <td>0.962295</td>\n",
       "      <td>0.962227</td>\n",
       "      <td>0.964178</td>\n",
       "      <td>0.963008</td>\n",
       "      <td>0.959215</td>\n",
       "      <td>0.962185</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.084938</td>\n",
       "      <td>0.054884</td>\n",
       "      <td>0.012497</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2850</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2850}</td>\n",
       "      <td>0.941782</td>\n",
       "      <td>0.943068</td>\n",
       "      <td>0.945396</td>\n",
       "      <td>0.943430</td>\n",
       "      <td>0.943536</td>\n",
       "      <td>0.943442</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>44</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.981880</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.983415</td>\n",
       "      <td>0.975422</td>\n",
       "      <td>0.981632</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>10</td>\n",
       "      <td>0.962295</td>\n",
       "      <td>0.962082</td>\n",
       "      <td>0.964178</td>\n",
       "      <td>0.963008</td>\n",
       "      <td>0.959215</td>\n",
       "      <td>0.962156</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.703267</td>\n",
       "      <td>0.118614</td>\n",
       "      <td>0.010674</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2807</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2807}</td>\n",
       "      <td>0.941782</td>\n",
       "      <td>0.943068</td>\n",
       "      <td>0.945396</td>\n",
       "      <td>0.943430</td>\n",
       "      <td>0.943536</td>\n",
       "      <td>0.943442</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>44</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.981880</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.983415</td>\n",
       "      <td>0.975422</td>\n",
       "      <td>0.981632</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>10</td>\n",
       "      <td>0.962295</td>\n",
       "      <td>0.962082</td>\n",
       "      <td>0.964178</td>\n",
       "      <td>0.963008</td>\n",
       "      <td>0.959215</td>\n",
       "      <td>0.962156</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.455016</td>\n",
       "      <td>0.178044</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>3996</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 3996}</td>\n",
       "      <td>0.941782</td>\n",
       "      <td>0.943068</td>\n",
       "      <td>0.945396</td>\n",
       "      <td>0.943430</td>\n",
       "      <td>0.943536</td>\n",
       "      <td>0.943442</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>44</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.981880</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.983415</td>\n",
       "      <td>0.975422</td>\n",
       "      <td>0.981632</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>10</td>\n",
       "      <td>0.962295</td>\n",
       "      <td>0.962082</td>\n",
       "      <td>0.964178</td>\n",
       "      <td>0.963008</td>\n",
       "      <td>0.959215</td>\n",
       "      <td>0.962156</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.001942</td>\n",
       "      <td>0.131079</td>\n",
       "      <td>0.009367</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>3599</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 3599}</td>\n",
       "      <td>0.941782</td>\n",
       "      <td>0.943068</td>\n",
       "      <td>0.945396</td>\n",
       "      <td>0.943430</td>\n",
       "      <td>0.943536</td>\n",
       "      <td>0.943442</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>44</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.981880</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.983415</td>\n",
       "      <td>0.975422</td>\n",
       "      <td>0.981632</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>10</td>\n",
       "      <td>0.962295</td>\n",
       "      <td>0.962082</td>\n",
       "      <td>0.964178</td>\n",
       "      <td>0.963008</td>\n",
       "      <td>0.959215</td>\n",
       "      <td>0.962156</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.077704</td>\n",
       "      <td>0.881833</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.006260</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>4598</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 4598}</td>\n",
       "      <td>0.941782</td>\n",
       "      <td>0.943068</td>\n",
       "      <td>0.945396</td>\n",
       "      <td>0.943430</td>\n",
       "      <td>0.943536</td>\n",
       "      <td>0.943442</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>44</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.981880</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.983415</td>\n",
       "      <td>0.975422</td>\n",
       "      <td>0.981632</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>10</td>\n",
       "      <td>0.962295</td>\n",
       "      <td>0.962082</td>\n",
       "      <td>0.964178</td>\n",
       "      <td>0.963008</td>\n",
       "      <td>0.959215</td>\n",
       "      <td>0.962156</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.011879</td>\n",
       "      <td>0.122628</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2798</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2798}</td>\n",
       "      <td>0.941782</td>\n",
       "      <td>0.943068</td>\n",
       "      <td>0.945396</td>\n",
       "      <td>0.943430</td>\n",
       "      <td>0.943536</td>\n",
       "      <td>0.943442</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>44</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.981880</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.983415</td>\n",
       "      <td>0.975422</td>\n",
       "      <td>0.981632</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>10</td>\n",
       "      <td>0.962295</td>\n",
       "      <td>0.962082</td>\n",
       "      <td>0.964178</td>\n",
       "      <td>0.963008</td>\n",
       "      <td>0.959215</td>\n",
       "      <td>0.962156</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.994265</td>\n",
       "      <td>0.103757</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.011691</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2016</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2016}</td>\n",
       "      <td>0.941782</td>\n",
       "      <td>0.943068</td>\n",
       "      <td>0.945396</td>\n",
       "      <td>0.943430</td>\n",
       "      <td>0.943536</td>\n",
       "      <td>0.943442</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>44</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.981880</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.983415</td>\n",
       "      <td>0.975422</td>\n",
       "      <td>0.981632</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>10</td>\n",
       "      <td>0.962295</td>\n",
       "      <td>0.962082</td>\n",
       "      <td>0.964178</td>\n",
       "      <td>0.963008</td>\n",
       "      <td>0.959215</td>\n",
       "      <td>0.962156</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.452783</td>\n",
       "      <td>0.560668</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>4162</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 4162}</td>\n",
       "      <td>0.941782</td>\n",
       "      <td>0.943068</td>\n",
       "      <td>0.945396</td>\n",
       "      <td>0.943430</td>\n",
       "      <td>0.943536</td>\n",
       "      <td>0.943442</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>44</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.981880</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.983415</td>\n",
       "      <td>0.975422</td>\n",
       "      <td>0.981632</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>10</td>\n",
       "      <td>0.962295</td>\n",
       "      <td>0.962082</td>\n",
       "      <td>0.964178</td>\n",
       "      <td>0.963008</td>\n",
       "      <td>0.959215</td>\n",
       "      <td>0.962156</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.094159</td>\n",
       "      <td>1.288260</td>\n",
       "      <td>0.012495</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>2930</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2930}</td>\n",
       "      <td>0.946831</td>\n",
       "      <td>0.949428</td>\n",
       "      <td>0.950874</td>\n",
       "      <td>0.947716</td>\n",
       "      <td>0.948695</td>\n",
       "      <td>0.948709</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>23</td>\n",
       "      <td>0.968059</td>\n",
       "      <td>0.968673</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.968673</td>\n",
       "      <td>0.960061</td>\n",
       "      <td>0.966889</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>19</td>\n",
       "      <td>0.957327</td>\n",
       "      <td>0.958954</td>\n",
       "      <td>0.959842</td>\n",
       "      <td>0.958080</td>\n",
       "      <td>0.954344</td>\n",
       "      <td>0.957710</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.206243</td>\n",
       "      <td>1.324533</td>\n",
       "      <td>0.014141</td>\n",
       "      <td>0.007597</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>3906</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 3906}</td>\n",
       "      <td>0.946831</td>\n",
       "      <td>0.949428</td>\n",
       "      <td>0.950874</td>\n",
       "      <td>0.947716</td>\n",
       "      <td>0.948695</td>\n",
       "      <td>0.948709</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>23</td>\n",
       "      <td>0.968059</td>\n",
       "      <td>0.968673</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.968673</td>\n",
       "      <td>0.960061</td>\n",
       "      <td>0.966889</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>19</td>\n",
       "      <td>0.957327</td>\n",
       "      <td>0.958954</td>\n",
       "      <td>0.959842</td>\n",
       "      <td>0.958080</td>\n",
       "      <td>0.954344</td>\n",
       "      <td>0.957710</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.517519</td>\n",
       "      <td>0.941035</td>\n",
       "      <td>0.006255</td>\n",
       "      <td>0.007660</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>2160</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2160}</td>\n",
       "      <td>0.946831</td>\n",
       "      <td>0.949413</td>\n",
       "      <td>0.950874</td>\n",
       "      <td>0.947716</td>\n",
       "      <td>0.948695</td>\n",
       "      <td>0.948706</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>25</td>\n",
       "      <td>0.968059</td>\n",
       "      <td>0.968366</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.968673</td>\n",
       "      <td>0.960061</td>\n",
       "      <td>0.966828</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>21</td>\n",
       "      <td>0.957327</td>\n",
       "      <td>0.958796</td>\n",
       "      <td>0.959842</td>\n",
       "      <td>0.958080</td>\n",
       "      <td>0.954344</td>\n",
       "      <td>0.957678</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.682162</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>4354</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 4354}</td>\n",
       "      <td>0.954769</td>\n",
       "      <td>0.957173</td>\n",
       "      <td>0.959690</td>\n",
       "      <td>0.955562</td>\n",
       "      <td>0.957493</td>\n",
       "      <td>0.956938</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>10</td>\n",
       "      <td>0.953010</td>\n",
       "      <td>0.960995</td>\n",
       "      <td>0.950553</td>\n",
       "      <td>0.957617</td>\n",
       "      <td>0.948080</td>\n",
       "      <td>0.954051</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>34</td>\n",
       "      <td>0.953889</td>\n",
       "      <td>0.959080</td>\n",
       "      <td>0.955100</td>\n",
       "      <td>0.956588</td>\n",
       "      <td>0.952763</td>\n",
       "      <td>0.955484</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.682034</td>\n",
       "      <td>0.059906</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>4946</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 4946}</td>\n",
       "      <td>0.954769</td>\n",
       "      <td>0.957173</td>\n",
       "      <td>0.959690</td>\n",
       "      <td>0.955562</td>\n",
       "      <td>0.957493</td>\n",
       "      <td>0.956938</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>10</td>\n",
       "      <td>0.953010</td>\n",
       "      <td>0.960995</td>\n",
       "      <td>0.950553</td>\n",
       "      <td>0.957617</td>\n",
       "      <td>0.948080</td>\n",
       "      <td>0.954051</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>34</td>\n",
       "      <td>0.953889</td>\n",
       "      <td>0.959080</td>\n",
       "      <td>0.955100</td>\n",
       "      <td>0.956588</td>\n",
       "      <td>0.952763</td>\n",
       "      <td>0.955484</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.709011</td>\n",
       "      <td>0.087569</td>\n",
       "      <td>0.009386</td>\n",
       "      <td>0.007664</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>2652</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2652}</td>\n",
       "      <td>0.954769</td>\n",
       "      <td>0.957173</td>\n",
       "      <td>0.959690</td>\n",
       "      <td>0.955562</td>\n",
       "      <td>0.957493</td>\n",
       "      <td>0.956938</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>10</td>\n",
       "      <td>0.953010</td>\n",
       "      <td>0.960995</td>\n",
       "      <td>0.950553</td>\n",
       "      <td>0.957617</td>\n",
       "      <td>0.948080</td>\n",
       "      <td>0.954051</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>34</td>\n",
       "      <td>0.953889</td>\n",
       "      <td>0.959080</td>\n",
       "      <td>0.955100</td>\n",
       "      <td>0.956588</td>\n",
       "      <td>0.952763</td>\n",
       "      <td>0.955484</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.716493</td>\n",
       "      <td>0.082675</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>4039</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 4039}</td>\n",
       "      <td>0.954769</td>\n",
       "      <td>0.957173</td>\n",
       "      <td>0.959690</td>\n",
       "      <td>0.955562</td>\n",
       "      <td>0.957493</td>\n",
       "      <td>0.956938</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>10</td>\n",
       "      <td>0.953010</td>\n",
       "      <td>0.960995</td>\n",
       "      <td>0.950553</td>\n",
       "      <td>0.957617</td>\n",
       "      <td>0.948080</td>\n",
       "      <td>0.954051</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>34</td>\n",
       "      <td>0.953889</td>\n",
       "      <td>0.959080</td>\n",
       "      <td>0.955100</td>\n",
       "      <td>0.956588</td>\n",
       "      <td>0.952763</td>\n",
       "      <td>0.955484</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.818384</td>\n",
       "      <td>0.144549</td>\n",
       "      <td>0.009917</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>3962</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 3962}</td>\n",
       "      <td>0.954769</td>\n",
       "      <td>0.957173</td>\n",
       "      <td>0.959690</td>\n",
       "      <td>0.955562</td>\n",
       "      <td>0.957493</td>\n",
       "      <td>0.956938</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>10</td>\n",
       "      <td>0.953010</td>\n",
       "      <td>0.960995</td>\n",
       "      <td>0.950553</td>\n",
       "      <td>0.957617</td>\n",
       "      <td>0.948080</td>\n",
       "      <td>0.954051</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>34</td>\n",
       "      <td>0.953889</td>\n",
       "      <td>0.959080</td>\n",
       "      <td>0.955100</td>\n",
       "      <td>0.956588</td>\n",
       "      <td>0.952763</td>\n",
       "      <td>0.955484</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.578969</td>\n",
       "      <td>0.095828</td>\n",
       "      <td>0.009357</td>\n",
       "      <td>0.007640</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>1596</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 1596}</td>\n",
       "      <td>0.946077</td>\n",
       "      <td>0.949106</td>\n",
       "      <td>0.950273</td>\n",
       "      <td>0.948432</td>\n",
       "      <td>0.947272</td>\n",
       "      <td>0.948232</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>35</td>\n",
       "      <td>0.959152</td>\n",
       "      <td>0.962224</td>\n",
       "      <td>0.962531</td>\n",
       "      <td>0.965909</td>\n",
       "      <td>0.954839</td>\n",
       "      <td>0.960931</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>25</td>\n",
       "      <td>0.952570</td>\n",
       "      <td>0.955620</td>\n",
       "      <td>0.956363</td>\n",
       "      <td>0.957091</td>\n",
       "      <td>0.951040</td>\n",
       "      <td>0.954537</td>\n",
       "      <td>0.002329</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.308315</td>\n",
       "      <td>0.118638</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>2205</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2205}</td>\n",
       "      <td>0.946364</td>\n",
       "      <td>0.949091</td>\n",
       "      <td>0.949970</td>\n",
       "      <td>0.948463</td>\n",
       "      <td>0.947272</td>\n",
       "      <td>0.948232</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>36</td>\n",
       "      <td>0.959152</td>\n",
       "      <td>0.961916</td>\n",
       "      <td>0.962224</td>\n",
       "      <td>0.966523</td>\n",
       "      <td>0.954839</td>\n",
       "      <td>0.960931</td>\n",
       "      <td>0.003852</td>\n",
       "      <td>24</td>\n",
       "      <td>0.952715</td>\n",
       "      <td>0.955461</td>\n",
       "      <td>0.956057</td>\n",
       "      <td>0.957408</td>\n",
       "      <td>0.951040</td>\n",
       "      <td>0.954536</td>\n",
       "      <td>0.002322</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.140951</td>\n",
       "      <td>0.178428</td>\n",
       "      <td>0.009364</td>\n",
       "      <td>0.007646</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>3571</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 3571}</td>\n",
       "      <td>0.946093</td>\n",
       "      <td>0.948772</td>\n",
       "      <td>0.949682</td>\n",
       "      <td>0.948447</td>\n",
       "      <td>0.947304</td>\n",
       "      <td>0.948060</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>39</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.961302</td>\n",
       "      <td>0.962224</td>\n",
       "      <td>0.966216</td>\n",
       "      <td>0.955453</td>\n",
       "      <td>0.960931</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>22</td>\n",
       "      <td>0.952729</td>\n",
       "      <td>0.954996</td>\n",
       "      <td>0.955912</td>\n",
       "      <td>0.957249</td>\n",
       "      <td>0.951361</td>\n",
       "      <td>0.954450</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.004804</td>\n",
       "      <td>0.110531</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>0.007645</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>2642</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2642}</td>\n",
       "      <td>0.946380</td>\n",
       "      <td>0.948788</td>\n",
       "      <td>0.949682</td>\n",
       "      <td>0.948447</td>\n",
       "      <td>0.947272</td>\n",
       "      <td>0.948114</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>38</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.961609</td>\n",
       "      <td>0.962224</td>\n",
       "      <td>0.966216</td>\n",
       "      <td>0.954839</td>\n",
       "      <td>0.960869</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>26</td>\n",
       "      <td>0.952875</td>\n",
       "      <td>0.955156</td>\n",
       "      <td>0.955912</td>\n",
       "      <td>0.957249</td>\n",
       "      <td>0.951040</td>\n",
       "      <td>0.954446</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6.235631</td>\n",
       "      <td>0.482106</td>\n",
       "      <td>0.010960</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>3688</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 3688}</td>\n",
       "      <td>0.945807</td>\n",
       "      <td>0.948788</td>\n",
       "      <td>0.949682</td>\n",
       "      <td>0.948447</td>\n",
       "      <td>0.947288</td>\n",
       "      <td>0.948002</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>41</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.961609</td>\n",
       "      <td>0.962224</td>\n",
       "      <td>0.966216</td>\n",
       "      <td>0.955146</td>\n",
       "      <td>0.960931</td>\n",
       "      <td>0.003625</td>\n",
       "      <td>23</td>\n",
       "      <td>0.952584</td>\n",
       "      <td>0.955156</td>\n",
       "      <td>0.955912</td>\n",
       "      <td>0.957249</td>\n",
       "      <td>0.951201</td>\n",
       "      <td>0.954420</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.152303</td>\n",
       "      <td>0.293101</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>4395</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 4395}</td>\n",
       "      <td>0.946380</td>\n",
       "      <td>0.948772</td>\n",
       "      <td>0.949409</td>\n",
       "      <td>0.948447</td>\n",
       "      <td>0.947256</td>\n",
       "      <td>0.948053</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>40</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.961302</td>\n",
       "      <td>0.962531</td>\n",
       "      <td>0.966216</td>\n",
       "      <td>0.954531</td>\n",
       "      <td>0.960808</td>\n",
       "      <td>0.003839</td>\n",
       "      <td>28</td>\n",
       "      <td>0.952875</td>\n",
       "      <td>0.954996</td>\n",
       "      <td>0.955925</td>\n",
       "      <td>0.957249</td>\n",
       "      <td>0.950880</td>\n",
       "      <td>0.954385</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6.155484</td>\n",
       "      <td>0.203884</td>\n",
       "      <td>0.009384</td>\n",
       "      <td>0.007662</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>4392</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 4392}</td>\n",
       "      <td>0.945807</td>\n",
       "      <td>0.948772</td>\n",
       "      <td>0.949666</td>\n",
       "      <td>0.948447</td>\n",
       "      <td>0.947288</td>\n",
       "      <td>0.947996</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>42</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.961302</td>\n",
       "      <td>0.961916</td>\n",
       "      <td>0.966216</td>\n",
       "      <td>0.955146</td>\n",
       "      <td>0.960808</td>\n",
       "      <td>0.003594</td>\n",
       "      <td>27</td>\n",
       "      <td>0.952584</td>\n",
       "      <td>0.954996</td>\n",
       "      <td>0.955752</td>\n",
       "      <td>0.957249</td>\n",
       "      <td>0.951201</td>\n",
       "      <td>0.954357</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.042102</td>\n",
       "      <td>0.045528</td>\n",
       "      <td>0.009371</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>1030</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1030}</td>\n",
       "      <td>0.949939</td>\n",
       "      <td>0.951495</td>\n",
       "      <td>0.952046</td>\n",
       "      <td>0.949572</td>\n",
       "      <td>0.950015</td>\n",
       "      <td>0.950614</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>18</td>\n",
       "      <td>0.955774</td>\n",
       "      <td>0.957924</td>\n",
       "      <td>0.957310</td>\n",
       "      <td>0.954238</td>\n",
       "      <td>0.945929</td>\n",
       "      <td>0.954235</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>29</td>\n",
       "      <td>0.952848</td>\n",
       "      <td>0.954699</td>\n",
       "      <td>0.954671</td>\n",
       "      <td>0.951900</td>\n",
       "      <td>0.947968</td>\n",
       "      <td>0.952417</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3.151908</td>\n",
       "      <td>0.165059</td>\n",
       "      <td>0.012489</td>\n",
       "      <td>0.006244</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>1650</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1650}</td>\n",
       "      <td>0.949939</td>\n",
       "      <td>0.951495</td>\n",
       "      <td>0.952061</td>\n",
       "      <td>0.949572</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950613</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>19</td>\n",
       "      <td>0.955774</td>\n",
       "      <td>0.957924</td>\n",
       "      <td>0.957617</td>\n",
       "      <td>0.954238</td>\n",
       "      <td>0.945622</td>\n",
       "      <td>0.954235</td>\n",
       "      <td>0.004508</td>\n",
       "      <td>30</td>\n",
       "      <td>0.952848</td>\n",
       "      <td>0.954699</td>\n",
       "      <td>0.954831</td>\n",
       "      <td>0.951900</td>\n",
       "      <td>0.947806</td>\n",
       "      <td>0.952416</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.299527</td>\n",
       "      <td>1.190772</td>\n",
       "      <td>0.009368</td>\n",
       "      <td>0.007649</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>3227</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 3227}</td>\n",
       "      <td>0.949939</td>\n",
       "      <td>0.951495</td>\n",
       "      <td>0.952046</td>\n",
       "      <td>0.949572</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950610</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>20</td>\n",
       "      <td>0.955774</td>\n",
       "      <td>0.957924</td>\n",
       "      <td>0.957310</td>\n",
       "      <td>0.954238</td>\n",
       "      <td>0.945622</td>\n",
       "      <td>0.954174</td>\n",
       "      <td>0.004463</td>\n",
       "      <td>31</td>\n",
       "      <td>0.952848</td>\n",
       "      <td>0.954699</td>\n",
       "      <td>0.954671</td>\n",
       "      <td>0.951900</td>\n",
       "      <td>0.947806</td>\n",
       "      <td>0.952384</td>\n",
       "      <td>0.002530</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>7.054609</td>\n",
       "      <td>1.867330</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>4544</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 4544}</td>\n",
       "      <td>0.949939</td>\n",
       "      <td>0.951495</td>\n",
       "      <td>0.952046</td>\n",
       "      <td>0.949572</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950610</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>20</td>\n",
       "      <td>0.955774</td>\n",
       "      <td>0.957924</td>\n",
       "      <td>0.957310</td>\n",
       "      <td>0.954238</td>\n",
       "      <td>0.945622</td>\n",
       "      <td>0.954174</td>\n",
       "      <td>0.004463</td>\n",
       "      <td>31</td>\n",
       "      <td>0.952848</td>\n",
       "      <td>0.954699</td>\n",
       "      <td>0.954671</td>\n",
       "      <td>0.951900</td>\n",
       "      <td>0.947806</td>\n",
       "      <td>0.952384</td>\n",
       "      <td>0.002530</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2.255071</td>\n",
       "      <td>0.102793</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.007642</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1312</td>\n",
       "      <td>{'C': 10.0, 'class_weight': 'balanced', 'max_iter': 1312}</td>\n",
       "      <td>0.947738</td>\n",
       "      <td>0.948202</td>\n",
       "      <td>0.949649</td>\n",
       "      <td>0.948360</td>\n",
       "      <td>0.948939</td>\n",
       "      <td>0.948578</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>27</td>\n",
       "      <td>0.952396</td>\n",
       "      <td>0.955774</td>\n",
       "      <td>0.955774</td>\n",
       "      <td>0.958845</td>\n",
       "      <td>0.947773</td>\n",
       "      <td>0.954112</td>\n",
       "      <td>0.003770</td>\n",
       "      <td>33</td>\n",
       "      <td>0.950061</td>\n",
       "      <td>0.951973</td>\n",
       "      <td>0.952702</td>\n",
       "      <td>0.953574</td>\n",
       "      <td>0.948355</td>\n",
       "      <td>0.951333</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5.256786</td>\n",
       "      <td>1.717892</td>\n",
       "      <td>0.009365</td>\n",
       "      <td>0.007647</td>\n",
       "      <td>1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>4810</td>\n",
       "      <td>{'C': 1, 'class_weight': 'balanced', 'max_iter': 4810}</td>\n",
       "      <td>0.951444</td>\n",
       "      <td>0.953974</td>\n",
       "      <td>0.952396</td>\n",
       "      <td>0.951985</td>\n",
       "      <td>0.952795</td>\n",
       "      <td>0.952519</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>15</td>\n",
       "      <td>0.950860</td>\n",
       "      <td>0.954853</td>\n",
       "      <td>0.952396</td>\n",
       "      <td>0.949939</td>\n",
       "      <td>0.942550</td>\n",
       "      <td>0.950119</td>\n",
       "      <td>0.004134</td>\n",
       "      <td>45</td>\n",
       "      <td>0.951152</td>\n",
       "      <td>0.954413</td>\n",
       "      <td>0.952396</td>\n",
       "      <td>0.950961</td>\n",
       "      <td>0.947645</td>\n",
       "      <td>0.951313</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5.364295</td>\n",
       "      <td>1.830943</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>0.006255</td>\n",
       "      <td>1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>4134</td>\n",
       "      <td>{'C': 1, 'class_weight': 'balanced', 'max_iter': 4134}</td>\n",
       "      <td>0.951444</td>\n",
       "      <td>0.953974</td>\n",
       "      <td>0.952396</td>\n",
       "      <td>0.951985</td>\n",
       "      <td>0.952795</td>\n",
       "      <td>0.952519</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>15</td>\n",
       "      <td>0.950860</td>\n",
       "      <td>0.954853</td>\n",
       "      <td>0.952396</td>\n",
       "      <td>0.949939</td>\n",
       "      <td>0.942550</td>\n",
       "      <td>0.950119</td>\n",
       "      <td>0.004134</td>\n",
       "      <td>45</td>\n",
       "      <td>0.951152</td>\n",
       "      <td>0.954413</td>\n",
       "      <td>0.952396</td>\n",
       "      <td>0.950961</td>\n",
       "      <td>0.947645</td>\n",
       "      <td>0.951313</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.060436</td>\n",
       "      <td>1.666234</td>\n",
       "      <td>0.009378</td>\n",
       "      <td>0.007657</td>\n",
       "      <td>1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3289</td>\n",
       "      <td>{'C': 1, 'class_weight': 'balanced', 'max_iter': 3289}</td>\n",
       "      <td>0.951444</td>\n",
       "      <td>0.953974</td>\n",
       "      <td>0.952396</td>\n",
       "      <td>0.951985</td>\n",
       "      <td>0.952795</td>\n",
       "      <td>0.952519</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>15</td>\n",
       "      <td>0.950860</td>\n",
       "      <td>0.954853</td>\n",
       "      <td>0.952396</td>\n",
       "      <td>0.949939</td>\n",
       "      <td>0.942550</td>\n",
       "      <td>0.950119</td>\n",
       "      <td>0.004134</td>\n",
       "      <td>45</td>\n",
       "      <td>0.951152</td>\n",
       "      <td>0.954413</td>\n",
       "      <td>0.952396</td>\n",
       "      <td>0.950961</td>\n",
       "      <td>0.947645</td>\n",
       "      <td>0.951313</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2.866735</td>\n",
       "      <td>0.110381</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1779</td>\n",
       "      <td>{'C': 10.0, 'class_weight': 'balanced', 'max_iter': 1779}</td>\n",
       "      <td>0.946805</td>\n",
       "      <td>0.948171</td>\n",
       "      <td>0.949313</td>\n",
       "      <td>0.948103</td>\n",
       "      <td>0.948986</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>33</td>\n",
       "      <td>0.951167</td>\n",
       "      <td>0.955160</td>\n",
       "      <td>0.954853</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.948694</td>\n",
       "      <td>0.953867</td>\n",
       "      <td>0.003687</td>\n",
       "      <td>39</td>\n",
       "      <td>0.948981</td>\n",
       "      <td>0.951652</td>\n",
       "      <td>0.952075</td>\n",
       "      <td>0.953748</td>\n",
       "      <td>0.948840</td>\n",
       "      <td>0.951059</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3.658001</td>\n",
       "      <td>0.145140</td>\n",
       "      <td>0.006259</td>\n",
       "      <td>0.007666</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2383</td>\n",
       "      <td>{'C': 10.0, 'class_weight': 'balanced', 'max_iter': 2383}</td>\n",
       "      <td>0.947095</td>\n",
       "      <td>0.948397</td>\n",
       "      <td>0.949267</td>\n",
       "      <td>0.948103</td>\n",
       "      <td>0.948663</td>\n",
       "      <td>0.948305</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>32</td>\n",
       "      <td>0.951167</td>\n",
       "      <td>0.953931</td>\n",
       "      <td>0.953931</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.948080</td>\n",
       "      <td>0.953314</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>40</td>\n",
       "      <td>0.949127</td>\n",
       "      <td>0.951156</td>\n",
       "      <td>0.951593</td>\n",
       "      <td>0.953748</td>\n",
       "      <td>0.948371</td>\n",
       "      <td>0.950799</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5.685053</td>\n",
       "      <td>0.190396</td>\n",
       "      <td>0.009379</td>\n",
       "      <td>0.007658</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3895</td>\n",
       "      <td>{'C': 10.0, 'class_weight': 'balanced', 'max_iter': 3895}</td>\n",
       "      <td>0.946805</td>\n",
       "      <td>0.948107</td>\n",
       "      <td>0.949557</td>\n",
       "      <td>0.947512</td>\n",
       "      <td>0.948663</td>\n",
       "      <td>0.948129</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>37</td>\n",
       "      <td>0.951167</td>\n",
       "      <td>0.953931</td>\n",
       "      <td>0.953931</td>\n",
       "      <td>0.959152</td>\n",
       "      <td>0.948080</td>\n",
       "      <td>0.953252</td>\n",
       "      <td>0.003656</td>\n",
       "      <td>41</td>\n",
       "      <td>0.948981</td>\n",
       "      <td>0.951010</td>\n",
       "      <td>0.951739</td>\n",
       "      <td>0.953297</td>\n",
       "      <td>0.948371</td>\n",
       "      <td>0.950680</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>6.406788</td>\n",
       "      <td>0.217981</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>4409</td>\n",
       "      <td>{'C': 10.0, 'class_weight': 'balanced', 'max_iter': 4409}</td>\n",
       "      <td>0.947095</td>\n",
       "      <td>0.948365</td>\n",
       "      <td>0.949557</td>\n",
       "      <td>0.947512</td>\n",
       "      <td>0.948663</td>\n",
       "      <td>0.948238</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>34</td>\n",
       "      <td>0.951167</td>\n",
       "      <td>0.953317</td>\n",
       "      <td>0.953931</td>\n",
       "      <td>0.959152</td>\n",
       "      <td>0.948080</td>\n",
       "      <td>0.953129</td>\n",
       "      <td>0.003641</td>\n",
       "      <td>42</td>\n",
       "      <td>0.949127</td>\n",
       "      <td>0.950835</td>\n",
       "      <td>0.951739</td>\n",
       "      <td>0.953297</td>\n",
       "      <td>0.948371</td>\n",
       "      <td>0.950674</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2.445811</td>\n",
       "      <td>0.146421</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>1424</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1424}</td>\n",
       "      <td>0.947352</td>\n",
       "      <td>0.949433</td>\n",
       "      <td>0.949023</td>\n",
       "      <td>0.948632</td>\n",
       "      <td>0.948308</td>\n",
       "      <td>0.948550</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>28</td>\n",
       "      <td>0.950553</td>\n",
       "      <td>0.951474</td>\n",
       "      <td>0.954853</td>\n",
       "      <td>0.958538</td>\n",
       "      <td>0.946851</td>\n",
       "      <td>0.952454</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>43</td>\n",
       "      <td>0.948950</td>\n",
       "      <td>0.950453</td>\n",
       "      <td>0.951929</td>\n",
       "      <td>0.953559</td>\n",
       "      <td>0.947579</td>\n",
       "      <td>0.950494</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2.757976</td>\n",
       "      <td>0.096251</td>\n",
       "      <td>0.008139</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>1494</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1494}</td>\n",
       "      <td>0.947320</td>\n",
       "      <td>0.949402</td>\n",
       "      <td>0.949267</td>\n",
       "      <td>0.948585</td>\n",
       "      <td>0.948473</td>\n",
       "      <td>0.948609</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>26</td>\n",
       "      <td>0.949939</td>\n",
       "      <td>0.950860</td>\n",
       "      <td>0.953931</td>\n",
       "      <td>0.957617</td>\n",
       "      <td>0.944393</td>\n",
       "      <td>0.951348</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>44</td>\n",
       "      <td>0.948628</td>\n",
       "      <td>0.950130</td>\n",
       "      <td>0.951593</td>\n",
       "      <td>0.953080</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.949972</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2.756442</td>\n",
       "      <td>0.187894</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>1664</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1664}</td>\n",
       "      <td>0.948425</td>\n",
       "      <td>0.949449</td>\n",
       "      <td>0.949251</td>\n",
       "      <td>0.948632</td>\n",
       "      <td>0.948393</td>\n",
       "      <td>0.948830</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>22</td>\n",
       "      <td>0.943182</td>\n",
       "      <td>0.951781</td>\n",
       "      <td>0.953624</td>\n",
       "      <td>0.958538</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.949996</td>\n",
       "      <td>0.006111</td>\n",
       "      <td>48</td>\n",
       "      <td>0.945796</td>\n",
       "      <td>0.950613</td>\n",
       "      <td>0.951433</td>\n",
       "      <td>0.953559</td>\n",
       "      <td>0.945617</td>\n",
       "      <td>0.949404</td>\n",
       "      <td>0.003169</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6.267838</td>\n",
       "      <td>0.277150</td>\n",
       "      <td>0.012489</td>\n",
       "      <td>0.006244</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>4439</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 4439}</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.949080</td>\n",
       "      <td>0.948413</td>\n",
       "      <td>0.948632</td>\n",
       "      <td>0.948885</td>\n",
       "      <td>0.948476</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>29</td>\n",
       "      <td>0.945332</td>\n",
       "      <td>0.950246</td>\n",
       "      <td>0.954238</td>\n",
       "      <td>0.958538</td>\n",
       "      <td>0.941014</td>\n",
       "      <td>0.949874</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>49</td>\n",
       "      <td>0.946349</td>\n",
       "      <td>0.949662</td>\n",
       "      <td>0.951317</td>\n",
       "      <td>0.953559</td>\n",
       "      <td>0.944933</td>\n",
       "      <td>0.949164</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>6.710346</td>\n",
       "      <td>1.368571</td>\n",
       "      <td>0.010792</td>\n",
       "      <td>0.006021</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>3362</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 3362}</td>\n",
       "      <td>0.947385</td>\n",
       "      <td>0.948820</td>\n",
       "      <td>0.948655</td>\n",
       "      <td>0.948632</td>\n",
       "      <td>0.948853</td>\n",
       "      <td>0.948469</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>30</td>\n",
       "      <td>0.945639</td>\n",
       "      <td>0.950860</td>\n",
       "      <td>0.953317</td>\n",
       "      <td>0.958538</td>\n",
       "      <td>0.940399</td>\n",
       "      <td>0.949751</td>\n",
       "      <td>0.006253</td>\n",
       "      <td>50</td>\n",
       "      <td>0.946511</td>\n",
       "      <td>0.949839</td>\n",
       "      <td>0.950980</td>\n",
       "      <td>0.953559</td>\n",
       "      <td>0.944607</td>\n",
       "      <td>0.949099</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7.867586</td>\n",
       "      <td>0.834092</td>\n",
       "      <td>0.004257</td>\n",
       "      <td>0.005319</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>4688</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 4688}</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.949080</td>\n",
       "      <td>0.948655</td>\n",
       "      <td>0.948632</td>\n",
       "      <td>0.948591</td>\n",
       "      <td>0.948465</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>31</td>\n",
       "      <td>0.945332</td>\n",
       "      <td>0.950246</td>\n",
       "      <td>0.953317</td>\n",
       "      <td>0.958538</td>\n",
       "      <td>0.941014</td>\n",
       "      <td>0.949689</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>51</td>\n",
       "      <td>0.946349</td>\n",
       "      <td>0.949662</td>\n",
       "      <td>0.950980</td>\n",
       "      <td>0.953559</td>\n",
       "      <td>0.944787</td>\n",
       "      <td>0.949068</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.211949</td>\n",
       "      <td>0.149031</td>\n",
       "      <td>0.009366</td>\n",
       "      <td>0.007647</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>3911</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 3911}</td>\n",
       "      <td>0.965090</td>\n",
       "      <td>0.964530</td>\n",
       "      <td>0.967409</td>\n",
       "      <td>0.969468</td>\n",
       "      <td>0.969048</td>\n",
       "      <td>0.967109</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>4</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>0.910319</td>\n",
       "      <td>0.893428</td>\n",
       "      <td>0.906941</td>\n",
       "      <td>0.904147</td>\n",
       "      <td>0.904662</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>52</td>\n",
       "      <td>0.935928</td>\n",
       "      <td>0.936641</td>\n",
       "      <td>0.928948</td>\n",
       "      <td>0.937163</td>\n",
       "      <td>0.935474</td>\n",
       "      <td>0.934831</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.203337</td>\n",
       "      <td>0.198071</td>\n",
       "      <td>0.012504</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>3610</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 3610}</td>\n",
       "      <td>0.965090</td>\n",
       "      <td>0.964530</td>\n",
       "      <td>0.967409</td>\n",
       "      <td>0.969468</td>\n",
       "      <td>0.969048</td>\n",
       "      <td>0.967109</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>4</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>0.910319</td>\n",
       "      <td>0.893428</td>\n",
       "      <td>0.906941</td>\n",
       "      <td>0.904147</td>\n",
       "      <td>0.904662</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>52</td>\n",
       "      <td>0.935928</td>\n",
       "      <td>0.936641</td>\n",
       "      <td>0.928948</td>\n",
       "      <td>0.937163</td>\n",
       "      <td>0.935474</td>\n",
       "      <td>0.934831</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.216015</td>\n",
       "      <td>0.178052</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.007649</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>1277</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1277}</td>\n",
       "      <td>0.965090</td>\n",
       "      <td>0.964530</td>\n",
       "      <td>0.967409</td>\n",
       "      <td>0.969468</td>\n",
       "      <td>0.969048</td>\n",
       "      <td>0.967109</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>4</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>0.910319</td>\n",
       "      <td>0.893428</td>\n",
       "      <td>0.906941</td>\n",
       "      <td>0.904147</td>\n",
       "      <td>0.904662</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>52</td>\n",
       "      <td>0.935928</td>\n",
       "      <td>0.936641</td>\n",
       "      <td>0.928948</td>\n",
       "      <td>0.937163</td>\n",
       "      <td>0.935474</td>\n",
       "      <td>0.934831</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.242350</td>\n",
       "      <td>0.196195</td>\n",
       "      <td>0.011417</td>\n",
       "      <td>0.006080</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>3319</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 3319}</td>\n",
       "      <td>0.965090</td>\n",
       "      <td>0.964530</td>\n",
       "      <td>0.967409</td>\n",
       "      <td>0.969468</td>\n",
       "      <td>0.969048</td>\n",
       "      <td>0.967109</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>4</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>0.910319</td>\n",
       "      <td>0.893428</td>\n",
       "      <td>0.906941</td>\n",
       "      <td>0.904147</td>\n",
       "      <td>0.904662</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>52</td>\n",
       "      <td>0.935928</td>\n",
       "      <td>0.936641</td>\n",
       "      <td>0.928948</td>\n",
       "      <td>0.937163</td>\n",
       "      <td>0.935474</td>\n",
       "      <td>0.934831</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.188240</td>\n",
       "      <td>0.193751</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>2270</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2270}</td>\n",
       "      <td>0.965090</td>\n",
       "      <td>0.964530</td>\n",
       "      <td>0.967409</td>\n",
       "      <td>0.969468</td>\n",
       "      <td>0.969048</td>\n",
       "      <td>0.967109</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>4</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>0.910319</td>\n",
       "      <td>0.893428</td>\n",
       "      <td>0.906941</td>\n",
       "      <td>0.904147</td>\n",
       "      <td>0.904662</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>52</td>\n",
       "      <td>0.935928</td>\n",
       "      <td>0.936641</td>\n",
       "      <td>0.928948</td>\n",
       "      <td>0.937163</td>\n",
       "      <td>0.935474</td>\n",
       "      <td>0.934831</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.204889</td>\n",
       "      <td>0.210907</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>2319</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2319}</td>\n",
       "      <td>0.965090</td>\n",
       "      <td>0.964530</td>\n",
       "      <td>0.967409</td>\n",
       "      <td>0.969468</td>\n",
       "      <td>0.969048</td>\n",
       "      <td>0.967109</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>4</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>0.910319</td>\n",
       "      <td>0.893428</td>\n",
       "      <td>0.906941</td>\n",
       "      <td>0.904147</td>\n",
       "      <td>0.904662</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>52</td>\n",
       "      <td>0.935928</td>\n",
       "      <td>0.936641</td>\n",
       "      <td>0.928948</td>\n",
       "      <td>0.937163</td>\n",
       "      <td>0.935474</td>\n",
       "      <td>0.934831</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.977332</td>\n",
       "      <td>0.225257</td>\n",
       "      <td>0.006256</td>\n",
       "      <td>0.007661</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3711</td>\n",
       "      <td>{'C': 0.1, 'class_weight': 'balanced', 'max_iter': 3711}</td>\n",
       "      <td>0.973538</td>\n",
       "      <td>0.971044</td>\n",
       "      <td>0.974123</td>\n",
       "      <td>0.976256</td>\n",
       "      <td>0.977115</td>\n",
       "      <td>0.974415</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>1</td>\n",
       "      <td>0.858722</td>\n",
       "      <td>0.865172</td>\n",
       "      <td>0.843980</td>\n",
       "      <td>0.871314</td>\n",
       "      <td>0.865745</td>\n",
       "      <td>0.860987</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>58</td>\n",
       "      <td>0.912533</td>\n",
       "      <td>0.915056</td>\n",
       "      <td>0.904394</td>\n",
       "      <td>0.920805</td>\n",
       "      <td>0.918065</td>\n",
       "      <td>0.914170</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.885142</td>\n",
       "      <td>0.070101</td>\n",
       "      <td>0.012497</td>\n",
       "      <td>0.011690</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3053</td>\n",
       "      <td>{'C': 0.1, 'class_weight': 'balanced', 'max_iter': 3053}</td>\n",
       "      <td>0.973538</td>\n",
       "      <td>0.971044</td>\n",
       "      <td>0.974123</td>\n",
       "      <td>0.976256</td>\n",
       "      <td>0.977115</td>\n",
       "      <td>0.974415</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>1</td>\n",
       "      <td>0.858722</td>\n",
       "      <td>0.865172</td>\n",
       "      <td>0.843980</td>\n",
       "      <td>0.871314</td>\n",
       "      <td>0.865745</td>\n",
       "      <td>0.860987</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>58</td>\n",
       "      <td>0.912533</td>\n",
       "      <td>0.915056</td>\n",
       "      <td>0.904394</td>\n",
       "      <td>0.920805</td>\n",
       "      <td>0.918065</td>\n",
       "      <td>0.914170</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.823222</td>\n",
       "      <td>0.023790</td>\n",
       "      <td>0.009977</td>\n",
       "      <td>0.008215</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>4452</td>\n",
       "      <td>{'C': 0.1, 'class_weight': 'balanced', 'max_iter': 4452}</td>\n",
       "      <td>0.973538</td>\n",
       "      <td>0.971044</td>\n",
       "      <td>0.974123</td>\n",
       "      <td>0.976256</td>\n",
       "      <td>0.977115</td>\n",
       "      <td>0.974415</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>1</td>\n",
       "      <td>0.858722</td>\n",
       "      <td>0.865172</td>\n",
       "      <td>0.843980</td>\n",
       "      <td>0.871314</td>\n",
       "      <td>0.865745</td>\n",
       "      <td>0.860987</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>58</td>\n",
       "      <td>0.912533</td>\n",
       "      <td>0.915056</td>\n",
       "      <td>0.904394</td>\n",
       "      <td>0.920805</td>\n",
       "      <td>0.918065</td>\n",
       "      <td>0.914170</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0        1.067382      0.184177         0.009385        0.007663       1   \n",
       "1        1.129413      0.210010         0.012499        0.006249       1   \n",
       "2        1.061435      0.170488         0.009374        0.007654       1   \n",
       "3        1.152884      0.189215         0.003131        0.006262       1   \n",
       "4        0.165045      0.012192         0.012508        0.006254     0.1   \n",
       "5        0.174107      0.010383         0.013951        0.007517     0.1   \n",
       "6        0.209974      0.052688         0.013097        0.011501     0.1   \n",
       "7        0.175538      0.015315         0.019844        0.017344     0.1   \n",
       "8        0.244732      0.042463         0.006738        0.005211     0.1   \n",
       "9        3.618168      0.092111         0.006757        0.008315    10.0   \n",
       "10       4.084938      0.054884         0.012497        0.006248    10.0   \n",
       "11       4.703267      0.118614         0.010674        0.002503    10.0   \n",
       "12       5.455016      0.178044         0.003124        0.006248    10.0   \n",
       "13       5.001942      0.131079         0.009367        0.007648    10.0   \n",
       "14       6.077704      0.881833         0.003130        0.006260    10.0   \n",
       "15       4.011879      0.122628         0.006247        0.007651    10.0   \n",
       "16       2.994265      0.103757         0.012502        0.011691    10.0   \n",
       "17       5.452783      0.560668         0.006248        0.007653    10.0   \n",
       "18       4.094159      1.288260         0.012495        0.006247       1   \n",
       "19       4.206243      1.324533         0.014141        0.007597       1   \n",
       "20       3.517519      0.941035         0.006255        0.007660       1   \n",
       "21       0.682162      0.084123         0.003124        0.006248     0.1   \n",
       "22       0.682034      0.059906         0.009373        0.007653     0.1   \n",
       "23       0.709011      0.087569         0.009386        0.007664     0.1   \n",
       "24       0.716493      0.082675         0.006248        0.007653     0.1   \n",
       "25       0.818384      0.144549         0.009917        0.003675     0.1   \n",
       "26       2.578969      0.095828         0.009357        0.007640    10.0   \n",
       "27       3.308315      0.118638         0.003124        0.006248    10.0   \n",
       "28       5.140951      0.178428         0.009364        0.007646    10.0   \n",
       "29       4.004804      0.110531         0.009363        0.007645    10.0   \n",
       "30       6.235631      0.482106         0.010960        0.003129    10.0   \n",
       "31       6.152303      0.293101         0.009373        0.007653    10.0   \n",
       "32       6.155484      0.203884         0.009384        0.007662    10.0   \n",
       "33       2.042102      0.045528         0.009371        0.007651       1   \n",
       "34       3.151908      0.165059         0.012489        0.006244       1   \n",
       "35       5.299527      1.190772         0.009368        0.007649       1   \n",
       "36       7.054609      1.867330         0.006249        0.007653       1   \n",
       "37       2.255071      0.102793         0.009359        0.007642    10.0   \n",
       "38       5.256786      1.717892         0.009365        0.007647       1   \n",
       "39       5.364295      1.830943         0.012509        0.006255       1   \n",
       "40       5.060436      1.666234         0.009378        0.007657       1   \n",
       "41       2.866735      0.110381         0.004964        0.006410    10.0   \n",
       "42       3.658001      0.145140         0.006259        0.007666    10.0   \n",
       "43       5.685053      0.190396         0.009379        0.007658    10.0   \n",
       "44       6.406788      0.217981         0.006248        0.007652    10.0   \n",
       "45       2.445811      0.146421         0.006248        0.007652    10.0   \n",
       "46       2.757976      0.096251         0.008139        0.002377    10.0   \n",
       "47       2.756442      0.187894         0.012496        0.006248    10.0   \n",
       "48       6.267838      0.277150         0.012489        0.006244    10.0   \n",
       "49       6.710346      1.368571         0.010792        0.006021    10.0   \n",
       "50       7.867586      0.834092         0.004257        0.005319    10.0   \n",
       "51       1.211949      0.149031         0.009366        0.007647     0.1   \n",
       "52       1.203337      0.198071         0.012504        0.006252     0.1   \n",
       "53       1.216015      0.178052         0.006245        0.007649     0.1   \n",
       "54       1.242350      0.196195         0.011417        0.006080     0.1   \n",
       "55       1.188240      0.193751         0.009373        0.007653     0.1   \n",
       "56       1.204889      0.210907         0.006248        0.007653     0.1   \n",
       "57       0.977332      0.225257         0.006256        0.007661     0.1   \n",
       "58       0.885142      0.070101         0.012497        0.011690     0.1   \n",
       "59       0.823222      0.023790         0.009977        0.008215     0.1   \n",
       "\n",
       "   param_class_weight param_max_iter  \\\n",
       "0    {0: 1.0, 1: 1.0}           4264   \n",
       "1    {0: 1.0, 1: 1.0}           1928   \n",
       "2    {0: 1.0, 1: 1.0}           2513   \n",
       "3    {0: 1.0, 1: 1.0}           4048   \n",
       "4    {0: 1.0, 1: 1.0}           2424   \n",
       "5    {0: 1.0, 1: 1.0}           2386   \n",
       "6    {0: 1.0, 1: 1.0}           1093   \n",
       "7    {0: 1.0, 1: 1.0}           4454   \n",
       "8    {0: 1.0, 1: 1.0}           3677   \n",
       "9    {0: 1.0, 1: 1.0}           2497   \n",
       "10   {0: 1.0, 1: 1.0}           2850   \n",
       "11   {0: 1.0, 1: 1.0}           2807   \n",
       "12   {0: 1.0, 1: 1.0}           3996   \n",
       "13   {0: 1.0, 1: 1.0}           3599   \n",
       "14   {0: 1.0, 1: 1.0}           4598   \n",
       "15   {0: 1.0, 1: 1.0}           2798   \n",
       "16   {0: 1.0, 1: 1.0}           2016   \n",
       "17   {0: 1.0, 1: 1.0}           4162   \n",
       "18   {0: 5.0, 1: 1.0}           2930   \n",
       "19   {0: 5.0, 1: 1.0}           3906   \n",
       "20   {0: 5.0, 1: 1.0}           2160   \n",
       "21   {0: 5.0, 1: 1.0}           4354   \n",
       "22   {0: 5.0, 1: 1.0}           4946   \n",
       "23   {0: 5.0, 1: 1.0}           2652   \n",
       "24   {0: 5.0, 1: 1.0}           4039   \n",
       "25   {0: 5.0, 1: 1.0}           3962   \n",
       "26   {0: 5.0, 1: 1.0}           1596   \n",
       "27   {0: 5.0, 1: 1.0}           2205   \n",
       "28   {0: 5.0, 1: 1.0}           3571   \n",
       "29   {0: 5.0, 1: 1.0}           2642   \n",
       "30   {0: 5.0, 1: 1.0}           3688   \n",
       "31   {0: 5.0, 1: 1.0}           4395   \n",
       "32   {0: 5.0, 1: 1.0}           4392   \n",
       "33  {0: 10.0, 1: 1.0}           1030   \n",
       "34  {0: 10.0, 1: 1.0}           1650   \n",
       "35  {0: 10.0, 1: 1.0}           3227   \n",
       "36  {0: 10.0, 1: 1.0}           4544   \n",
       "37           balanced           1312   \n",
       "38           balanced           4810   \n",
       "39           balanced           4134   \n",
       "40           balanced           3289   \n",
       "41           balanced           1779   \n",
       "42           balanced           2383   \n",
       "43           balanced           3895   \n",
       "44           balanced           4409   \n",
       "45  {0: 10.0, 1: 1.0}           1424   \n",
       "46  {0: 10.0, 1: 1.0}           1494   \n",
       "47  {0: 10.0, 1: 1.0}           1664   \n",
       "48  {0: 10.0, 1: 1.0}           4439   \n",
       "49  {0: 10.0, 1: 1.0}           3362   \n",
       "50  {0: 10.0, 1: 1.0}           4688   \n",
       "51  {0: 10.0, 1: 1.0}           3911   \n",
       "52  {0: 10.0, 1: 1.0}           3610   \n",
       "53  {0: 10.0, 1: 1.0}           1277   \n",
       "54  {0: 10.0, 1: 1.0}           3319   \n",
       "55  {0: 10.0, 1: 1.0}           2270   \n",
       "56  {0: 10.0, 1: 1.0}           2319   \n",
       "57           balanced           3711   \n",
       "58           balanced           3053   \n",
       "59           balanced           4452   \n",
       "\n",
       "                                                              params  \\\n",
       "0       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 4264}   \n",
       "1       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1928}   \n",
       "2       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2513}   \n",
       "3       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 4048}   \n",
       "4     {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2424}   \n",
       "5     {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2386}   \n",
       "6     {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1093}   \n",
       "7     {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 4454}   \n",
       "8     {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 3677}   \n",
       "9    {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2497}   \n",
       "10   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2850}   \n",
       "11   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2807}   \n",
       "12   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 3996}   \n",
       "13   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 3599}   \n",
       "14   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 4598}   \n",
       "15   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2798}   \n",
       "16   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2016}   \n",
       "17   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 4162}   \n",
       "18      {'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2930}   \n",
       "19      {'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 3906}   \n",
       "20      {'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2160}   \n",
       "21    {'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 4354}   \n",
       "22    {'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 4946}   \n",
       "23    {'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2652}   \n",
       "24    {'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 4039}   \n",
       "25    {'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 3962}   \n",
       "26   {'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 1596}   \n",
       "27   {'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2205}   \n",
       "28   {'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 3571}   \n",
       "29   {'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2642}   \n",
       "30   {'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 3688}   \n",
       "31   {'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 4395}   \n",
       "32   {'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 4392}   \n",
       "33     {'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1030}   \n",
       "34     {'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1650}   \n",
       "35     {'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 3227}   \n",
       "36     {'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 4544}   \n",
       "37         {'C': 10.0, 'class_weight': 'balanced', 'max_iter': 1312}   \n",
       "38            {'C': 1, 'class_weight': 'balanced', 'max_iter': 4810}   \n",
       "39            {'C': 1, 'class_weight': 'balanced', 'max_iter': 4134}   \n",
       "40            {'C': 1, 'class_weight': 'balanced', 'max_iter': 3289}   \n",
       "41         {'C': 10.0, 'class_weight': 'balanced', 'max_iter': 1779}   \n",
       "42         {'C': 10.0, 'class_weight': 'balanced', 'max_iter': 2383}   \n",
       "43         {'C': 10.0, 'class_weight': 'balanced', 'max_iter': 3895}   \n",
       "44         {'C': 10.0, 'class_weight': 'balanced', 'max_iter': 4409}   \n",
       "45  {'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1424}   \n",
       "46  {'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1494}   \n",
       "47  {'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1664}   \n",
       "48  {'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 4439}   \n",
       "49  {'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 3362}   \n",
       "50  {'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 4688}   \n",
       "51   {'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 3911}   \n",
       "52   {'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 3610}   \n",
       "53   {'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1277}   \n",
       "54   {'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 3319}   \n",
       "55   {'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2270}   \n",
       "56   {'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2319}   \n",
       "57          {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 3711}   \n",
       "58          {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 3053}   \n",
       "59          {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 4452}   \n",
       "\n",
       "    split0_test_precision  split1_test_precision  split2_test_precision  \\\n",
       "0                0.939878               0.943418               0.943049   \n",
       "1                0.939878               0.943418               0.943049   \n",
       "2                0.939878               0.943418               0.943049   \n",
       "3                0.939878               0.943418               0.943049   \n",
       "4                0.934695               0.935159               0.935502   \n",
       "5                0.934695               0.935159               0.935502   \n",
       "6                0.934695               0.935159               0.935502   \n",
       "7                0.934695               0.935159               0.935502   \n",
       "8                0.934695               0.935159               0.935502   \n",
       "9                0.941782               0.943346               0.945396   \n",
       "10               0.941782               0.943068               0.945396   \n",
       "11               0.941782               0.943068               0.945396   \n",
       "12               0.941782               0.943068               0.945396   \n",
       "13               0.941782               0.943068               0.945396   \n",
       "14               0.941782               0.943068               0.945396   \n",
       "15               0.941782               0.943068               0.945396   \n",
       "16               0.941782               0.943068               0.945396   \n",
       "17               0.941782               0.943068               0.945396   \n",
       "18               0.946831               0.949428               0.950874   \n",
       "19               0.946831               0.949428               0.950874   \n",
       "20               0.946831               0.949413               0.950874   \n",
       "21               0.954769               0.957173               0.959690   \n",
       "22               0.954769               0.957173               0.959690   \n",
       "23               0.954769               0.957173               0.959690   \n",
       "24               0.954769               0.957173               0.959690   \n",
       "25               0.954769               0.957173               0.959690   \n",
       "26               0.946077               0.949106               0.950273   \n",
       "27               0.946364               0.949091               0.949970   \n",
       "28               0.946093               0.948772               0.949682   \n",
       "29               0.946380               0.948788               0.949682   \n",
       "30               0.945807               0.948788               0.949682   \n",
       "31               0.946380               0.948772               0.949409   \n",
       "32               0.945807               0.948772               0.949666   \n",
       "33               0.949939               0.951495               0.952046   \n",
       "34               0.949939               0.951495               0.952061   \n",
       "35               0.949939               0.951495               0.952046   \n",
       "36               0.949939               0.951495               0.952046   \n",
       "37               0.947738               0.948202               0.949649   \n",
       "38               0.951444               0.953974               0.952396   \n",
       "39               0.951444               0.953974               0.952396   \n",
       "40               0.951444               0.953974               0.952396   \n",
       "41               0.946805               0.948171               0.949313   \n",
       "42               0.947095               0.948397               0.949267   \n",
       "43               0.946805               0.948107               0.949557   \n",
       "44               0.947095               0.948365               0.949557   \n",
       "45               0.947352               0.949433               0.949023   \n",
       "46               0.947320               0.949402               0.949267   \n",
       "47               0.948425               0.949449               0.949251   \n",
       "48               0.947368               0.949080               0.948413   \n",
       "49               0.947385               0.948820               0.948655   \n",
       "50               0.947368               0.949080               0.948655   \n",
       "51               0.965090               0.964530               0.967409   \n",
       "52               0.965090               0.964530               0.967409   \n",
       "53               0.965090               0.964530               0.967409   \n",
       "54               0.965090               0.964530               0.967409   \n",
       "55               0.965090               0.964530               0.967409   \n",
       "56               0.965090               0.964530               0.967409   \n",
       "57               0.973538               0.971044               0.974123   \n",
       "58               0.973538               0.971044               0.974123   \n",
       "59               0.973538               0.971044               0.974123   \n",
       "\n",
       "    split3_test_precision  split4_test_precision  mean_test_precision  \\\n",
       "0                0.941382               0.943175             0.942181   \n",
       "1                0.941382               0.943175             0.942181   \n",
       "2                0.941382               0.943175             0.942181   \n",
       "3                0.941382               0.943175             0.942181   \n",
       "4                0.932894               0.933563             0.934363   \n",
       "5                0.932894               0.933563             0.934363   \n",
       "6                0.932894               0.933563             0.934363   \n",
       "7                0.932894               0.933563             0.934363   \n",
       "8                0.932894               0.933563             0.934363   \n",
       "9                0.943430               0.943536             0.943498   \n",
       "10               0.943430               0.943536             0.943442   \n",
       "11               0.943430               0.943536             0.943442   \n",
       "12               0.943430               0.943536             0.943442   \n",
       "13               0.943430               0.943536             0.943442   \n",
       "14               0.943430               0.943536             0.943442   \n",
       "15               0.943430               0.943536             0.943442   \n",
       "16               0.943430               0.943536             0.943442   \n",
       "17               0.943430               0.943536             0.943442   \n",
       "18               0.947716               0.948695             0.948709   \n",
       "19               0.947716               0.948695             0.948709   \n",
       "20               0.947716               0.948695             0.948706   \n",
       "21               0.955562               0.957493             0.956938   \n",
       "22               0.955562               0.957493             0.956938   \n",
       "23               0.955562               0.957493             0.956938   \n",
       "24               0.955562               0.957493             0.956938   \n",
       "25               0.955562               0.957493             0.956938   \n",
       "26               0.948432               0.947272             0.948232   \n",
       "27               0.948463               0.947272             0.948232   \n",
       "28               0.948447               0.947304             0.948060   \n",
       "29               0.948447               0.947272             0.948114   \n",
       "30               0.948447               0.947288             0.948002   \n",
       "31               0.948447               0.947256             0.948053   \n",
       "32               0.948447               0.947288             0.947996   \n",
       "33               0.949572               0.950015             0.950614   \n",
       "34               0.949572               0.950000             0.950613   \n",
       "35               0.949572               0.950000             0.950610   \n",
       "36               0.949572               0.950000             0.950610   \n",
       "37               0.948360               0.948939             0.948578   \n",
       "38               0.951985               0.952795             0.952519   \n",
       "39               0.951985               0.952795             0.952519   \n",
       "40               0.951985               0.952795             0.952519   \n",
       "41               0.948103               0.948986             0.948276   \n",
       "42               0.948103               0.948663             0.948305   \n",
       "43               0.947512               0.948663             0.948129   \n",
       "44               0.947512               0.948663             0.948238   \n",
       "45               0.948632               0.948308             0.948550   \n",
       "46               0.948585               0.948473             0.948609   \n",
       "47               0.948632               0.948393             0.948830   \n",
       "48               0.948632               0.948885             0.948476   \n",
       "49               0.948632               0.948853             0.948469   \n",
       "50               0.948632               0.948591             0.948465   \n",
       "51               0.969468               0.969048             0.967109   \n",
       "52               0.969468               0.969048             0.967109   \n",
       "53               0.969468               0.969048             0.967109   \n",
       "54               0.969468               0.969048             0.967109   \n",
       "55               0.969468               0.969048             0.967109   \n",
       "56               0.969468               0.969048             0.967109   \n",
       "57               0.976256               0.977115             0.974415   \n",
       "58               0.976256               0.977115             0.974415   \n",
       "59               0.976256               0.977115             0.974415   \n",
       "\n",
       "    std_test_precision  rank_test_precision  split0_test_recall  \\\n",
       "0             0.001358                   52            0.993857   \n",
       "1             0.001358                   52            0.993857   \n",
       "2             0.001358                   52            0.993857   \n",
       "3             0.001358                   52            0.993857   \n",
       "4             0.000984                   56            0.997850   \n",
       "5             0.000984                   56            0.997850   \n",
       "6             0.000984                   56            0.997850   \n",
       "7             0.000984                   56            0.997850   \n",
       "8             0.000984                   56            0.997850   \n",
       "9             0.001147                   43            0.983722   \n",
       "10            0.001159                   44            0.983722   \n",
       "11            0.001159                   44            0.983722   \n",
       "12            0.001159                   44            0.983722   \n",
       "13            0.001159                   44            0.983722   \n",
       "14            0.001159                   44            0.983722   \n",
       "15            0.001159                   44            0.983722   \n",
       "16            0.001159                   44            0.983722   \n",
       "17            0.001159                   44            0.983722   \n",
       "18            0.001394                   23            0.968059   \n",
       "19            0.001394                   23            0.968059   \n",
       "20            0.001393                   25            0.968059   \n",
       "21            0.001705                   10            0.953010   \n",
       "22            0.001705                   10            0.953010   \n",
       "23            0.001705                   10            0.953010   \n",
       "24            0.001705                   10            0.953010   \n",
       "25            0.001705                   10            0.953010   \n",
       "26            0.001452                   35            0.959152   \n",
       "27            0.001282                   36            0.959152   \n",
       "28            0.001243                   39            0.959459   \n",
       "29            0.001161                   38            0.959459   \n",
       "30            0.001339                   41            0.959459   \n",
       "31            0.001091                   40            0.959459   \n",
       "32            0.001333                   42            0.959459   \n",
       "33            0.000972                   18            0.955774   \n",
       "34            0.000979                   19            0.955774   \n",
       "35            0.000974                   20            0.955774   \n",
       "36            0.000974                   20            0.955774   \n",
       "37            0.000659                   27            0.952396   \n",
       "38            0.000854                   15            0.950860   \n",
       "39            0.000854                   15            0.950860   \n",
       "40            0.000854                   15            0.950860   \n",
       "41            0.000870                   33            0.951167   \n",
       "42            0.000716                   32            0.951167   \n",
       "43            0.000944                   37            0.951167   \n",
       "44            0.000868                   34            0.951167   \n",
       "45            0.000708                   28            0.950553   \n",
       "46            0.000740                   26            0.949939   \n",
       "47            0.000437                   22            0.943182   \n",
       "48            0.000598                   29            0.945332   \n",
       "49            0.000549                   30            0.945639   \n",
       "50            0.000576                   31            0.945332   \n",
       "51            0.002007                    4            0.908477   \n",
       "52            0.002007                    4            0.908477   \n",
       "53            0.002007                    4            0.908477   \n",
       "54            0.002007                    4            0.908477   \n",
       "55            0.002007                    4            0.908477   \n",
       "56            0.002007                    4            0.908477   \n",
       "57            0.002140                    1            0.858722   \n",
       "58            0.002140                    1            0.858722   \n",
       "59            0.002140                    1            0.858722   \n",
       "\n",
       "    split1_test_recall  split2_test_recall  split3_test_recall  \\\n",
       "0             0.988329            0.991708            0.991400   \n",
       "1             0.988329            0.991708            0.991400   \n",
       "2             0.988329            0.991708            0.991400   \n",
       "3             0.988329            0.991708            0.991400   \n",
       "4             0.996622            0.997850            0.999079   \n",
       "5             0.996622            0.997850            0.999079   \n",
       "6             0.996622            0.997850            0.999079   \n",
       "7             0.996622            0.997850            0.999079   \n",
       "8             0.996622            0.997850            0.999079   \n",
       "9             0.981880            0.983722            0.983415   \n",
       "10            0.981880            0.983722            0.983415   \n",
       "11            0.981880            0.983722            0.983415   \n",
       "12            0.981880            0.983722            0.983415   \n",
       "13            0.981880            0.983722            0.983415   \n",
       "14            0.981880            0.983722            0.983415   \n",
       "15            0.981880            0.983722            0.983415   \n",
       "16            0.981880            0.983722            0.983415   \n",
       "17            0.981880            0.983722            0.983415   \n",
       "18            0.968673            0.968980            0.968673   \n",
       "19            0.968673            0.968980            0.968673   \n",
       "20            0.968366            0.968980            0.968673   \n",
       "21            0.960995            0.950553            0.957617   \n",
       "22            0.960995            0.950553            0.957617   \n",
       "23            0.960995            0.950553            0.957617   \n",
       "24            0.960995            0.950553            0.957617   \n",
       "25            0.960995            0.950553            0.957617   \n",
       "26            0.962224            0.962531            0.965909   \n",
       "27            0.961916            0.962224            0.966523   \n",
       "28            0.961302            0.962224            0.966216   \n",
       "29            0.961609            0.962224            0.966216   \n",
       "30            0.961609            0.962224            0.966216   \n",
       "31            0.961302            0.962531            0.966216   \n",
       "32            0.961302            0.961916            0.966216   \n",
       "33            0.957924            0.957310            0.954238   \n",
       "34            0.957924            0.957617            0.954238   \n",
       "35            0.957924            0.957310            0.954238   \n",
       "36            0.957924            0.957310            0.954238   \n",
       "37            0.955774            0.955774            0.958845   \n",
       "38            0.954853            0.952396            0.949939   \n",
       "39            0.954853            0.952396            0.949939   \n",
       "40            0.954853            0.952396            0.949939   \n",
       "41            0.955160            0.954853            0.959459   \n",
       "42            0.953931            0.953931            0.959459   \n",
       "43            0.953931            0.953931            0.959152   \n",
       "44            0.953317            0.953931            0.959152   \n",
       "45            0.951474            0.954853            0.958538   \n",
       "46            0.950860            0.953931            0.957617   \n",
       "47            0.951781            0.953624            0.958538   \n",
       "48            0.950246            0.954238            0.958538   \n",
       "49            0.950860            0.953317            0.958538   \n",
       "50            0.950246            0.953317            0.958538   \n",
       "51            0.910319            0.893428            0.906941   \n",
       "52            0.910319            0.893428            0.906941   \n",
       "53            0.910319            0.893428            0.906941   \n",
       "54            0.910319            0.893428            0.906941   \n",
       "55            0.910319            0.893428            0.906941   \n",
       "56            0.910319            0.893428            0.906941   \n",
       "57            0.865172            0.843980            0.871314   \n",
       "58            0.865172            0.843980            0.871314   \n",
       "59            0.865172            0.843980            0.871314   \n",
       "\n",
       "    split4_test_recall  mean_test_recall  std_test_recall  rank_test_recall  \\\n",
       "0             0.989247          0.990908         0.001949                 6   \n",
       "1             0.989247          0.990908         0.001949                 6   \n",
       "2             0.989247          0.990908         0.001949                 6   \n",
       "3             0.989247          0.990908         0.001949                 6   \n",
       "4             0.997235          0.997727         0.000815                 1   \n",
       "5             0.997235          0.997727         0.000815                 1   \n",
       "6             0.997235          0.997727         0.000815                 1   \n",
       "7             0.997235          0.997727         0.000815                 1   \n",
       "8             0.997235          0.997727         0.000815                 1   \n",
       "9             0.975422          0.981632         0.003179                10   \n",
       "10            0.975422          0.981632         0.003179                10   \n",
       "11            0.975422          0.981632         0.003179                10   \n",
       "12            0.975422          0.981632         0.003179                10   \n",
       "13            0.975422          0.981632         0.003179                10   \n",
       "14            0.975422          0.981632         0.003179                10   \n",
       "15            0.975422          0.981632         0.003179                10   \n",
       "16            0.975422          0.981632         0.003179                10   \n",
       "17            0.975422          0.981632         0.003179                10   \n",
       "18            0.960061          0.966889         0.003427                19   \n",
       "19            0.960061          0.966889         0.003427                19   \n",
       "20            0.960061          0.966828         0.003397                21   \n",
       "21            0.948080          0.954051         0.004689                34   \n",
       "22            0.948080          0.954051         0.004689                34   \n",
       "23            0.948080          0.954051         0.004689                34   \n",
       "24            0.948080          0.954051         0.004689                34   \n",
       "25            0.948080          0.954051         0.004689                34   \n",
       "26            0.954839          0.960931         0.003723                25   \n",
       "27            0.954839          0.960931         0.003852                24   \n",
       "28            0.955453          0.960931         0.003519                22   \n",
       "29            0.954839          0.960869         0.003724                26   \n",
       "30            0.955146          0.960931         0.003625                23   \n",
       "31            0.954531          0.960808         0.003839                28   \n",
       "32            0.955146          0.960808         0.003594                27   \n",
       "33            0.945929          0.954235         0.004345                29   \n",
       "34            0.945622          0.954235         0.004508                30   \n",
       "35            0.945622          0.954174         0.004463                31   \n",
       "36            0.945622          0.954174         0.004463                31   \n",
       "37            0.947773          0.954112         0.003770                33   \n",
       "38            0.942550          0.950119         0.004134                45   \n",
       "39            0.942550          0.950119         0.004134                45   \n",
       "40            0.942550          0.950119         0.004134                45   \n",
       "41            0.948694          0.953867         0.003687                39   \n",
       "42            0.948080          0.953314         0.003756                40   \n",
       "43            0.948080          0.953252         0.003656                41   \n",
       "44            0.948080          0.953129         0.003641                42   \n",
       "45            0.946851          0.952454         0.003968                43   \n",
       "46            0.944393          0.951348         0.004395                44   \n",
       "47            0.942857          0.949996         0.006111                48   \n",
       "48            0.941014          0.949874         0.006219                49   \n",
       "49            0.940399          0.949751         0.006253                50   \n",
       "50            0.941014          0.949689         0.006100                51   \n",
       "51            0.904147          0.904662         0.005970                52   \n",
       "52            0.904147          0.904662         0.005970                52   \n",
       "53            0.904147          0.904662         0.005970                52   \n",
       "54            0.904147          0.904662         0.005970                52   \n",
       "55            0.904147          0.904662         0.005970                52   \n",
       "56            0.904147          0.904662         0.005970                52   \n",
       "57            0.865745          0.860987         0.009393                58   \n",
       "58            0.865745          0.860987         0.009393                58   \n",
       "59            0.865745          0.860987         0.009393                58   \n",
       "\n",
       "    split0_test_f1  split1_test_f1  split2_test_f1  split3_test_f1  \\\n",
       "0         0.966114        0.965352        0.966766        0.965744   \n",
       "1         0.966114        0.965352        0.966766        0.965744   \n",
       "2         0.966114        0.965352        0.966766        0.965744   \n",
       "3         0.966114        0.965352        0.966766        0.965744   \n",
       "4         0.965241        0.964912        0.965671        0.964852   \n",
       "5         0.965241        0.964912        0.965671        0.964852   \n",
       "6         0.965241        0.964912        0.965671        0.964852   \n",
       "7         0.965241        0.964912        0.965671        0.964852   \n",
       "8         0.965241        0.964912        0.965671        0.964852   \n",
       "9         0.962295        0.962227        0.964178        0.963008   \n",
       "10        0.962295        0.962082        0.964178        0.963008   \n",
       "11        0.962295        0.962082        0.964178        0.963008   \n",
       "12        0.962295        0.962082        0.964178        0.963008   \n",
       "13        0.962295        0.962082        0.964178        0.963008   \n",
       "14        0.962295        0.962082        0.964178        0.963008   \n",
       "15        0.962295        0.962082        0.964178        0.963008   \n",
       "16        0.962295        0.962082        0.964178        0.963008   \n",
       "17        0.962295        0.962082        0.964178        0.963008   \n",
       "18        0.957327        0.958954        0.959842        0.958080   \n",
       "19        0.957327        0.958954        0.959842        0.958080   \n",
       "20        0.957327        0.958796        0.959842        0.958080   \n",
       "21        0.953889        0.959080        0.955100        0.956588   \n",
       "22        0.953889        0.959080        0.955100        0.956588   \n",
       "23        0.953889        0.959080        0.955100        0.956588   \n",
       "24        0.953889        0.959080        0.955100        0.956588   \n",
       "25        0.953889        0.959080        0.955100        0.956588   \n",
       "26        0.952570        0.955620        0.956363        0.957091   \n",
       "27        0.952715        0.955461        0.956057        0.957408   \n",
       "28        0.952729        0.954996        0.955912        0.957249   \n",
       "29        0.952875        0.955156        0.955912        0.957249   \n",
       "30        0.952584        0.955156        0.955912        0.957249   \n",
       "31        0.952875        0.954996        0.955925        0.957249   \n",
       "32        0.952584        0.954996        0.955752        0.957249   \n",
       "33        0.952848        0.954699        0.954671        0.951900   \n",
       "34        0.952848        0.954699        0.954831        0.951900   \n",
       "35        0.952848        0.954699        0.954671        0.951900   \n",
       "36        0.952848        0.954699        0.954671        0.951900   \n",
       "37        0.950061        0.951973        0.952702        0.953574   \n",
       "38        0.951152        0.954413        0.952396        0.950961   \n",
       "39        0.951152        0.954413        0.952396        0.950961   \n",
       "40        0.951152        0.954413        0.952396        0.950961   \n",
       "41        0.948981        0.951652        0.952075        0.953748   \n",
       "42        0.949127        0.951156        0.951593        0.953748   \n",
       "43        0.948981        0.951010        0.951739        0.953297   \n",
       "44        0.949127        0.950835        0.951739        0.953297   \n",
       "45        0.948950        0.950453        0.951929        0.953559   \n",
       "46        0.948628        0.950130        0.951593        0.953080   \n",
       "47        0.945796        0.950613        0.951433        0.953559   \n",
       "48        0.946349        0.949662        0.951317        0.953559   \n",
       "49        0.946511        0.949839        0.950980        0.953559   \n",
       "50        0.946349        0.949662        0.950980        0.953559   \n",
       "51        0.935928        0.936641        0.928948        0.937163   \n",
       "52        0.935928        0.936641        0.928948        0.937163   \n",
       "53        0.935928        0.936641        0.928948        0.937163   \n",
       "54        0.935928        0.936641        0.928948        0.937163   \n",
       "55        0.935928        0.936641        0.928948        0.937163   \n",
       "56        0.935928        0.936641        0.928948        0.937163   \n",
       "57        0.912533        0.915056        0.904394        0.920805   \n",
       "58        0.912533        0.915056        0.904394        0.920805   \n",
       "59        0.912533        0.915056        0.904394        0.920805   \n",
       "\n",
       "    split4_test_f1  mean_test_f1  std_test_f1  rank_test_f1  \n",
       "0         0.965662      0.965928     0.000485             1  \n",
       "1         0.965662      0.965928     0.000485             1  \n",
       "2         0.965662      0.965928     0.000485             1  \n",
       "3         0.965662      0.965928     0.000485             1  \n",
       "4         0.964349      0.965005     0.000438             5  \n",
       "5         0.964349      0.965005     0.000438             5  \n",
       "6         0.964349      0.965005     0.000438             5  \n",
       "7         0.964349      0.965005     0.000438             5  \n",
       "8         0.964349      0.965005     0.000438             5  \n",
       "9         0.959215      0.962185     0.001642            10  \n",
       "10        0.959215      0.962156     0.001643            11  \n",
       "11        0.959215      0.962156     0.001643            11  \n",
       "12        0.959215      0.962156     0.001643            11  \n",
       "13        0.959215      0.962156     0.001643            11  \n",
       "14        0.959215      0.962156     0.001643            11  \n",
       "15        0.959215      0.962156     0.001643            11  \n",
       "16        0.959215      0.962156     0.001643            11  \n",
       "17        0.959215      0.962156     0.001643            11  \n",
       "18        0.954344      0.957710     0.001882            19  \n",
       "19        0.954344      0.957710     0.001882            19  \n",
       "20        0.954344      0.957678     0.001862            21  \n",
       "21        0.952763      0.955484     0.002202            22  \n",
       "22        0.952763      0.955484     0.002202            22  \n",
       "23        0.952763      0.955484     0.002202            22  \n",
       "24        0.952763      0.955484     0.002202            22  \n",
       "25        0.952763      0.955484     0.002202            22  \n",
       "26        0.951040      0.954537     0.002329            27  \n",
       "27        0.951040      0.954536     0.002322            28  \n",
       "28        0.951361      0.954450     0.002134            29  \n",
       "29        0.951040      0.954446     0.002217            30  \n",
       "30        0.951201      0.954420     0.002214            31  \n",
       "31        0.950880      0.954385     0.002259            32  \n",
       "32        0.951201      0.954357     0.002183            33  \n",
       "33        0.947968      0.952417     0.002471            34  \n",
       "34        0.947806      0.952416     0.002559            35  \n",
       "35        0.947806      0.952384     0.002530            36  \n",
       "36        0.947806      0.952384     0.002530            36  \n",
       "37        0.948355      0.951333     0.001886            38  \n",
       "38        0.947645      0.951313     0.002208            39  \n",
       "39        0.947645      0.951313     0.002208            39  \n",
       "40        0.947645      0.951313     0.002208            39  \n",
       "41        0.948840      0.951059     0.001890            42  \n",
       "42        0.948371      0.950799     0.001905            43  \n",
       "43        0.948371      0.950680     0.001805            44  \n",
       "44        0.948371      0.950674     0.001774            45  \n",
       "45        0.947579      0.950494     0.002114            46  \n",
       "46        0.946429      0.949972     0.002310            47  \n",
       "47        0.945617      0.949404     0.003169            48  \n",
       "48        0.944933      0.949164     0.003163            49  \n",
       "49        0.944607      0.949099     0.003189            50  \n",
       "50        0.944787      0.949068     0.003159            51  \n",
       "51        0.935474      0.934831     0.002998            52  \n",
       "52        0.935474      0.934831     0.002998            52  \n",
       "53        0.935474      0.934831     0.002998            52  \n",
       "54        0.935474      0.934831     0.002998            52  \n",
       "55        0.935474      0.934831     0.002998            52  \n",
       "56        0.935474      0.934831     0.002998            52  \n",
       "57        0.918065      0.914170     0.005626            58  \n",
       "58        0.918065      0.914170     0.005626            58  \n",
       "59        0.918065      0.914170     0.005626            58  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>1.067382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.184177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.009385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.007663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_C</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_class_weight</th>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_max_iter</th>\n",
       "      <td>4264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 4264}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.939878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.943418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.943049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_precision</th>\n",
       "      <td>0.941382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_precision</th>\n",
       "      <td>0.943175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.942181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.001358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.993857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.988329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.991708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_recall</th>\n",
       "      <td>0.9914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_recall</th>\n",
       "      <td>0.989247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.990908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.001949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.966114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.965352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.966766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_f1</th>\n",
       "      <td>0.965744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_f1</th>\n",
       "      <td>0.965662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.965928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.000485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  0\n",
       "mean_fit_time                                                              1.067382\n",
       "std_fit_time                                                               0.184177\n",
       "mean_score_time                                                            0.009385\n",
       "std_score_time                                                             0.007663\n",
       "param_C                                                                           1\n",
       "param_class_weight                                                 {0: 1.0, 1: 1.0}\n",
       "param_max_iter                                                                 4264\n",
       "params                 {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 4264}\n",
       "split0_test_precision                                                      0.939878\n",
       "split1_test_precision                                                      0.943418\n",
       "split2_test_precision                                                      0.943049\n",
       "split3_test_precision                                                      0.941382\n",
       "split4_test_precision                                                      0.943175\n",
       "mean_test_precision                                                        0.942181\n",
       "std_test_precision                                                         0.001358\n",
       "rank_test_precision                                                              52\n",
       "split0_test_recall                                                         0.993857\n",
       "split1_test_recall                                                         0.988329\n",
       "split2_test_recall                                                         0.991708\n",
       "split3_test_recall                                                           0.9914\n",
       "split4_test_recall                                                         0.989247\n",
       "mean_test_recall                                                           0.990908\n",
       "std_test_recall                                                            0.001949\n",
       "rank_test_recall                                                                  6\n",
       "split0_test_f1                                                             0.966114\n",
       "split1_test_f1                                                             0.965352\n",
       "split2_test_f1                                                             0.966766\n",
       "split3_test_f1                                                             0.965744\n",
       "split4_test_f1                                                             0.965662\n",
       "mean_test_f1                                                               0.965928\n",
       "std_test_f1                                                                0.000485\n",
       "rank_test_f1                                                                      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                0\n",
      "params               {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 4264}\n",
      "mean_test_precision                                                      0.942181\n",
      "mean_test_recall                                                         0.990908\n",
      "mean_test_f1                                                             0.965928\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(dual='auto')\n",
    "\n",
    "svc_cv = RandomizedSearchCV(svc, {'class_weight':['balanced', {0: 5.0, 1: 1.0}, {0: 1.0, 1: 1.0}, {0: 10.0, 1: 1.0}],\n",
    "                           'max_iter': randint(1000,5000),\n",
    "                           'C':[0.1, 1, 10.0]},\n",
    "                           cv = 5, verbose=3, n_iter= 60,\\\n",
    "                            scoring = ['precision','recall','f1'], refit=False)\n",
    "\n",
    "svc_cv.fit(X_tr_vec, y_tr)\n",
    "\n",
    "svc_cvdf = pd.DataFrame(svc_cv.cv_results_).sort_values(by='rank_test_f1').reset_index(drop=True)\n",
    "svc_best = pd.DataFrame(svc_cvdf.iloc[0,:])\n",
    "display(svc_cvdf)\n",
    "display(svc_best)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(svc_best.loc[['params', 'mean_test_precision','mean_test_recall','mean_test_f1'],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a1c8e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16279, number of negative: 1544\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 40627\n",
      "[LightGBM] [Info] Number of data points in the train set: 17823, number of used features: 1999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.913370 -> initscore=2.355499\n",
      "[LightGBM] [Info] Start training from score 2.355499\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABlUAAANVCAYAAADhqHiEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3gU1dvG8e+mF5IQeu+99ypdQwdRlCLSLChWwIYCCiIgdv0BilKkFwUEpBfpCChFitJ7r6Glz/vHvNnNkkICSSbl/lzXXjtz5szMM5tJMjPPnnNshmEYiIiIiIiIiIiIiIiISIJcrA5AREREREREREREREQkPVBSRUREREREREREREREJBGUVBEREREREREREREREUkEJVVEREREREREREREREQSQUkVERERERERERERERGRRFBSRUREREREREREREREJBGUVBEREREREREREREREUkEJVVEREREREREREREREQSQUkVERERERERERERERGRRFBSRdKNyZMnY7PZ7C83Nzfy5s1L586dOXTokNXhAVCkSBF69uxpdRix3L59m1GjRlG1alWyZMmCr68vVapUYcSIEdy+fdvq8BJtxIgRLFiwIFb5H3/8gc1m448//kj1mKIdPXqUV199lVKlSuHt7Y2Pjw/ly5dn0KBBnDlzxl6vcePGVKhQwbI4H8aMGTP4+uuvU2z7D/L7s3nzZj766COuX78ea1njxo1p3LhxssQWrVmzZrz00kv2+ehzL/rl6upKzpw5adu2LTt27IhzG4ZhMGPGDJo2bUpgYCCenp4UK1aMV155hVOnTsW770WLFtG2bVty586Nh4cH2bJlo1mzZkyfPp3w8HAArl27RtasWeP8PUlIYs/fjOqjjz7CZrNx+fLleOs8zN+Z48ePY7PZ+Pzzz+9bd8mSJXz00UfxLg8NDWXMmDE0atSI7Nmz4+7uTvbs2WncuDE//PADN2/edKof8/y02Wz4+vpStmxZhg4dGuvvf8+ePbHZbPj5+XHr1q1Y+z5x4gQuLi7YbLYEYxQREZHY4rqfLVCgAL169bLkeqtnz54UKVIkSetEX9NMnjw5RWK6n+hrleiXh4cHxYsX56233iI4ONiSmGKK6/OJ/rkfP348UdvYs2cPvXr1omjRonh5eZElSxaqVavG6NGjuXr1asoEnob07NmTLFmyJFgnqZ9pTNHX9L/88st9697v/jc4OJhRo0ZRu3ZtsmbNiru7O7lz56ZFixbMmDGD0NBQe93ocyPmy9/fn8qVK/P1118TGRnptO3GjRtjs9koVqwYhmHE2vf69evt27Hq91Eks3OzOgCRpJo0aRJlypQhJCSETZs28cknn7B27Vr+/fdfAgMDLY1t/vz5+Pv7WxrDvS5cuMCjjz7KkSNHeP311xk9ejQAa9asYfjw4cycOZNVq1aRO3duiyO9vxEjRtCxY0cef/xxp/Jq1aqxZcsWypUrZ0lcixcvpnPnzuTIkYNXX32VqlWrYrPZ+Oeff5g4cSK///47O3futCS25DRjxgz27t3Lm2++mSLbf5Dfn82bNzN06FB69uxJ1qxZnZaNHTs2GaOD3377jU2bNjFlypRYy0aMGEGTJk0IDw9n586dDB06lEaNGrFr1y5KlixprxcVFUXXrl2ZPXs2Xbp0YfLkyQQEBLBnzx4+++wzZsyYweLFi6lfv759HcMw6N27N5MnT6ZVq1Z8+eWXFCxYkBs3brB27Vr69u3L5cuXeeONNwgMDKRfv368/fbbtGrVCg8Pj/seV2Y5fx9Wav2dWbJkCWPGjIkzaXHp0iVatGjB3r176dGjB6+//jq5cuXiypUrrFmzhnfeeYeNGzcydepUp/U6duzIgAEDALh16xbr1q1j2LBh7Nmzh19//dWprru7OxEREcyePZvnnnvOadmkSZPw8/NLEw8tRERE0qvo+9m7d++yfv16Ro4cybp16/jnn3/w9fVNtTgGDx7MG2+8kaR18ubNy5YtWyhevHgKRXV/3t7erFmzBoDr16/zyy+/8MUXX7Bnzx5WrFhhWVzJ4ccff6Rv376ULl2at99+m3LlyhEeHs6OHTv4/vvv2bJlC/Pnz7c6TMu1bt2aLVu2kDdv3hTdT0L3v4cOHaJFixZcvHiRF198kQ8++IDAwEDOnTvH8uXL6d27NwcOHODjjz92Wu+1116ja9eugHn+Lly4kH79+nHq1Cm++OILp7p+fn4cO3aMNWvW0KxZM6dlEydOxN/fX9flIlYyRNKJSZMmGYCxfft2p/KhQ4cagDFx4kSLIrNWRESEERISEu/yoKAgw83NzdiwYUOsZRs2bDDc3NyM5s2bp2SIcbpf3HHx9fU1evTokTIBPaCjR48avr6+RtWqVY3r16/HWh4VFWX8+uuv9vlGjRoZ5cuXT9GYoqKijDt37iT7dlu3bm0ULlw42bf7MLF+9tlnBmAcO3Ys+QKKR61atYzOnTs7la1du9YAjLlz5zqV//zzzwZgDBkyxKl8xIgRBmCMGjUq1vbPnz9vFC5c2MidO7dx7do1e/mnn35qAMbQoUPjjOvcuXNOv9/nz5833NzcjOnTp9/3mJJ6/j6MsLAwIzw8PFm2ldw+/PBDAzAuXbqUIts/duyYARifffbZfeu+8sorRnyXZ0FBQYa7u7uxbt26OJdfvnzZmDp1qlMZYLzyyiux6j777LOGi4uLcffuXXtZjx49DF9fX6Nz585GvXr1nOpHRUUZhQsXNl544QUDMD788MP7HouIiIg4xHc/O3jwYAMwpk2bFu+6t2/fTunw0oXoa5V7NWnSxACMo0ePWhCVQ/Q136RJk+xl0T/3+92vbN682XB1dTVatGgR531yaGio8dtvvyVLnHfu3DGioqKSZVvJLb6fcXKJ7/4tLvHd/4aHhxvlypUzsmbNauzfvz/OdY8fP27Mnz/fPp/Q/UCDBg2MvHnzOpVFPzeoU6eO0bVrV6dlwcHBho+Pj/26POb5JiKpR91/SbpXo0YNwGyREdOOHTto164d2bJlw8vLi6pVqzJnzpxY6585c4YXX3yRggUL4uHhQb58+ejYsaPT9oKDg3nrrbcoWrQoHh4e5M+fnzfffDNW1ykxuy+6dOkSHh4eDB48ONY+//33X2w2G99++6297Pz58/Tp04cCBQrg4eFB0aJFGTp0KBEREfY60U1GR48ezfDhwylatCienp6sXbs2zs9mx44drFixgueee45HHnkk1vJHHnmE3r17s3z5cv766y97uc1m49VXX+WHH36gVKlSeHp6Uq5cOWbNmhVrGw8bd0hICAMGDKBKlSoEBASQLVs26taty2+//ea0H5vNxu3bt/n555/tzVyju3aKq1ue6GbDhw8fplWrVmTJkoWCBQsyYMAAp2a4AKdPn6Zjx474+fmRNWtWnnnmGbZv356oprRffvklt2/fZuzYsQQEBMRabrPZeOKJJ2KVb9++nQYNGuDj40OxYsUYNWoUUVFR9uWJ/Vyi9/Hqq6/y/fffU7ZsWTw9Pfn5558BGDp0KLVr1yZbtmz4+/tTrVo1JkyYEGcT4hkzZlC3bl2yZMlClixZqFKlChMmTADM5se///47J06ccGqyHC0sLIzhw4dTpkwZPD09yZkzJ7169eLSpUtO+yhSpAht2rRh3rx5VK1aFS8vL4YOHWpfFrP7r6ioKIYPH07p0qXx9vYma9asVKpUiW+++QYwu2x6++23AShatKg9pujzIK7uv0JDQxk2bBhly5bFy8uL7Nmz06RJEzZv3hzr84hp586dbNu2jWeffTbBetHi+rsUFhbGZ599RtmyZXnnnXdirZM7d25GjhzJhQsX7J97eHg4n376KWXKlInzbwlAnjx5nH6/c+fOzWOPPcb3339/3ziTev7G10XbvZ919O/k1KlTGTBgAPnz58fT05N9+/Zhs9nsxxfT0qVLsdlsLFy40F526NAhunbtSq5cufD09KRs2bKMGTPmvseVEuLr/uvHH390+js5Y8aMBLvT+PLLLylatChZsmShbt26bN261b6sZ8+e9uOL+Xt2/Phxtm/fzooVK3jxxRdp2LBhnNvOnj073bp1S9TxBAQE2Lusu1fv3r3ZvHkz//33n71s1apVnDhxgl69eiVq+yIiIpI4derUAcxuNsFxH/PPP/8QFBSEn5+f/Vvqib3mhoSv7aP3c+/1yty5c6lduzYBAQH2+5TevXvbl8fX/dfGjRtp1qwZfn5++Pj4UK9ePX7//XenOtFdNq1du5aXX36ZHDlykD17dp544gnOnj37wJ8fxP9MYPbs2dStWxdfX1+yZMlC8+bN42yB/eeff9K2bVuyZ8+Ol5cXxYsXd2qdcPjwYXr16kXJkiXx8fEhf/78tG3bln/++eeh4o5pxIgR2Gw2xo8fj6enZ6zlHh4etGvXzj4fX3es916vR3/uK1asoHfv3uTMmRMfHx9mz56NzWZj9erVsbYxbtw4bDYbe/bssZcl9vlKaoir+y/DMBgxYgSFCxfGy8uLGjVqsHLlyni7hA4PD+eDDz4gX758+Pv78+ijjzpd+yZ0/zt//nz279/PBx98QNmyZeOMsXDhwrF62IhPQEAA7u7ucS7r3bs38+bNc+ruOvq5TOfOnRO1fRFJGer+S9K9Y8eOAVCqVCl72dq1a2nRogW1a9fm+++/JyAggFmzZtGpUyfu3Lljv8g4c+YMNWvWJDw8nPfff59KlSpx5coVli9fzrVr18idOzd37tyhUaNGnD592l5n3759DBkyhH/++YdVq1Y5PVyOljNnTtq0acPPP//M0KFDcXFx5DAnTZqEh4cHzzzzDGAmJmrVqoWLiwtDhgyhePHibNmyheHDh3P8+HEmTZrktO1vv/2WUqVK8fnnn+Pv7+/UvVBMK1euBEjwn/njjz/O+PHjWblyJdWrV7eXL1y4kLVr1zJs2DB8fX0ZO3YsXbp0wc3NjY4dOyZb3KGhoVy9epW33nqL/PnzExYWxqpVq3jiiSeYNGkS3bt3B2DLli00bdqUJk2a2B8u36+rqPDwcNq1a8dzzz3HgAEDWL9+PR9//DEBAQEMGTIEMMebadKkCVevXuXTTz+lRIkSLFu2jE6dOiW47WgrVqwgd+7c9puhxDh//jzPPPMMAwYM4MMPP2T+/PkMHDiQfPny2Y83sZ9LtAULFrBhwwaGDBlCnjx5yJUrF2De+PTp04dChQoBsHXrVl577TXOnDlj/wwAhgwZwscff8wTTzzBgAEDCAgIYO/evfabu7Fjx/Liiy9y5MiRWE3Oo6KiaN++PRs2bOCdd96hXr16nDhxgg8//JDGjRuzY8cOvL297fX//vtvDhw4wKBBgyhatGi83RyMHj2ajz76iEGDBtGwYUPCw8P5999/7ReUzz//PFevXuW7775j3rx59ubf8XXPFBERQcuWLdmwYQNvvvkmTZs2JSIigq1bt3Ly5Enq1asX789s8eLFuLq6xvsw+15x/V3666+/uHbtGi+++GKcfzMA2rZti4uLCytXrmTAgAHs2LGDq1ev8sILL8S7TlwaN27MwIEDuX79eqxu0WJ6kPM3KQYOHEjdunX5/vvvcXFxoWDBglStWpVJkybF6lpq8uTJ5MqVi1atWgGwf/9+6tWrR6FChfjiiy/IkycPy5cv5/XXX+fy5ct8+OGHKRJzUowfP54+ffrw5JNP8tVXX3Hjxg2GDh0aK3EbbcyYMZQpU8beN/PgwYNp1aoVx44dIyAggMGDB3P79m1++eUXtmzZYl8vb968zJgxA8DpZjqxDMOwJ7qju//6+eef6dy5c5w3cI8++iiFCxdm4sSJfPrppwBMmDCBhg0bxvv/RkRERB7M4cOHAfP+MVpYWBjt2rWjT58+vPfee0RERCTpmvt+1/Zx2bJlC506daJTp0589NFHeHl5ceLECXtXW/FZt24djz32GJUqVWLChAl4enoyduxY2rZty8yZM2PdVz3//PO0bt2aGTNmcOrUKd5++226det23/0k5NixY7i5uVGsWDF72YgRIxg0aBC9evVi0KBB9i84NWjQgG3bttnvGZYvX07btm0pW7YsX375JYUKFeL48eNOXYmdPXuW7NmzM2rUKHLmzMnVq1f5+eefqV27Njt37qR06dIPHDtAZGQka9asoXr16hQsWPChthWf3r1707p1a6ZOncrt27dp06YNuXLlYtKkSbG6lpo8eTLVqlWjUqVKQOKfr1jpgw8+YOTIkbz44os88cQTnDp1iueff57w8HCne7Jo77//PvXr1+enn34iODiYd999l7Zt23LgwAFcXV0TvP+Nfs7yINflUVFR9uvyGzdu8Ntvv7Fs2TLefffdOOt37tyZfv36MXPmTF5++WXAvC7v2LFjmut6XiTTsbiljEiiRTeb3bp1qxEeHm7cvHnTWLZsmZEnTx6jYcOGTt3KlClTxqhatWqsrmbatGlj5M2b14iMjDQMwzB69+5tuLu7x9tk0zAMY+TIkYaLi0usZtq//PKLARhLliyxlxUuXNipe6qFCxcagLFixQp7WUREhJEvXz7jySeftJf16dPHyJIli3HixAmnfXz++ecGYOzbt88wDEeT0eLFixthYWH3+8iMl156yQCMf//9N946Bw4cMADj5ZdftpcBhre3t3H+/HmnuMuUKWOUKFEiReOOiIgwwsPDjeeee86oWrWq07L4uv+KbsK7du1ae1mPHj0MwJgzZ45T3VatWhmlS5e2z48ZM8YAjKVLlzrV69OnT6Ka0np5eRl16tRJsE5MjRo1MgDjzz//dCovV65cgt2wJfS5AEZAQIBx9erVBPcdGRlphIeHG8OGDTOyZ89ub/J99OhRw9XV1XjmmWcSXD++5s8zZ840gFjdRG3fvt0AjLFjx9rLChcubLi6uhr//fdfrO3c+/vTpk0bo0qVKgnGlFD3X40aNTIaNWpkn58yZYoBGD/++GOC24xLy5YtjTJlysQqjz73Zs+ebYSHhxt37twxNm3aZJQuXdooV66cUzdes2bNMgDj+++/T3BfuXPnNsqWLZukde61cuXKOM/reyX1/L33ZxTt3s86+nNp2LBhrLrffvutATidA1evXjU8PT2NAQMG2MuaN29uFChQwLhx44bT+q+++qrh5eV13/M9KRLT/de9f2ciIyONPHnyGLVr13aqd+LECcPd3d3pdyX6b2DFihWNiIgIe/m2bdsMwJg5c6a9LL7uv+L7ex4VFWWEh4fbXzG3bxjm34e4Xi1btjRu3brlVDdmdwsffvihkSdPHiM8PNy4cuWK4enpaUyePNm4dOmSuv8SERF5AHHdzy5evNjImTOn4efnZ7/3ir6Pubd768Recyf22r5Hjx5O1yvR93BxdQkbLa7urerUqWPkypXLuHnzpr0sIiLCqFChglGgQAH7PUf08fft29dpm6NHjzYA49y5cwnGGx2zr6+v/brn8uXLxrhx4wwXFxfj/ffft9c7efKk4ebmZrz22mtO69+8edPIkyeP8fTTT9vLihcvbhQvXtypS9T7iYiIMMLCwoySJUsa/fr1s5c/aPdf58+fN4BYXQ0nJL7rsXuv16P3371791h1+/fvb3h7ezv9zPfv328AxnfffWcvS+zzleSQmO6/7v1Mo+8lOnXq5FRvy5YtBhDnfUqrVq2c6s6ZM8cAjC1bttjL4rv/bdGihQHE6qYtoevy6HMjrlfPnj1jXcPH7Da8R48eRo0aNQzDMIx9+/YZgPHHH3/Yf/fV/ZeINdT9l6Q7derUwd3dHT8/P1q0aEFgYCC//fYbbm5mw6vDhw/z77//2luBRERE2F+tWrXi3Llz9madS5cupUmTJvE22QTzG+oVKlSgSpUqTttq3rx5nF3BxNSyZUvy5Mnj1GJj+fLlnD171qkZ9eLFi2nSpAn58uVz2kfLli0B89s/MbVr1y7e5qFJZfx/N1D3fgu+WbNmToPXu7q60qlTJw4fPszp06eTNe65c+dSv359smTJgpubG+7u7kyYMIEDBw481LHZbDbatm3rVFapUiWnb2itW7fOfi7F1KVLl4fad0Ly5MlDrVq1EowLkva5NG3alMDAwFjla9as4dFHHyUgIABXV1fc3d0ZMmQIV65c4eLFi4D5TZvIyEheeeWVBzqexYsXkzVrVtq2bet0HlSpUoU8efLE+h2pVKlSnN8WuletWrXYvXs3ffv2Zfny5Q89CN/SpUvx8vJy+t1LrLNnz9pb/8SlU6dOuLu74+PjQ/369QkODub3339PsJVIfAzDSFKrlLhEx3rmzJmH2s7DevLJJ2OVPfPMM3h6ejp1GzFz5kxCQ0PtXUuFhISwevVqOnTogI+PT6y/4yEhIU7dZt0r+htg0a/IyMhkP7b//vuP8+fP8/TTTzuVFypUiPr168e5TuvWrZ2624r+9l9C3xq9n99++w13d3f7K65u3J5++mm2b9/O9u3bWb9+Pd9++y07duygRYsW8baq6dWrFxcuXGDp0qVMnz4dDw8PnnrqqQeOU0REREwx72fbtGlDnjx5WLp0qdO9F8S+jkrsNfeDXtvXrFkTMK8b5syZk6jryNu3b/Pnn3/SsWNHsmTJYi93dXXl2Wef5fTp005dKkHsb/ffez10v+u427dv2697cuTIwcsvv0ynTp345JNP7HWWL19OREQE3bt3d9qWl5cXjRo1sn9WBw8e5MiRIzz33HN4eXnFe5wRERGMGDGCcuXK4eHhgZubGx4eHhw6dOih71lTS1zX5b179+bu3bvMnj3bXjZp0iQ8PT3tg6kn5flKXCIjI53WidnldXLZunUroaGhsa7L69SpE2+XvPc7Dx/EN99843RdXrly5Vh13njjDft1+dq1axkxYgRz5sxJ8PlD79692bFjB//88w8TJkygePHiie5BQURSjpIqku5MmTKF7du3s2bNGvr06cOBAwec/gFF96P61ltvOf1Dc3d3p2/fvgBcvnwZMMc9KVCgQIL7u3DhAnv27Im1LT8/PwzDsG8rLm5ubjz77LPMnz/f3mXR5MmTyZs3L82bN3fax6JFi2Lto3z58k7xRovu5uh+ort8iu6KKC7R/ZDe28w4T548sepGl125ciXZ4p43bx5PP/00+fPnZ9q0aWzZsoXt27fTu3dvQkJCEnWc8fHx8Yl1cezp6em03StXrsS6gQHiLItLoUKFEvx845I9e/ZYZZ6enty9e9c+n9TPJa7Pdtu2bQQFBQHmuA+bNm1i+/btfPDBBwD2/UX3wXy/34X4XLhwgevXr+Ph4RHrXDh//vwDn78DBw7k888/Z+vWrbRs2ZLs2bPTrFkzduzY8UBxXrp0iXz58jl1xZdYd+/eTfBG69NPP2X79u2sW7eODz74gAsXLvD44487PbBOzO/j7du3uXz5sv33MTHrxCU61pjnVFwe5PxNirh+1tmyZaNdu3ZMmTLFfpM8efJkatWqZf/bceXKFSIiIvjuu+9inVPR3YMl9Le3d+/eTuvc26VBcoj+O5iUvx/3/u5H95d9v58TOM6Fe2/0GjdubL8xa9OmTZzr5syZkxo1alCjRg0aNGjAa6+9xrfffsvGjRvjHTeqcOHCNGvWjIkTJzJx4kQ6d+6Mj4/PfeMUERGRhEXfz+7cuZOzZ8+yZ8+eWF/I8PHxidW1T2KvuR/02r5hw4YsWLDAnowoUKAAFSpUYObMmfGuc+3aNQzDiPOaL1++fIDjmina/a6Hhg0b5nRsxYsXd6rv7e1tv/ZZtGgRjRs3ZubMmYwaNcpeJ/qZQM2aNWN9VrNnz07yZ9W/f38GDx7M448/zqJFi/jzzz/Zvn07lStXTtR13P3kyJEDHx+fVL8uL1++PDVr1rR/CTQyMpJp06bRvn17smXLBiTt+UpcmjVr5rTOg3zB7X7SynV5165d7edmtWrV4ly3QIEC9uvy6C6bBw8ezNy5c1m+fHmc60R3wfvDDz8wdepUevfu/dBfwhORh6cxVSTdKVu2rH0guiZNmhAZGclPP/3EL7/8QseOHcmRIwdgPpCNa4BwwN7nac6cOe2tLuKTI0cOvL29mThxYrzLE9KrVy8+++wze5+jCxcu5M0333T6tnKOHDmoVKmS07drYoq+II2W2H+gjz32GO+//z4LFiyI1RIj2oIFC+x1Yzp//nysutFl0RcgyRH3tGnTKFq0qH2gvGjxfXs6uWXPnp1t27bFKo/r+OPSvHlzvvvuO7Zu3Zqs41Ik9XOJ67OdNWsW7u7uLF682CkhEP0zjxbdf/Pp06cfqA/f6EEmly1bFudyPz+/+8YaFzc3N/r370///v25fv06q1at4v3336d58+acOnUqyQ94c+bMycaNG4mKikpyYiVHjhxcvXo13uXFihWz/11q2LAh3t7eDBo0iO+++4633noLgOrVqxMYGMjChQsZOXJknJ/DwoULiYqKsv8+1qhRg2zZsvHbb7/Fu05comO939+npJ6/Xl5ecZ6Dly9fjnNf8cXbq1cv5s6dy8qVKylUqBDbt29n3Lhx9uWBgYH2bzjG9y3LokWLxhvnRx99xKuvvmqfv/ccTA7RfwfvHRAVEv/3Iymi/54vXLjQniwFyJo1q/3ciythG5/ob+Pt3r073jq9e/emW7duREVFOf18RERE5MHFvJ+NT1zXUIm95n6Ya/v27dvTvn17QkND2bp1KyNHjqRr164UKVKEunXrxqofGBiIi4sL586di7UsevD5+12P3uvFF190+qLIvYO2u7i4OH1+jz32GNWrV2fo0KE888wzFCxY0L7PX375hcKFC8e7r5ifVUKmTZtG9+7dGTFihFP55cuXH6hl+r1cXV1p1qwZS5cu5fTp04lKiHl6esZ5XX5vEitaQtflffv25cCBAxw9epRz587ZW48DSXq+EpcffviBmzdvxtpecrrfdXl8rVUe1GOPPcb48eNZuHCh/V4PzN4ConsM8PPzS/QzjZjX5TG/fBtT9NhANpuNHj16POQRiEhyUEsVSfdGjx5NYGAgQ4YMISoqitKlS1OyZEl2795t/wbAva/oC86WLVuydu3aBJurtmnThiNHjpA9e/Y4t3W/f9Bly5aldu3aTJo0iRkzZjh1cRNzH3v37qV48eJx7uPe5ERi1ahRg6CgICZMmMCmTZtiLd+4cSMTJ06kRYsWToPUA6xevdrpoiQyMpLZs2dTvHhx+0VecsRts9nw8PBwusg7f/48v/32W6y697bmSA6NGjXi5s2bLF261Kl81qxZiVq/X79++Pr60rdvX27cuBFruWEYsQa2S4ykfC4JbcPNzc0pgXf37l2mTp3qVC8oKAhXV9f7PjSN7/Nv06YNV65cITIyMs7z4GEHbgTzwXHHjh155ZVXuHr1qr2FVVK+UdSyZUtCQkLi/WZ+QsqUKcPRo0cTXf+dd96hRIkSjBo1yn4T4eHhwdtvv82BAwf47LPPYq1z8eJFBg4cSO7cuXn++ecBcHd359133+Xff//l448/jnNfFy9ejPX7HR1r9ACc8Unq+VukSBH27NnjVOfgwYMJ/g2NS1BQEPnz52fSpElMmjQJLy8vpxaHPj4+NGnShJ07d1KpUqU4z6uEEghFihRJ9nPwXqVLlyZPnjzMmTPHqfzkyZNs3rz5gbcb3zkd/ff8xx9/ZMOGDQ+8/Wi7du0CSLBbuw4dOtChQwd69+6drEljERERSbrEXnMn9to+IZ6enjRq1IhPP/0UgJ07d8ZZz9fXl9q1azNv3jyna5eoqCimTZtGgQIFEtXtb0z58uVzOq6KFSveN9YxY8YQEhLC8OHDAfOLQ25ubhw5ciTeZwIApUqVonjx4kycODHBB+A2my1Wcuf3339P1q52Bw4ciGEYvPDCC4SFhcVaHh4ezqJFi+zzcV2Xr1mzhlu3biVpv126dMHLy4vJkyczefJk8ufP7/QFnqQ8X4lL6dKlk/T85EHUrl0bT09Pp27MwOwW7GG684rv/rdDhw6UK1eOESNG8O+//z7w9qMl5rq8R48etG3blrfffpv8+fM/9D5F5OGppYqke4GBgQwcOJB33nmHGTNm0K1bN3744QdatmxJ8+bN6dmzJ/nz5+fq1ascOHCAv//+m7lz5wJm0+KlS5fSsGFD3n//fSpWrMj169dZtmwZ/fv3p0yZMrz55pv8+uuvNGzYkH79+lGpUiWioqI4efIkK1asYMCAAdSuXTvBGHv37k2fPn04e/Ys9erVi/WAb9iwYaxcuZJ69erx+uuvU7p0aUJCQjh+/DhLlizh+++/f+CumaZMmcKjjz5KUFAQr7/+ur0bnDVr1vDNN99QpkyZOB8y58iRg6ZNmzJ48GB8fX0ZO3Ys//77r1OyITnibtOmDfPmzaNv37507NiRU6dO8fHHH5M3b14OHTrkVLdixYr88ccfLFq0iLx58+Ln5/fQD0t79OjBV199Rbdu3Rg+fDglSpRg6dKl9qa392vRULRoUXsrpCpVqvDqq69StWpVAPbv38/EiRMxDIMOHTokKa6kfC7xad26NV9++SVdu3blxRdf5MqVK3z++eexbgiKFCnC+++/z8cff8zdu3fp0qULAQEB7N+/n8uXLzN06FDA/PznzZvHuHHjqF69uv1bYp07d2b69Om0atWKN954g1q1auHu7s7p06dZu3Yt7du3T/LxA7Rt25YKFSpQo0YNcubMyYkTJ/j6668pXLgwJUuWtMcEZv+1PXr0wN3dndKlS8d5Yd+lSxcmTZrESy+9xH///UeTJk2Iiorizz//pGzZsnTu3DneWBo3bszEiRM5ePBgom4M3d3dGTFiBE8//TTffPMNgwYNAuDdd99l9+7d9vdOnToREBDAnj17+Oyzz7h58yaLFy92GhcjOhHz4Ycfsm3bNrp27UrBggW5ceMG69evZ/z48QwdOtSp24itW7eSPXv2+96EJvX8ffbZZ+nWrRt9+/blySef5MSJE4wePdr+Lb/EcnV1pXv37nz55Zf4+/vzxBNPxBoL5JtvvuGRRx6hQYMGvPzyyxQpUoSbN29y+PBhFi1axJo1a5K0z8RYtGhRnOdOx44dY5W5uLgwdOhQ+vTpQ8eOHenduzfXr19n6NCh5M2b94G6mQPHOf3pp5/SsmVLXF1dqVSpEh4eHkybNo3mzZvz6KOP0rNnT5o3b06uXLkIDg5mz549rFq1KlZXIWB+ay96DJqQkBB27drF8OHDyZo1a6wkf0xeXl788ssvD3QcIiIikrwSe82d2Gv7ew0ZMoTTp0/TrFkzChQowPXr1+1jRDRq1CjeuEaOHMljjz1GkyZNeOutt/Dw8GDs2LHs3buXmTNnpko3RY0aNaJVq1ZMmjSJ9957j6JFizJs2DA++OADjh49ah+L9cKFC2zbtg1fX1/75zBmzBjatm1LnTp16NevH4UKFeLkyZMsX76c6dOnA+a92eTJkylTpgyVKlXir7/+4rPPPnvge/S41K1bl3HjxtG3b1+qV6/Oyy+/TPny5QkPD2fnzp2MHz+eChUq2McMffbZZxk8eDBDhgyhUaNG7N+/n//9739xjq+XkKxZs9KhQwcmT57M9evXeeutt2Jdxyb2+UpyiYyMjPMa1NfX1z5+a0zZsmWjf//+jBw5ksDAQDp06MDp06eT5bo8rvtfV1dXFixYQPPmzalVqxYvvPACjRs3JjAwkOvXr/Pnn3+ye/fuOMfuPXnypP26/Pbt22zZsoWRI0dSuHDheFsCgZlsvLfHCRGxWDIPfC+SYiZNmmQAxvbt22Mtu3v3rlGoUCGjZMmSRkREhGEYhrF7927j6aefNnLlymW4u7sbefLkMZo2bWp8//33TuueOnXK6N27t5EnTx7D3d3dyJcvn/H0008bFy5csNe5deuWMWjQIKN06dKGh4eHERAQYFSsWNHo16+fcf78eXu9woULGz169IgV340bNwxvb28DMH788cc4j+/SpUvG66+/bhQtWtRwd3c3smXLZlSvXt344IMPjFu3bhmGYRjHjh0zAOOzzz5L0md369YtY8SIEUaVKlUMHx8fw8fHx6hUqZIxfPhw+7ZjAoxXXnnFGDt2rFG8eHHD3d3dKFOmjDF9+vQUiXvUqFFGkSJFDE9PT6Ns2bLGjz/+aHz44YfGvX+idu3aZdSvX9/w8fExAKNRo0aGYRjG2rVrDcBYu3atvW6PHj0MX1/fWPuKa7snT540nnjiCSNLliyGn5+f8eSTTxpLliwxAOO3335L8LONduTIEaNv375GiRIlDE9PT8Pb29soV66c0b9/f+PYsWP2eo0aNTLKly8fa/0ePXoYhQsXfqDPJfrnFZeJEycapUuXNjw9PY1ixYoZI0eONCZMmGAATnEZhmFMmTLFqFmzpuHl5WVkyZLFqFq1qjFp0iT78qtXrxodO3Y0smbNathsNqc4wsPDjc8//9yoXLmyff0yZcoYffr0MQ4dOmSvV7hwYaN169Zxxnrv788XX3xh1KtXz8iRI4fh4eFhFCpUyHjuueeM48ePO603cOBAI1++fIaLi4vTedCoUSP7ORLt7t27xpAhQ4ySJUsaHh4eRvbs2Y2mTZsamzdvjjOmaDdu3DCyZMlijB492qk8+tybO3dunOvVrl3bCAwMNK5fv24vi4qKMqZPn240btzYyJo1q+Hh4WEULVrUePnll40TJ07EG8Nvv/1mtG7d2siZM6fh5uZmBAYGGk2aNDG+//57IzQ01Gn7hQsXNl577bUEjymmxJ6/UVFRxujRo41ixYoZXl5eRo0aNYw1a9bE+qzv97kYhmEcPHjQAAzAWLlyZZx1jh07ZvTu3dvInz+/4e7ubuTMmdOoV6+eMXz48EQfW2JE/17F94p5TDH/zhiGYYwfP94oUaKE4eHhYZQqVcqYOHGi0b59e6Nq1apOxxHf30DA+PDDD+3zoaGhxvPPP2/kzJnT/nsW82cQEhJifPfdd8YjjzxiZM2a1XBzczOyZctmNGjQwPj000+NK1euxNp+zJe7u7tRrFgxo1evXsbhw4ed6sb3dzOmS5cuxYpZRERE7i+h+9mYEvp/nNhrbsO4/7X9vfcfixcvNlq2bGnkz5/f8PDwMHLlymW0atXK2LBhg71O9DVNzO0YhmFs2LDBaNq0qeHr62t4e3sbderUMRYtWpSo44/vGiupn80///xjuLi4GL169bKXLViwwGjSpInh7+9veHp6GoULFzY6duxorFq1ymndLVu2GC1btjQCAgIMT09Po3jx4ka/fv3sy69du2Y899xzRq5cuQwfHx/jkUceMTZs2BDrGjiuzyf6uO+994rPrl27jB49ehiFChUyPDw8DF9fX6Nq1arGkCFDjIsXL9rrhYaGGu+8845RsGBBw9vb22jUqJGxa9euWPdUiTnvVqxYYb9WPHjwYJx1Evt85WH16NEj3mvy6PM1rs80KirKGD58uFGgQAHDw8PDqFSpkrF48WKjcuXKRocOHez14rtPietnl9D9r2GY94gjRowwatasafj7+xtubm5Grly5jMcee8wYM2aMcfv27Vjbj/ny8vIySpUqZbz55pvGuXPnnLYd33ODmLZv3x7n76OIpA6bYRhGMuVnRCSDsNlsvPLKK/zvf/+zOhTLjBgxgkGDBnHy5Mlk/QaSpF+vvfYaq1evZt++fWl6YMDVq1cTFBTEvn37KFOmjNXhZDrXr1+nVKlSPP7444wfP97qcEREREREMqVjx45RpkwZPvzwQ95//32rwxGRDEbdf4lIphedPCpTpgzh4eGsWbOGb7/9lm7duimhInaDBg1iypQp/Prrr3F2B5VWDB8+nN69eyuhkgrOnz/PJ598QpMmTciePTsnTpzgq6++4ubNm7zxxhtWhyciIiIikins3r2bmTNnUq9ePfz9/fnvv/8YPXo0/v7+PPfcc1aHJyIZkJIqIpLp+fj48NVXX3H8+HFCQ0MpVKgQ7777rn0cDBGA3LlzM336dK5du2Z1KPG6du0ajRo1om/fvlaHkil4enpy/Phx+vbty9WrV/Hx8aFOnTp8//33lC9f3urwREREREQyBV9fX3bs2MGECRO4fv06AQEBNG7cmE8++YTcuXNbHZ6IZEDq/ktERERERERERERERCQRXKwOQEREREREREREREREJD1QUkVERERERERERERERCQRlFQRERERERERERERERFJhEw3UH1UVBRnz57Fz88Pm81mdTgiIiIiIinOMAxu3rxJvnz5cHHR96rk/nTfJCIiIiKZSVLumTJdUuXs2bMULFjQ6jBERERERFLdqVOnKFCggNVhSDqg+yYRERERyYwSc8+U6ZIqfn5+gPnh+Pv7WxyNiIiIiEjKCw4OpmDBgvZrYZH70X2TiIiIiGQmSblnynRJleim6/7+/ro5EBEREZFMRd04SWLpvklEREREMqPE3DOpQ2UREREREREREREREZFEUFJFREREREREREREREQkEZRUERERERERERERERERSQQlVURERERERERERERERBJBSRUREREREREREREREZFEUFJFREREREREREREREQkEZRUERERERERERERERERSQQlVURERERERERERERERBJBSRUREREREREREREREZFEUFJFREREREREREREREQkEZRUERERERERERERERERSQQlVURERERERERERERERBJBSRUREREREREREREREZFEUFJFREREREREREREREQkEZRUERERERERERERERERSQQlVURERERERERERERERBJBSRUREREREREREREREZFEUFJFREREREREREREREQkEZRUERERERERERERERERSQQlVURERERERERERERERBJBSRUREREREREREREREZFEUFJFREREREREREREREQkEZRUERERERERERERERERSQQlVURERERERERERERERBJBSRUREREREREREREREZFEUFJFREREREREREREREQkEZRUERERERERERERERERSQQlVURERERERERERERERBLB0qTK+vXradu2Lfny5cNms7FgwYL7rrNu3TqqV6+Ol5cXxYoV4/vvv0/5QEVERERERFJASt0T/frrr5QrVw5PT0/KlSvH/PnzUyB6EREREZHMx9Kkyu3bt6lcuTL/+9//ElX/2LFjtGrVigYNGrBz507ef/99Xn/9dX799dcUjlRERERERCT5pcQ90ZYtW+jUqRPPPvssu3fv5tlnn+Xpp5/mzz//TKnDEBERERHJNGyGYRhWBwFgs9mYP38+jz/+eLx13n33XRYuXMiBAwfsZS+99BK7d+9my5YtidpPcHAwAQEB3LhxA39//4cNW0REJP0zDLhxFKIiIPgkGJFmedhNuHEMPPyc64cFw6U9kCV/6sZ5dT+E3gDfPKm7X5GHdCfUhqebgasrUONtyFsr1WPQNXD6kFz3RJ06dSI4OJilS5fa67Ro0YLAwEBmzpyZqFisPGdu34ak5H98fKBGDXBzS7mYRERERCRjS8r1b7q67NyyZQtBQUFOZc2bN2fChAmEh4fj7u4ea53Q0FBCQ0Pt88HBwSkep4iIiKXC78LdS85lt89DxF3HfGQIHF8BeydC6PVUDU8kMwmNcKXdhGfI7nOHKV3m41nmGatDknQuMfdEW7ZsoV+/frHqfP311/FuNy3dN128CO+8k7R1+veHrl1TJh4RERERkZjSVVLl/Pnz5M6d26ksd+7cREREcPnyZfLmzRtrnZEjRzJ06NDUClFERCT5hVyHqDAIuWYmQMJvmy1K3Lyc64XegAPT4cwGK6IUkXtERtnoNuMJVh8qZi+b/YSFAUmGkJh7ovjqnD9/Pt7tpqX7Jk9PqFw5cXXPnoVLl8xEjIiIiIhIakhXSRUwm8THFN172b3l0QYOHEj//v3t88HBwRQsWDDlAhQREXkY4Xfg3FY4tRa2Dk/dffsVgvz14eZpKNTULAu5ZnbzdW+XW5Fh4J0DvLOnbowY4F8U4vm/L5JWGIbBq/0388uefwHw9nblzc/fgyLlLI5MMoLE3BPFVSe+eyZIW/dN+fLBhAmJq/vttzBlSsrGIyIiIiISU7pKquTJkyfWt6suXryIm5sb2bPH/VDH09MTT0/P1AhPRETEdPMMhN+COxfh6gGICAEXdzCi4PR68C/sqBsZCseXQs7KcPCX5I2jaCtwz2JOG5Fw7aBZFi0qwhwvpUIv8CsANpfk3b9IJvbRh2v5foKZUHFzc+HXXztR99GSFkclGUFi7oniq3Nv65WYdN8kIiIiIpI46SqpUrduXRYtWuRUtmLFCmrUqBHneCoiIiKpJioCbp2BjR+YXXAl1bVDCS/3zgl568DFnVDqSbh1DrIWB997HpCF3waf3FCiA3hnS3ocIvLQfv/9IMOGrbfPT57cnpYtlVCR5JGYe6K6deuycuVKp3FVVqxYQb169VI1VhERERGRjMjSpMqtW7c4fPiwff7YsWPs2rWLbNmyUahQIQYOHMiZM2eY8v/tuV966SX+97//0b9/f1544QW2bNnChAkTmDlzplWHICIimZERBdcOw+5x5vglURFwaXfy7iNvXSjYCEo9DbmrJu+2RSRFtWhRghdeqMaPP/7N118355lnKlkdkqRhKXFP9MYbb9CwYUM+/fRT2rdvz2+//caqVavYuHFjqh+fiIiIiEhGY2lSZceOHTRp0sQ+H92Hb48ePZg8eTLnzp3j5MmT9uVFixZlyZIl9OvXjzFjxpAvXz6+/fZbnnzyyVSPXUREMjDDgIg7cOkfuP3/3afcPAX7JsPFvxO/nXLd4cp+yF0dclcDVy/AAGwQUDTmDs3xSTz8ze66vLIm26GISOpzdXXhhx/a8NRT5XjsseJWhyNpXErcE9WrV49Zs2YxaNAgBg8eTPHixZk9eza1a9dOvQMTEREREcmgbEb0qIaZRHBwMAEBAdy4cQN/f3+rwxERESsZBgQfhysHzIHPN35gdq/1MAo2hoDiUPVVyFUlGYIUkfQgKsrAxSX+QcCtpmtgSar0cs5ED1TfrRu8+abV0YiIiIhIepWU6990NaaKiIjIQwm5Bju/g7+/gWxl4eymh9teYCko1REq9QGvbODuayZnRCRTOXDgEk8//QvTpnWgcuU8VocjIiIiIiIiKUhJFRERybjCb0PwKQi9DjdPwuJOjmX3S6jYXKBgE7h1Fsr3MMsiQiBPDSgcBK7uKRa2iKQfp07doHnzaZw6FUzDhpNZvbo7NWrkszosERERERERSSFKqoiISMZx7RBc3gf/zYL/Zid+PRc3KNIccteEHOWhRAdwcU25OEUkQ7hy5Y49oQJQvHggpUpltzgqERERERERSUlKqoiISNpjGGaC5NpBs8XI3ctwYBqc3Qw+ucEr0Ln+hb+Svo++V8AzwOyuy+aSPHGLSKZx61YYrVvP4MCBywCUKJGNpUufwd/f0+LIREREREREJCUpqSIiImlD8CnY8wP8+UnC9W4chRtJ3LZfQSj5hNkNWMGmULoTuOnBp4g8mLCwSDp2nMOff54BIE+eLKxY0Y3cubNYHJmIiIiIiIikNCVVREQk9Z1aBwufhJCr/19gJH0bLvf8C4uKcEw/8gl454TST5utUUREkklUlEHPngtYvvwIAAEBnixf3o2iRQPvs6aIiIiIiIhkBEqqiIhIyosMh02DYfunZldbRlTi1vMvAnlrQ/5HwMMP3LOYg8d7Z0vRcEVE4mIYBm++uYyZM/cC4OXlxqJFXahUKbfFkYmIiIiIiEhqUVJFRESSx+0LsHcShFyBwwvg+mFw84GIO8717k2oZCsDHv5mq5WQK1C6C9QdAr56SCkiacvGjSf57rttALi62pgzpyMNGhS2OCoRERERERFJTUqqiIhI0oRch91j4dA8uLwXIkPBNy/cPhe77r0JlZgq94XKL0HOiikWqohIcmrQoDBjx7bi1VeXMmFCO9q2LW11SCIiIiIiIpLKlFQREZHY7lw2W5rEdP0wrBsAdy7Grh9XQiWmgGLmAPMVn4eGn4FX1mQLVUQkNb38ck0efbQYJUtmtzoUERERERERsYCSKiIimcXNM3DzlNl6JPxO3O9nNsKJlQ++D+8cULSVmTzx8IesxcyxUERE0qlbt8LIksXDqUwJFRERERERkcxLSRURkYwkNBgOTIfT68GvABxfZrYSObIw+fdV8kmo9S7krAwu7mCzJf8+REQs9NdfZ2nRYjrjxrWmY8dyVocjIiIiIiIiaYCSKiIi6dXt8/DnSPhvNmQtYZad3RS73uW9D76PKq+YCRMADLh5Giq9CIUfBZvLg29XRCSNO3ToCi1bTufy5Ts8/fRcfv+9Ky1blrQ6LBEREREREbGYkioiIulJZBgs7w1nt5hjlES7cyHx26j9Prj5gLuP+e7m7Zh29zG77cpVRUkTEcm0zp69SVDQNC5dugNA/fqFaNy4iLVBiYiIiIiISJqgpIqISFp28wwcnAv7JsGlPYlfL1c1qPMBeOc0kya+ecDFzXwXEZF4Xbt2l+bNp3H8+HUAKlXKzaJFXfD2dk94RREREREREckUlFQREUkrIsPh+hFz+tYZWNYTbp1OeB03byj9NDT9zmxpAubYJmplIiKSZHfuhNO27Uz27r0IQNGiWVm27BmyZvWyODIRERERERFJK5RUERGxyt2r8OdwOLoYXD0TP/ZJzkqQqyo0GwPuvikbo4hIJhEeHsnTT89l06ZTAOTK5cuKFc+SN6+fxZGJiIiIiIhIWqKkiohISjMMuHMRruyDHV9AxF04tTZp2yj3LBRvD8XagJtnysQpIpJJRUUZPP/8In7//RAAfn4eLFv2DCVKZLM4MhEREREREUlrlFQREUkp14+aSZTdY5O2XsEm4F8YsEHxtlCyQ4qEJyIipn37LjJ7ttla0NPTlYULu1C1al6LoxIREREREZG0SEkVEZGHFRkORpQ5bUTCgemw8sXEr1++FzT6DLwCAZs5JoqIiKSaihVzs3x5Nzp0mM2ECe1o3LiI1SGJiIiIiIhIGqWkiojIw9gwELaNSlxd7xxQoCGU6w6FHzUHmdeA8iIiaUKjRkU4evQNDUovIoDZe+tPP8GWLfD555BNvQGKiIiIyP9TUkVEJCkMA24cg4s7YVHHxK1T6z2oPgB8cqRsbCIikmjHjl2jaNFApzIlVEQEIDIShg+HRYvM+V27oGlTS0MSERERkTRESRURkYQYBhxfBseWwsVdcGk3hAXHXbdAI/P90m4o2Bhqvw95aqZWpCIikkjr158gKGgqb79dj2HDmmBTt4si8v/CwuC992D9ekeZYVgXj4iIiIikPUqqiIjE5/x2mF7r/vWy5IeOqyB7mZSPSUREHsru3edp23YmoaGRDB++gRIlstGjRxWrwxKRNCAyEj74wEyoeHhAlixw9arVUYmIiIhIWqOkiohITJHhcGW/2b3X8l6xl/sVhJxVIFcVyFkZ8tYBv/ypHaWIiDyAo0ev0bz5NIKDQwFo2bIEXbtWtDgqEUkLoqLgo49g7VozofL11+aYKkqqiIiIiMi9lFQRkcwrKhIu74VTa8z3izvhyj6IDItdN6AYdNkEvnlSP04REXlo58/f4rHHpnLhwm0A6tYtwNy5T+Hu7mpxZCKSFowfD0uXgqsrjBoFtWqZSRURERERkXspqSIimU9UBOydBCtfTFz9st2g1dSUjUlERFLMjRshtGgxjaNHrwFQvnxOFi/uiq+vh8WRiUhasGKFI4EyaBA0bGhtPCIiIiKStimpIiKZx4W/YMULZouUuNhcILA05Kpqdu+Vq6rZ1ZdPjtSMUkREklFISATt2s1i9+4LABQqFMDy5d3Ils3b4shEJC04csTs9gvg2WehbVtLwxERERGRdEBJFRHJ2AwDTqyAX1vEXyf/I1B9ABQJAnef1ItNRERSVEREFF26/Mr69ScAyJHDhxUrupE/v7/FkYlIWhAaCu+/D2FhULcuvPaa1RGlfcHBcOsW5MtndSQiIiIi1lFSRUQyHiMKbp2D4OOw72f458e46z25HAo/arZQERGRDOfs2Zts23YGgCxZPFi69BlKl1brQxExffut2VIlWzYYOhRcdEkYp6go2LYNFi6EP/6AiAiYMweKFLE6MhERERFrKKkiIulf2C04uQaMCHN6WY+E6zefCOV7gs2WKuGJiIg1ChUKYNOm3rRtO5OvvmpOjRr6arWImHbuhNmzzemPPjITK+LszBlYtMh8XbjgvOziRSVVREREJPNSUkVE0q/IcFjUEY4sTFz9J5ZA0ZYpG5OIiKQpRYpkZdeuPri66ivoImIKD4cRI8zpDh2gXj1r40lLoqJgyxYz4bR5s6Pczw9atoR162InWEREREQyGyVVRCT9OrPx/gmVkk+CfxEo3gYKNk6NqERExEKbNp2kdu0CuLk5kihKqIhITFOnwrFjEBiocVSi3bpldu81dy6cOmWW2WxQuza0aweNG4OHh9nCR0kVERERyeyUVBGR9CkqAvb/7FxW8QUILGlO56wEhYPUxZeISCby++8Had9+Fq1bl2LWrCfx9na3OiQRSWOuXoVJk8zpfv3A39/aeKx28SJMmwbz58Pdu2ZZlizQvj089RQUKGBtfCIiIiJpkZIqIpL+nNkEsx5xLqv/MdQZZE08IiJiuU2bTvLUU3OJjDRYuPA/xo//izfeqGN1WCKSxvz0k5k8KFfO7M4qufz3H5w9C02aJN82U9KpU/Dzz7B4sTnwPECxYtC5s/m5eHtbG19qOX3a7OZs61YoWNBMtImIiIjcj5IqIpJ+HFsGJ1bBX1/EXlagcaqHIyIiacM//1ygTZuZ3L1rPhns1Kk8r71W2+KoRCStOX0afv3VnH799eRr0Lx5MwwYYI7V8ttvkD9/8mw3JRw9aiaWVq0yx08BqFYNevWCOnXSViPvyEhwdU3ebYaGwl9/mT+zzZvh5Enn5T17mt3CiYiIiCRESRURSR+uHYJ5cXyd0DMrPLMdAkukekgiImK948ev07z5NK5fDwHgsceKMWVKB1xc0tCTQRFJEyZNMh/U160LNWokzza3boW33jITKgA3bqTNpMrZszB+PPz+OxiGWfbII2YypXJla2OL6fJlWL7cjPPwYfjuO3Ncl4dx8SKsWwcbNsCOHRAW5ljm6gpVqpiJFnAkmkREREQSoqSKiKR9V/6FyWVjlxdrAx0WpX48IiKSJly8eJugoKmcO3cLgFq18jNvXic8PJL5q80iku5dugRLlpjTL7yQPNvctQv693d+SJ/WHspfuwYTJpgtdKITP02amJ9BqVIPvt2vv4bz52HUKKhV6+FivHMH/vjDTKRs3+78Ge7f/2BJlRMnYO1ac7t79zovy5UL6teHevXM2H19zfeEfnYhIWZC5tAhaN4c8uVLekwiIiKScSipIiJp2+3z8HMF57Kqr0P57pCrqjUxiYiI5YKDQ2nZcjqHDl0FoHTp7Pz+e1eyZPGwODIRSYtmzjSTClWqQKVKD7+9EyccCZVHHjEftl+4kHaSKhERMGeO2Trllpl3plYteOUVKF/+4bd/8KD5/vffD5ZUMQwzKTV/PqxZYyYtolWqZI57c+hQ0rb3779mImXtWjh2zHl5pUrQqJH5sypWLHHdnJ06ZXYRtmmTcwuXs2fhgw8SH5uIiIhkPEqqiEjatmEgGJGO+WpvQpOvLAtHRESsFxoaQYcOs/n773MA5M/vx4oVz5Ijh4/FkYlIWnTnDvzyizndo8fDb+/qVXjtNQgONhMUo0ZBly7msrSQVPnzT/j8c0dioXRpeOONh29RAmZy4uRJyJrVTCIlVXAwLF4M8+bB8eOO8oIFoVUraNkSChSAYcMSl1Q5dszsLmz5cjMJEs3NDWrWhMaNzWRKjhyJi2/rVjM5s2lT7PFWPDzMxMrt24nbloiIiGRcSqqISNpkRMH6d2HfZEeZV3Z4ZLhlIYmISNpw5044t2+bXxkODPRixYpnKVQowOKoRCStWrHCTKwULGh2+/QwwsPNMVTOnjXHTvnqK/DyAhcXc3lSkyrXr4O/v2P9h3H5Mowebbb8ADPx8eqr0K5d8mwf4L334N13zaTNnDmJWydmq5RVqxwtPry9za602reHChUS13oEzG7HVqyAZcscLWYAPD3NlihNmpg/Zz+/xB9X9M/tww8dZa6uULWqua369c1k1RdfJH6bIiIiknEpqSIiaU9kGEytBlf2OZe/eBLc9S1kEZHMLjDQm1WrutOjxwLeeqsu5crltDokEUnD5s0z3zt0ePjkwjffwJ49kCULfPstZMtmlrv+/1BOkZHxr3uv6dPN7T3+OLz//oPHZBiwaJGZ4Ll50zzGzp3NcVOSklhIDJst8cmP8HAz+TF9unPyo1QpePJJaNHCHM8kMe7cgdWrzeP8+29Huasr1K1rbqthQ/B5wFuFQoXMlikBAWbrlvr1zbFcYsa3bVvs9a5ehY0bzdf16zByJGTP/mAxiIiISPqhpIqIpA2GASdWwP5pcGBa7OXNxiihIiIidlmyePDrr09bHYaIpHH//WcOdu7mBm3aPNy2li+HWbPM6WHDoHBhx7KktFSJijIHep8xw5w/cuTBYzp7FoYPdzzwL1sWhgyBkiUffJsP68YN+PVXsyXL5ctmmaen2bXXE0+YMSY2MXP0KHz8MaxcaSZWwFy3alUzkdKsmZkIeVg//QRXrkDx4vdPvF26BD/+CBs2mOdWTJs2mS2DREREJGNTUkVE0obfu8B/s+Ne9twRyFosdeMREZE0Zc6cfTz6aDGyZfO2OhQRSUcWLTLfmzRxtCp5EKdOmckLgF69zFYRMUU/iL9fS5WwMLOLqZUrHzwWML+PtGQJfPqpmWzw8ICXXoJnnnG0mkltZ8/ClCnmZx4aapblzAmdOpnJFH//pG9z6VLHdMGCZsKidWvIlSt5Yo6WLVviz4+dO81XtHLlzITMg4wxIyIiIumTkioiYr3Dv8WdUHHzga5blFAREcnkZsz4h2eemUe5cjlZvrwbBQo8wJM5Ecl0oqIcyYtWrR58O5GRZiLk7l2oVg1efjl2nehERkItVcLC4J13zK6i3NygaVOze6ykunULRo0yxxQBqFLFbJ1SqFDSt5UcTp2CSZPg998dSaUyZaBrV3jsMXB3T/o2oxMc3t7mNtq2NY8zsS1cUkKJEo6YateGBg3MbsJy5IB+/ZRUERERyUyUVBER6xgGbPwAto10Lm/7C+SrC945wfUB7sJERCTDWLbsMD16LABg//5LzJz5D2+//ZAjTYtIpvD332YLAj8/qFPnwbczdSocOGCO1zF0aNzdQ92v+6+wMHj7bbN7KE9P+PJLuH076UmVvXvN8VfOnjX32aeP2XImuQaiT6qffoKJEx3HXasWPPecmXx6mATIiy9CzZpQseKDj5OS3GrUMJN0vr5my6D4nDoF69bB+vVm8uzrrxOuLyIiIumPkioiYo2Lu2BqVeeyUk9B8wngkcwjaoqISLr055+nefLJOUREmE/rXnyxGm+9Vc/iqEQkvYhOWDRt+mCtJaIdOGC+v/025M0bd52EWqqEhcFbb8HmzWZC5euvzYTBmjVJi2P+fLO7r4gIyJfP7I6sUqWkbSMlREVBvXrw/PPJF4+Hh9kaJK0JDEx4+Vdfwc2bzmX//APVq6dcTCIiIpL6lFQRkdR1/QjsGgN/feVcXqYrtJpmbZt+ERFJM/bvv0SrVjO4cyccgCeeKMvYsa2x6f+EiCRCVJQjaREU9ODbiNawYcID3cfXUiUqCgYPdiRUvvnGbPGQFOHh8NlnMG+eOd+kidkdWZYsSdtOcqpQAX75BR55xEymlCtnXSxpgdv/P1m5edNMsFWvDsePw8WLEBxsaWgiIiKSApRUEZHUcX47/PYE3Dode1m++tBikhIqIiICwMmTN2jefBpXr94FoEmTIkyf/gSurhb1byMi6c6+fXD9upl4eNBWArt2OabffTfhS9W4Bqo3DPj8c1i92mwp89VXSU+oXL1qtnLZs8fc/8svm919WX3Z3KoVNG/uaKGT2XXvbrZiqVbNbLXj7w9vvhk7qXL7ttkF3Nq15vn16qvQurVVUYuIiMiDUlJFRFLenvGwsk/cy/LWhc4brL8zFBGRNOHy5Ts0bz6N06fNp1DVquVlwYLOeHnpslVEEm/TJvO9Th1HK4KkatrUbO3StSvkzp1w3bi6/5o0CebMMS9zhw0zxxtJipMn4bXX4MwZMzn0ySfmwOhphRIqDhUrmq+Y/P3N9xMnzFZGf/wB27aZ3bdF27hRSRUREZH0SHenIpKyIsNgbb/Y5XU/gsKPQt46SqiIiAgAt2+H0br1DP799zIAJUpkY+nSZ/D397Q4MhFJb6KTKg+ThHj7bWjf3mx5cD/Rl7PRLVVWr4axY83pAQPgsceStu+9e82WDtevm+On/O9/UKhQ0rYh1opOqkyZ4lxeuDBkzQq7d6d6SCIiIpJMlFQRkZRzYDos6eZc9th4qPSCNfGIiEia5unpRrlyOdm27Qx582ZhxYpu5Mrla3VYIpLOXLniGFw+MQmR+OTMab4SI2ZLlUOHzDFPwGzl0rlz0va7caPZ3VhoqDlWyddfQ7ZsSduGWK9YMcd0+fLQuLE5Hk6RImYLJiVVRERE0i8lVUQk+YUGw39zYOU9yRPfPFC+pyUhiYhI2ufm5sLEie0oVMifjh3LUbRooNUhiUg6tHWr+V62LGTPnjr7jB5T5epVs2VKSAjUrg1vvJG07axdCwMHml1E1a8Po0aBt3fyxyspr107s3VRoUKQK5fV0YiIiEhyUlJFRJLXkUWwoF3s8pxVoN2v4Oqe6iGJiEj6YbPZGDq0idVhiEg69tdf5nvNmqm3z+iWKuPHmwmVAgVg5MikjTuyejW8/77ZhVhQEHz8scYtSc/c3KBGDaujEBERkZTgYnUAIpJBrHwJvvGNO6HSbAx03wlZi8VeJiIimdo332xl376LVochIhnIzp3me7VqqbfP6JYqISHg6QlffOEYUyMxVqwwW6hERkKrVkqoiIiIiKRlSqqIyMMxDNjxBez5ASLuOC/LXh6eXAGVX7ImNhERSdO+/34Hb765nAYNJrFlyymrwxGRDODSJTh1yhw4vkqV1NuvS4w767ffhuLFE7/usWMwaJA5HkvbtvDRR0qoiIiIiKRl6v5LRB6MEQW7v4fVr8Relrs6lOsO1V5P/bhERCRd+OWX/fTt+zsA166FsHXraerWLWhxVCKS3kW3UilVCrJkSb39BgSY70FB0L590tYNDjbfW7WCwYOdEzSSORgGHDgAy5fDwYPmWDxlylgdlYiIiMRHSRUReTArX4J/foxd3noWlOmU+vGIiEi6sXr1UZ55Zh6GYc6/8049+vWra21QIpIh/P23+V69euru94UXoFw5aNnSbCWTVA0bwpAhSqhkNleuwLhxZvdvp2I02Jw3zxxfR0RERNImJVVEJGlCb8DsxnBpl3O5b154ajVkL2tFVCIikk7s2HGWxx+fTVhYJAC9elVh1KhHLY5KRDKKPXvM98qVU3e/uXJBhw5JW6dYMXB3N8d+GTXKHNhcMpe//3YkAj09IX9+OHoULl+2Ni4RERFJmC7bRCTxbp6B8QVil794CvziKBcREYnh4MErtGw5nVu3wgBo164048e3xfYgX+sWEbnH3btw+LA5XbGitbEkRpEisHo1eHs/WOsWSb9y5DDf3dygXj2z27iGDWHHDujf35FUCQ6GTZugcGGzJZSIiIikDUqqiEji3L0Ks+rHLu/wuxIqIiJyX2fOBBMUNJXLl+8A0KBBIWbNehI3N/V1IyLJ47//zMHec+Y0W46kBz4+VkcgVmjSBKZMgQIFwN/fUR6dbDl5EgYMMBMqERGQNy8sWmRNrCIiIhKbkioikji7/gfBJxzzeetCpz/A1cOykEREJH0ICYmgefNpnDhxA4BKlXKzcGEXvL3dLY5MRDKSgwfNdw3wLWmdzRZ3y5OcOc33W7dg3TpH+e3bqROXiIiIJI6+Gigi9xdyHTZ/6FzWZpYSKiIikiheXm688EI1AIoWzcqyZc+QNauXxVGJSEYTnVQpWdLaOEQeVPbs0KABFCoEvXrB6NGx69y+DYsXQ9++Zpdh69enfpwiIiKZnVqqiIiDYcD1w+Zg9P/NgYNznFunROu2A/wLpX58IiKSbr3xRh3y5MlC9er5yJvXz+pwRCQDik6qlCplbRwiD8rFBb76yjF/4v9vxaKizOTJsmXwxx8QFuaos2uXmVwRERGR1KOkikhmducS7J8Cdy7C9ji+BhUXn1yQvULKxiUiIhlSp076/yEiKSMy0jFIvZIqktHcumUOYB+tSBHw8HAkEkVERCR1KakikhmE3YR/Z8Plf8A7O9y9ArfPwsFfkradst2gyTfg5pkycYqISIZgGAYDB66mbt0CtG+vwQ1EJOWdPGl+e9/Lyxz8WyQj8PV1TGfPDs2bQ6tWULo0fPutI6ly5AjcvAlVqlgSpoiISKajpIpIRhcVCePyQMSdxK+ToyLkrgY+eaB8d/AvDO6+919PREQE+PTTTXz66SZcXGz89FNbevWqanVIIpLBHTpkvpcsaXahJJIR5MgB48aZvTTXqBH3uT1nDkyZYk7PmgUlSsS9LcOAfftg6VLYuhV69oS2bVMsdBERkQxNSRWRjOzcNphRO+E6Lm5QvB1UeQWylYEs+VInNhERyZB++ulvBg5cDUBUlIFhWByQiGQKFy+a7xqkXjKamjXjLnd1Nd9DQhxl16/HrnfihJlIWbYMTp92lK9eraSKiIjIg1JSRSSjOr4Sfg2KXd5iMgQUB7/84JsX3LxSPTQREcmYFiz4lz59FtvnR41qRu/eaqUiIqlH46lIZtG2LVy4AJUrw9SpcOaMY9nly7BihZlMOXDAUe7lBfnzm92FiYiIyINTUkUkozGi4PSG2AmVEo9Dq+ng7mNJWCIikrGtW3eczp1/ISrKbJrSv38d3nmnvsVRiUhmU6yY1RGIpI7CheHjj83pOXPM902bzK7Atm6FqCizzMUF6taFli2hYUNYtQqGDbMmZhERkYxCSRWRjOTIYlgQRxvuFpOhfI9UD0dERDKHXbvO067dLEJDIwF49tlKfPZZEDabzeLIRCSzKVLE6ghErDN1qmO6UiVo0QIeewwCA2PX3bjRHNzezy/+7RkG/POP2XXYqlWQO7eZtInv33tUFPz9t9m1mLs79OsXf12AiAjYsQP++suMUy3NREQkvVBSRSSjOL0+7oRK8fZQpkvqxyMiIpnCkSNXadFiGsHBoQC0alWSCRPa4eKihIqIpC5f37gfHotkdDlywNGjkCcPtGkDrVtDwYJx1/WJ0XHBP/9AvXqx6xw5YiZSli+Hs2cd5VevmmO4eHs7yqKiYNcuM+myapVZJ1rnzpDvniE7Q0PNljRr1sD69WZiB8yxX0aPTtJhi4iIWEZJFZH07vI+mNME7l5yLndxg6fWQIEG1sQlIiIZXkREFG3bzuTChdsA1KtXkLlzn8Ld3dXiyEQkMypcOOFvxYtkVKNGmYPQlyljdveVkIYNHdPRXYSBmTxZscJMphw+7Cj38YFHHjGXxVzvn39g5UozkXL5smOZvz/cumXWiTQbsHL7ttk12Zo15vvdu476rq5mvdDQpB+3iIiIVZRUEUmvDi2AJV0gIiT2srofQr2PUjsiERHJZNzcXPj225Y8/vgsihTJyqJFXfDxcbc6LBHJpAoVsjoCEWv4+0O5comr6+Fh1t2/30x2/PYbLF4MO3c66ri7Q/360Lw5NPj/7+hFJ1W+/RbWrYOLFx31s2SBJk3MLrxq1YKmTeHOHTPp8s8/ZsuU8HBH/Tx5zDpNmsDJk+bYMFeuwMKF5j49PR/u8xAREUlpSqqIpEfBp2Bhh7iXtZwKpTulbjwiIpJpPfpoMf74oyd582YhWzbv+68gIpJClFQRSZoPPnBM22xQo4Y5DkuTJmaiJlpIjO/xzZ1rvvv6QqNGEBRkJlI8PGJvf+xYx3ShQmYipWlTKFvW0ars9Gnz/d9/Ydgwc19PP508xyciIpJSlFQRSesMAy7vhT/6wcnV8ddr8CnUeif14hIRkUzJMIxYA9DXqJEvntoiIqmncGGrIxBJH2ImQIoUMcdhadUKcuWKv36hQnDhgplIad4c6taNO5ECkDevOS5L8eLQrBk8+igULRp393z58zvP37rlmA4Li38fIiIiVlJSRSStCb8LJ1fBwbmwf+r961d5FZp9l/JxiYhIpmcYBs8/v5AiRbIyaFDDWMkVERErxTcwt4g4e/VVc2yTRo2gfPn7j0Xk4gKzZpnTiUly/PgjBAdDgQL3r1utmtn92JgxsHSp2XLlhx9g9Wo4etRsTdMhnk4aRERErKKkikhaEhUBP5eHG8cSV79yX42dIiIiqWbgwNVMnLgLgBs3Qvn88yBrAxKRTC/mgNdKqogkTpUq5ispktJixN/fufuw+8mTxzGOysKFzst27VJSRURE0h4lVUTSgvDb5jgpf/SLP6ESWBJqvgvluoOrBgEWEZHU9cUXm/n0002A+Y3WOnUS8fVTEZEUdvasY9rPz7o4ROThZMtmvnt4QJ064OYGa9aYXYCJiIikNUqqiFjpzCaY9Uj8yx8ZCRV6gW/u1ItJRETkHj//vIu33lppnx87tjUdO5azMCIREdO5c1ZHICLJ4YUXoHZtcxB7Hx9YsMBMqqxcCcOHg6vrw+/j2jX4+2+oWtWRxBEREXkQSqqIWOXYMpjXMv7l7RdAifapFo6IiEhcFi8+yHPPOfriGDasMS+9VMPCiEREHM6csToCEUkO7u5QvbpjPmbS48gRKFXKMR8aCvv2Qbly4OWV8HYvXoQ//jDHaNm5E6KioGlTGD06WcMXEZFMRkkVkdQSGQ5bPoI9P8LdS3HX8S8MpZ6CGgPAN0+qhiciInKvjRtP8tRTc4mMNAB49dWaDBrU0OKoREQcGjWCFSvMb56LSMbxSIwOHSIjITwctm41f9/XrYM7d+CJJ+D992Ove/asmURZuxb27Im9/ObNlItbREQyByVVRFKDYcCM2nBxZ9zL286FUh1TNyYREZEE7NlzgTZtZhASEgFAly4V+OablthsNosjExFxeO89qFEDHn3U6khEJDm5uEDu3HDhAowdC3v3xk6GXL7smD5+3OwubPVq+O8/53qVKkGzZhARAd99l+Khi4hIJqCkikhKCr8Nu7+HdW/FXuabB7LkhyeWgk/O1I9NREQkHoZh0LPnAm7cCAUgKKg4kyc/jouLEioikrb4+5vfVheRjGvLFvM9Rw4zgWoYMHs2XL8OEyearVcOH3bUd3ExuxJr0sR85fz/2+0VK8z37dvhr7+cuxsDCAmBHTsgVy7n7sZERETupaSKSEraNQ7Wvx27/PmjEFA09eMRERFJBJvNxty5TxEUNI0cOXz49den8fBIhhFiRURERBKpfn3YuBEaNIDmzaFKFTNhsmCBuXzPHkf3Xm5uUKuW2SKlUSPImjX29kqXBpvNTMr06QOtWsELL5jjs6xZA5s3m4mVLFnMcVhERETio6SKSEoxDDg837nMJxc8tVoJFRERSfOKF8/Gpk29cXNzIUsWD6vDERERkUwmrvFSwGyxAmaCpVYtCAqCxo3NlmsJKVzY7B5szBiYNw+WLDFf97p166HCFhGRTEBJFZGUsuE9OLvZMf/UGijUxLp4REREEhAaGoGbmwuuri72sjx5slgYkYiIiEhs9evDlCmQNy8EBiZtXX9/GDgQ2rWDkSPh33+hSBFo2hSqVoXXXjPrzZ8Pjz9utmxJb6KizOPavNns5qxmTejdO3a9iAizlU5gIBQqlPpxioikZ0qqiKSUo4sd066ekLOydbGIiIgkIDIyiq5d5+HiYmPatA54euoSUURERNImmw3KlXu4bZQvD1Onmq1S/PzMsmvXHMs/+cQccyW9JBtu3ICtW2HTJvP96lXHsp07oWdPs2XP6dPmGDVbtpjjx9y5Yx7/smXg6WlZ+ABcvmx2+JFTQ86KSDqgO2aRlLDlY7iy3zHfcSV4Z7MuHhERkXgYhkHfvr8zb94BAMLCIvntt84WRyUiIiKSsmw2R0IFzHFY6tc3ExNgjq9ihTNnzARDpUrxt5SJbo2yaZPZImXfPrMsmo8P1K4NGzaYLVI++sgcf+b06djbunkTjh2DgwfNhMzff0O9ejBkSOx9Hj7sSNo8/7w5/syDunHDbEmzfbv5On7cMWZOvnwPvt205uZN+OcfuHsXmjQxj1FE0j8lVUSSW2gwbBnqmM9SAAo0sC4eERGRBAwZspbx4/8GwM3Nhb59a1gckYiIiEjqs9ngm2+geXO4cgUGDICCBc0xWFKyGzDDgAMHYN06+OMPOHLELP/6a3jkEUe9mzfNBMqmTWZLk5gtawBKlDCTIfXrmwkZd3d44gk4edIxdoyrK1SuDHXqQN268OyzZnm3bs7bWrQI3n7b3Oeff5qJlG3bnPdZpIjZRVpi3b5tJmy2bzdbyRw8GLtOVJSZgMibN312vWYY5ue9Z4/52r0bjh51LP/yS2jY0GwhtHev+dq/3+x67plnHmyf4eFw6JBje9evw7vvQv78yXJIIhIPJVVEkltkKBiRjvmGn1oXi4iISAK+/fZPhg/fYJ+fMuVxmjcvYWFEIiIiItZydTXfz50zXzdvmmOxJKfwcDOxsG4drF8PFy/GrnPyJJw6ZbY2Wb/e7MYrMsajBh8fMzlSr575ypUr9ja6dYN586BiRTOJUr06+Po6lmfPbiaQXFzMLtHq1IG5c80H808/bR5/TN7eZrImONhMvDRsCNni6ZQjJMRMKkS3RDlwwLk1DUCxYuaYLzVrwrffmsf8wQfm+wsvOOrdvWtuy8/PjDOtCAkxkyK7dzsSKTduxK7n6mr+7CZOhLFjzcSZYTiWb9iQuKSKYcDZs44Eyt698N9/EBbmXO+336BGDbN7u4YNwU1Pf0WSnc0wYv4aZ3zBwcEEBARw48YN/JP7v6IIwJ1LMO7/r2aKtYUOC62NR0REJA4zZvzDM8/Ms89/800LXn+9toURSUrSNbAklc4ZEcms5swxEx5r1pjzo0ebLT9y5Ih/nZMnzTFJcueOv050S5N162DjRrO1QjRvbzPp0bix+YB95Uqz7O5d520UK2a2XonZGuVhHD1qdglWpYojcfTmm2Z8YCZbypY1ky21a5vJmSFDzPgAWrWCYcPM6eguybZuNV979phdj8VUsKCZQKlRw3zFTMi8/TasXWtO16tntqLZscNMyOzbZyYlXF3NVjfZs5sJhhMnzC7IEvrZJKcLF5wTKP/955zoAvDwMMf8qVTJbBVUsSKMHOk4tmh580LJkmbCDKB4cfjxR+cE3s2b5rFHJ1D27YvdQgnMdSpUgPPnnVvGgLnvRx81E3f79pk/o4oVoUEcHarcvWsuP3XK/Bmk1ucqklYk5fpXuUoRERGRTGbZssP06LHAPj9oUAMlVEREREQwW2h07Ai1apnz77xjJh1++sm53okTsGKFmWA4etR8sL1ihXOrgOvXzSTKqlVm91kxH8Bnzw6NGpmvmjXNh/Fgjm8C5gNuV1eoVs18AN6wIRQokLzHWqyY+YrpjTegVCkoU8ZMfNz7XLFBA0dS5cQJmD/f7CJs2zazBUtMuXKZn2N0IiWhpNOwYWb3a7/8YiafNm+OXScyEmbNMsed2bHDHNslSxYz0eLjk/jjDgszW5icOmV+/nE9OzUMc/nff5uvXbvMViL3ypnTTJ5UqmS+SpeOnezq0sXcXqFCZkKjYkUzYRESAo89Zv6sjxyBhQvBy8uRRDl+PPb+3NzMn0+FCo5XwYJmd2mTJpnd1YGjdcy4cfD552aLpGg+PrB6tbnPffvM1/795nkc3ZqoTRtzLB4RiZtaqogkN7VUERGRNGzr1tM0azaFO3fCAejTpzrjxrXGlh47rpZE0zWwJJXOGRHJzAwDmjVzJAmKFDEf9p86ZSYUVq40x7G41/r15oPyP/4wEyk7djh3eVW0qCORUr583IOWX74Mv/5q1q1b1+zyKq1ZsiT2QPZgdi1Wq5bZqqVWLcfD/sTavBlef92czpnTTMREJ2R69Ii7lQaYiS8wEwFxJVeikyh//WW+du+G0FBzWe/e0Lev+XM6fNhMoOzcab6uXnXejouLmdCoXNmRSMmd++HGfzl4ELp2jX95/vzOCZTSpR0JuHtFRJjHkDu32Z3aokXOsRcv7jhvPTxidxsGZour6M/mlVegV68HOy6R9EgtVUREREQkTh9++Ic9ofLkk2UZM6aVEioiIiIiMdhsMHmyOSj8F1+YiY5u3cyukaK5uprdYjVqBCNGmGWvv252CxUzkVK6tJmgadYMChe+/75z5IA+fZL1cJJd0aLmu4uL2eqidm3zsyhf3jEmzYOoW9ccdyRr1tgJmbp1zURVxYrm2DA1asBrr5kJgNGjHfWeftocs2bfvriTKNFcXMyf08aNZjdeu3ebY5DE5OFhJjKqVDFbDFWqlLQWMYlRqhS0bWsmQHx9zf1VrGi+ly8PgYGJ35abm9nCCMzu03x8HEmZ6GRMUJCZnAoLMxN25cqZ+ylf3pz+5x9HkmrcOPOzPnDAnH/qqdiJwMhIs0y3E5LZqKWKSHKKCIXZDeH8NnNeLVVERCSNCQ4OpUOH2QAsWdIVT099xyYz0DWwJJXOGRERs9XCiy865l1czBYYjz0GTZqY3UaFhppjnMRUrpyZRGna1EwOZESXL5tdVWXJknr7jIpyfqj/wgtmi5LoVheVK5stLeJKomTLZiYIol+rVsEPPzjX8fExt1G1qplEKVcu/lYhyX1c589Dnjxxt15KTocPm918lSkTd0ui8HBzbJeJE2OvO3asGeP+/Y5uw/7910wEzp1rfvYi6ZlaqohY5dQaR0IFwCurZaGIiIjExd/fkyVLuhIWFqmEioiIiEgCSpY0v8Hv4+NIpNzbcsDTE1q2NMfbaNzYTKbky2dJuKnKikHM7004/PCD2bpkzBizy7Tdux3LAgMdCZQaNcwu3GImEJo1M7sby5HDTKJUrWq2GnmYljYPysUl9c6ZEiXMV3zc3c1E4qJFcOmS+TmGhJjjvrzxhpl0udfZs2a3YhUqpFzcImmN7qRFklP4Hef5Gm9ZE4eIiMj/u3UrjIiIKLJm9bKXeXq6KaEiIiIich9+fvDzz/ev9/HHKR+LxObiYrYWCgoyB3YvWNCRSClaNOEuqYoVMwd2l9jc3Mwk1Y0bZsuUt982xwkKDzdb7pQu7egy7MMPzZY2334L48fDzZvmmERq5CoZne6mRVJKw88gZyWroxARkUwsNDSCJ56YzYULt1m27Bny5k2DI52KiIiIiDyE6tVh+nSro8hYfHwc48e8847ZlV3RomYrF3d3R71Jk8zuxA4fhieegJMnzWTX/PnmeC4iGVUK99Qnkskc/MXqCERERACIjIyiR48FrFx5lD17LtC27Uwy2VB6IiIiIiLykHLlglatoGxZ54QKwKuvmu/BwWZCBcyWK8ePp2qIIqlOLVVEHtaJ1bD4aQi56lzuGWBNPCIikukZhsEbbyxj9ux9AHh7u/HNNy2wJdQHgoiIiIiISBLUrQu9e5tdhlWoAF9/bbZcEcnolFQReRBht2Dnt3BlPxyIo41pvvpQ9pnUj0tERAT4+OP1jBmzHQBXVxtz5z5F/fqFLI5KREREREQyEnd36NvXMf/99+b7V1/Bn39C//7WxCWS0pRUEXkQ+ybDxg/iXlb5ZXh0bKqGIyIiEm3cuO18+OEf9vmJE9vTunUp6wISEREREZFMwcvLfD9+3HyVKgWRkdCypTnIvUhGoaSKyIM4uzl2WYNRUOvd1I9FRETk/82Zs49XXllin//iiyC6d69sYUQiIiIiIpJZvPoqrFwJM2ea8x99ZL57ekKLFpaFJZLslFQRSaodX8K/Mx3zraZB4SDwyWldTCIikumtWnWUbt3mET0W/Xvv1ad//7rWBiUiIiIiIplGpUpQsSLs2QMHDpjdg4WGwn//KakiGYuL1QGIpCsHf4V1A2IU2CB/AyVURETEcnPm7CM8PAqA3r2rMGJEM4sjEhERERGRzMZmg8mTYeNGaNDALNu5Ey5dsjQskWSllioi9xNyHf6bDSFXYo+j0u5X8NfAvyIiYr3vv2+Dl5cbJ0/e4Icf2mKz2awOSUREREREMiGbzRxDpWFDWLUK9u6F1q1hxgwoUcLq6EQenpIqIgnZNwWW9Yh7WZ1BULJD6sYjIiISDxcXG99804KIiCjc3NQYWURERERErNW0KYwZAxcuQFQUzJkD779vdVQiD0933CIJ2Tcp7vLa70P9j1M3FhERkRiuXr3LwYNXnMpsNhvu7q4WRSQiIiIiIuLg5QXz50Pjxub8woVw/Trs3w/h4VZGJvJwlFQRScitc47pct2h3TzovBHqD7cuJhERyfRu3w6jTZsZ1K8/ke3bz1gdjoiIiIiISJw8PKB9e3M6IgIefRS6d4epU62NS+RhqPsvkbhEhMKs+nDtP0dZ8wngol8ZERGxVnh4JE89NZctW04D0LXrPA4ceEVdfomIiIiISJpUujS4uJhdgEU7fdq6eEQelp4Qi9wr/A5MLAW3Ynzz1yc32PSwSkRErBUVZdC790KWLj0MgL+/J3PnPqWEioiIiIiIpFm5csGPP0JwMOzcCVOmWB2RyMNRUkUkprtX4Phy54QKQJtZSqqIiIilDMNgwIDlTJu2BwBPT1cWLuxMlSp5LI5MREREREQkYZUrm+9HjpjvCxdC+fLw5JPWxSTyoJRUEQG4cRzmNIbgE7GX9TkLWfKmdkQiIiJORo3ayNdf/wmAi4uNWbM60qhREWuDEhERERERSYLAQMf0zz+DYUBQEPj7WxeTSFLpq/eSeRlRcPR3+LUF/FQ07oRKszFKqIiIiOV++ulv3n9/jX1+/Pg2PP54GQsjEhERERERSbqWLc0XwNmzMGoUzJplbUwiSaWWKpK5hFyHwwvg3xlwYmX89cp2g2yloXyP1IpMREQkTvPnH6BPn8X2+VGjmvHcc9UsjEhEREREROTBeHjAgAGwdy9cvAihoTB+PPToAZ6eZp2rV82xV/bsMbsICwqyNmaReympIplH+F1zAPq7l+KvU647tJgMNluqhSUiIpKQY8euExVlANC/fx3eeae+xRGJiIiIiIg8uKxZYf58mDMHRo82y8aMgVu3YNcuOHnSUdfdHZo1A1dXKyIViZuSKpI5RIbDyhfiT6i0nQt5aoF/odSNS0RE5D76969LYKAXGzac5LPPgrAp8S8iIiIiIhnA4487kiozZjjKbTYoUQIOHYLwcNixA2rXtiREkTgpqSKZw8E5cGC6c1mXzZC3jlqliIhImterV1V69apqdRgiIiIiIiLJxsMDevc2EyqlSkHVquarcmXw84MaNcx6SqpIWqOkimR8RxbDkm7OZU+vhXx1rYlHREQkAefO3WTv3os89lhxq0MRERERERFJUX37wssvx/2d50cfhVWrYNIkeOWV1I9NJD4uVgcgkuL2TXaef3wRFGxsRSQiIiIJun49hBYtptOq1QymT99jdTgiIiIiIiIpLr5OZAr9fy/9Hh7QqRNs25Z6MYkkREkVydi2fw6HfnXMV3sDirW2Lh4REZF43L0bTtu2M9mz5wIREVEMHryWu3fDrQ5LRERERETEEo0ame9hYXDkCPz6a8L1RVKLkiqSsf31hWPa1RPqD9cYKiIikuZERETRqdMvbNx4EoCcOX1Yvrwb3t7uFkcmIiIiIiJijXLloH9/qFbNnF+92hy4XsRqSqpIxmREwc0zcPu8o+yxH8Aji3UxiYiIxMEwDF54YRGLFh0EwM/Pg2XLulGyZHaLIxMREREREbGOzQZdu8KbbzrKbt+2LBwROw1ULxlL2E3Y9in8+YlzeWApKN/DmphEREQS8O67q5g8eRcAHh6uLFjQmWrV8loblIiIiIiISBpRpoxjev16yJbNbMWSLZt1MUnmpqSKZCzbR8dOqADkqJD6sYiIiNzHZ59t4rPPNgPg4mJjxownaNq0qMVRiYiIiIiIpB0uMfpaGjbMfG/cGD79FFxdLQlJMjl1/yUZQ1QEHJgJW4c7l7t6QJ3B8NiP1sQlIiISj59/3sU776yyz48b15onnyxnYUQiIiIiIiJpU1CQ8/z69VC3LsycaU08krmppYqkb5HhsPVj83Wv7nsgZ8XUj0lERCQRcuXyxdvbjbt3Ixg+vAkvvljd6pBERERERETSpPffh6eegpAQeO01iIoyy7/4wnzv0sW62CTzUVJF0rfjy+JOqOStoy6/REQkTWvZsiSrV3fn998P8f77DawOR0REREREJM3KkgWqVgXDgNGjYfNmWLDAXPbFF5A3r9lyxdPT0jAlk7C8+6+xY8dStGhRvLy8qF69Ohs2bEiw/vTp06lcuTI+Pj7kzZuXXr16ceXKlVSKVtKc2xec53NXh8cXQZdNYLNZE5OIiEgi1a1bkOHDm2LT/ywREREREZH7stmgaVOztUqrVo7yt96CSZOsi0syF0uTKrNnz+bNN9/kgw8+YOfOnTRo0ICWLVty8uTJOOtv3LiR7t2789xzz7Fv3z7mzp3L9u3bef7551M5ckmTgn6CbjugeBuwWZ4vFBERcXLs2DX+979tVochIiIiIiKS7gUEmIPWly/vKPvjD8vCkUzG0ifPX375Jc899xzPP/88ZcuW5euvv6ZgwYKMGzcuzvpbt26lSJEivP766xQtWpRHHnmEPn36sGPHjlSOXERERCTxLly4RVDQNF57bSlvv70CwzCsDklERERERCTdmzQJevY0p2/etDQUyUQsS6qEhYXx119/ERQU5FQeFBTE5s2b41ynXr16nD59miVLlmAYBhcuXOCXX36hdevW8e4nNDSU4OBgp5eIiIhIagkODqVly+kcPnwVgN9/P8TNm2EWRyUiIiIiIpL+ubhA/frm9IUL5pgrIinNsqTK5cuXiYyMJHfu3E7luXPn5vz583GuU69ePaZPn06nTp3w8PAgT548ZM2ale+++y7e/YwcOZKAgAD7q2DBgsl6HCIiIiLxCQmJoH37WezcaV7bFCzoz/Ll3fD31+iJIiIiIiIiyaFsWcf0iRPWxSGZh+UDT9w7MKthGPEO1rp//35ef/11hgwZwl9//cWyZcs4duwYL730UrzbHzhwIDdu3LC/Tp06lazxi4UMA85tsToKERGROEVGRtG166/88cdxALJn92bFimcpWDDA2sBEREREREQyEC8vx/QPP1gXh2QeblbtOEeOHLi6usZqlXLx4sVYrVeijRw5kvr16/P2228DUKlSJXx9fWnQoAHDhw8nb968sdbx9PTE01PfBs1wbhyDGXXgzkWrIxEREYnFMAxefvl35s//FwBfX3eWLHmGMmVyWByZiIiIiIhIxhMYCNeuwZ9/Wh2JZAaWtVTx8PCgevXqrFy50ql85cqV1KtXL8517ty5g4uLc8iurq4AGvA1s1n+XOyESs7K1sQiIiJyj0GD1vDjj38D4O7uwrx5nahVK7/FUYmIiIiIiGRM3bub71FR1sYhmYOl3X/179+fn376iYkTJ3LgwAH69evHyZMn7d15DRw4kO7RvxFA27ZtmTdvHuPGjePo0aNs2rSJ119/nVq1apEvXz6rDkNS2/HlcGqtc1mn9ZCnhjXxiIiIxDBu3HZGjNgIgM0GU6d2ICiouMVRiYiIiIiIZFxFi5rvt27BjBnWxiIZn2XdfwF06tSJK1euMGzYMM6dO0eFChVYsmQJhQsXBuDcuXOcPHnSXr9nz57cvHmT//3vfwwYMICsWbPStGlTPv30U6sOQVKLYcD5bXB5L6x43nlZv3BwsfRUFhERsWvSpCiFCgVw8uQNvv22JZ06VbA6JBERERERkQytZEnH9JdfQteu1sUiGZ/NyGT9ZgUHBxMQEMCNGzfw9/e3OhxJrL2TYHnv2OU134WGo1I/HhERkQScPh3MwoX/0bdvTatDEQF0DSxJp3NGRERE0pulS2HwYHN6zhwoVszaeCR9Scr1r6Xdf4kk2plNscuqvKqEioiIpEkFCvgroSIiIiIiIpKKHn3UMX3rlnVxSManpIqkfREhcHi+Y77mO9B2LjT+wrqYRERE/t++fRd56aXFhIVFWh2KiIiIiIhIpuXuDgUKmNPz5sHixdbGIxmXBqKQtG3fFFjWw7msXHfIUd6aeERERGI4ceI6zZtP48yZmxw7dp1ff32aLFk8rA5LREREREQkU1u82HzVrw+BgVZHIxmNWqpI2hV2K3ZCxTMrBBS1JBwREZGYLl26TVCQmVABuHr1LplsqDoREREREZE0JSgIcuZ0zL/8MhgGBAeb7yLJQUkVSXtCrsHafvCdn3N5sTbQfRe4+1gSloiISLSbN0Np1WoGBw9eAaBUqewsWdIVPz9PiyMTERERERHJvPr2NQesz5LFnD98GFq3hqZNYcoUa2OTjENJFUl7do2Fv792LqvxFnRYBP6FLQlJREQkWmhoBB06zGbHjrMA5Mvnx4oV3ciZ09fiyERERERERARg2jTH9MWL5vtPP1kTi2Q8GlNF0p5r/znPV3oR6n9sTSwiIiIxREZG8eyz81m9+hgAgYFerFjRjcKFs1obmIiIiIiIiNgVKACvvw5XroCrq9lK5e5diIoCFzUzkIekpIqkLTvHwP6pjvkuWyBfHeviERER+X+GYfDqq0uYO3c/AN7ebixe3JXy5XNZHJmIiIiIiIjcq3t38/3qVXX9JclLeTlJGyLD4epBWP92jEIb+BW0LCQREZGYxo3bwfff/wWAq6uNX355mnr19H9KREREREQkLXN1dUxv325dHJJxKKki1gu9AT8VhUmlIeKuo7z5RPDLb11cIiIiMXTtWpEGDQoBMGlSe1q1KmlxRCIiIiIiIpIUr7wCp05ZHYWkd0qqiPVOrYNbZ5zLspeDCj0tCUdERCQuWbN6sXx5NxYs6MSzz1a2OhwRyUDGjh1L0aJF8fLyonr16mzYsCHB+mPGjKFs2bJ4e3tTunRppsTRn8XXX39N6dKl8fb2pmDBgvTr14+QkJCUOgQRERGRNMvfHxo1csx36ABbt4JhWBeTpG8aU0WsZ0Q6pnNVg7y1oOKL1sUjIiLy/wzDwGaz2ee9vd1p376MhRGJSEYze/Zs3nzzTcaOHUv9+vX54YcfaNmyJfv376dQoUKx6o8bN46BAwfy448/UrNmTbZt28YLL7xAYGAgbdu2BWD69Om89957TJw4kXr16nHw4EF69uwJwFdffZWahyciIiJiOZsNvvgCeveGPXvMsldfBQ8P6NMHevSwNj5Jf9RSRax37ZBjunQneHQc5K5qXTwiIiLA9u1naNLkZy5evG11KCKSgX355Zc899xzPP/885QtW5avv/6aggULMm7cuDjrT506lT59+tCpUyeKFStG586dee655/j000/tdbZs2UL9+vXp2rUrRYoUISgoiC5durBjx47UOiwRERGRNOfzz6FsWcd8WBj8+ad18Uj6paSKWOf6UZhQAja8a3UkIiIiTv799zItW05n3boTPPLIRM6cCbY6JBHJgMLCwvjrr78ICgpyKg8KCmLz5s1xrhMaGoqXl5dTmbe3N9u2bSM8PByARx55hL/++ott27YBcPToUZYsWULr1q3jjSU0NJTg4GCnl4iIiEhGki0bjBgBnTtD8+ZWRyPpmZIqYp11A+D6Eeey7OWsiUVEROT/nT4dTFDQVK5cuQtAvnx+ZM/uY3FUIpIRXb58mcjISHLnzu1Unjt3bs6fPx/nOs2bN+enn37ir7/+wjAMduzYwcSJEwkPD+fy5csAdO7cmY8//phHHnkEd3d3ihcvTpMmTXjvvffijWXkyJEEBATYXwULFky+AxURERFJIwoWhLfeggYNzPlt22DFCmtjkvRHSRWxTvBJ5/nWs6BYK2tiERERAa5cuUNQ0FROnTK/oV2lSh5++60zXl4ahk5EUk7MsZsg9nhOMQ0ePJiWLVtSp04d3N3dad++vX28FFdXVwD++OMPPvnkE8aOHcvff//NvHnzWLx4MR9//HG8MQwcOJAbN27YX6dOnUqegxMRERFJg3LkcEzPmWNdHJI+KakiaUP/SCjTCWw6JUVExBq3b4fRps1MDhwwv+ldvHggy5Y9Q0CA133WFBF5MDly5MDV1TVWq5SLFy/Gar0Szdvbm4kTJ3Lnzh2OHz/OyZMnKVKkCH5+fuT4/6cDgwcP5tlnn+X555+nYsWKdOjQgREjRjBy5EiioqLi3K6npyf+/v5OLxEREZGMqnp1qFfPnN61y9JQJB3SE2yxnou7kikiImKpsLBIOnacy9atpwHIkycLK1Y8S+7cWSyOTEQyMg8PD6pXr87KlSudyleuXEm96Lv8eLi7u1OgQAFcXV2ZNWsWbdq0wcXFvKa+c+eOfTqaq6srhmFgGEbyHoSIiIhIOmSzQbdujvk7d8z3y5dhzx6IiHAsi4qC/fth6lTYuzd145S0SX1ZiIiISKYWFWXQq9dvLFt2GICAAE+WLXuGYsUCLY5MRDKD/v378+yzz1KjRg3q1q3L+PHjOXnyJC+99BJgdst15swZpkyZAsDBgwfZtm0btWvX5tq1a3z55Zfs3buXn3/+2b7Ntm3b8uWXX1K1alVq167N4cOHGTx4MO3atbN3ESYiIiKS2RUt6ph+7z24ft1MngAMGAD58sGGDbBxo5lsAfDwgP79zXWrV0/1kCWNUFJFREREMrUpU3YzY8Y/AHh5ubFoURcqV85jcVQikll06tSJK1euMGzYMM6dO0eFChVYsmQJhQsXBuDcuXOcPOkYizAyMpIvvviC//77D3d3d5o0acLmzZspUqSIvc6gQYOw2WwMGjSIM2fOkDNnTtq2bcsnn3yS2ocnIiIikmblzAlPPAHz5sHmzc7Lvvgi7nXCwmDUKHP6ww8he3ZHN2KSediMTNb+Ozg4mICAAG7cuKF+gq1kGDA2B4RcNbv/6hdmdUQiIpJJRUZG8corS/jpp7+ZN68T7dqVtjokkWSna2BJKp0zIiIikhlERsLIkXDjBjzyCBw8CLNnm8vy5YOGDaFBAyhRAjp0cHQTFs3FBZYtg2zZUj92SV5Juf5VSxWxxqKOZkJFRETEYq6uLowb15oXX6xOtWp5rQ5HREREREREUomrKwwa5Ji/c8fs1qtIEbOLL5vNsWz5cggPhy5d4MIFsywqCo4fV1Ils9Ho4JL6wu/AoXmOef9C1sUiIiKZUmhohNO8zWZTQkVERERERCST8/GBpk2hWDHnhAqAtzf4+8Ovv8LKlY7yF180W7hI5qGkiqQ+I8p5vvVMa+IQEZFMae3aY5Qq9T927jxndSgiIiIiIiKSznh5QWAgtGrlKPv2W+vikdSnpIqkvuPLHNOFHoU8Na2LRUREMpW//z5H+/azOHnyBo0b/8zevRetDklERERERETSoe7dHdNbt8L589bFIqlLSRVJXZf2wKKnHPP3tqMTERFJIYcOXaFFi2ncvBkGQIMGhShdOrvFUYmIiIiIiEh6VKIEzJnjmD950rpYJHUpqSKp58JfMKWyc1nRVnHXFRERSUZnz94kKGgaly7dAaB+/YLMmfMU7u6uFkcmIiIiIiIi6VWxYmZyRTIXJVUk9Rz81Xm+5rtQ/U1LQhERkczj2rW7NG8+jePHrwNQoUIuFi3qgo+Pu7WBiYiIiIiISLpnGOb72LHWxiGpR0kVST1GpGO6UFOo+6F1sYiISKZw5044bdvOtI+dUrhwAMuXdyMw0NviyERERERERCQj8PAw3/fuhVWrrI1FUoeSKmKNOkPAXQ+0REQk5YSHR9Kp0y9s2nQKgJw5fVi58lny5fOzODIRERERERHJKH74wTH93nvWxSGpR0kVERERyZCWLTvM4sUHAfDz82DZsm6ULKmB6UVERERERCT5+PjAgAGO+YMHrYtFUoeSKiIiIpIhtW1bmu+/b423txsLFnSmWrW8VockIiIiIiIiGVCnTo7pI0esi0NSh5vVAYiIiIiklD59atC+fRny5MlidSgiIiIiIiKSQbnEaLpw6ZJ1cUjqUEsVERERyTAuXrwdq0wJFREREREREUlpxYqZ71OmQGSktbFIylJSRURERDKEhQv/o0iRr5kzZ5/VoYiIiIiIiEgmExBgvl+/Ds2awblzloYjKUhJFUkdt87B9tFWRyEiIhnU+vUn6NTpF+7ejaBz51/YuPGk1SGJiIiIiIhIJtK9u2P61i04dMi6WCRlKakiqeOP/s7zLhrOR0REksfu3edp124mISERAHTtWpF69QpaHJWIiIiIiIhkJnXrwgcfOOYPH7YuFklZSqpI6gg+7pjOXR3y1LQsFBERyTiOHr1GixbTuXEjFICWLUswaVJ7XFxsFkcmIiIiIiIimYmbG3ToAD4+5vzYsdbGIylHSRVJfc9sA1cPq6MQEZF07sKFWwQFTeX8+VsA1KlTgLlzn8Ld3dXiyERERERERCSzKlPG6ggkpSmpIqnPptNOREQezo0bIbRoMZ0jR64BUK5cTn7/vSu+vkrai4iIiIiIiHVeesnqCCSl6em2iIiIpCshIRG0bz+LXbvOA1CoUADLl3cjWzZviyMTERERERGRzC5/fsf02bPWxSEpR0kVSXnXDsG5rVZHISIiGcSuXef5888zAOTI4cOKFd0oUMDf4qhEREREREREIGtWx/Qff1gVhaQkJVUkZRkGzGtpdRQiIpKB1KlTgOXLu5E/vx9LlnSldOkcVockIiIiIiIiAoCnJ/j6mtPz51sbi6QMJVUkZUVFwPUjjvnCj1kXi4iIZBgNGxbmyJHXqVkz//0ri4iIiIiIiKSi6tXN92PHrI1DUoaSKpKyQq85zz+xxJo4REQkXYsePyUmT083CyIRERERERERSViFCua7n5+1cUjKUFJFUk7IdfixiGM+fwNw0QMwERFJmmnT9lC16g988MFqDMOwOhwRERERERGRBJUoYb7fvAlXr1obiyQ/JVUk5VzYARF3HfPZy1kXi4iIpEtLlhyiV6/fABgxYiOLFh20OCIRERERERGRhGXL5phu0QL2/x979x0eVZn3Yfw76aEklEDooUkTWBSUJigIoUnvVRBUyorIq66IrIoKioKICov0DlJEOkFFQVCRJiAoSAtVegJJSJnM+8doYgxgEiZ5ptyf68o1z3OYSe64LOL8cs45aK4FjsdQBTkjoIDU8B3TFQAAF/Ldd6fUqdOnSkpKliQNHlxLrVtXMFwFAAAAAMCdFS2auk5OlrZvN9cCx2OogpxRY7DkH2y6AgDgIn7++YJatVqouLgkSVKXLvdq0qQWslgshssAAAAAALizggWlmTNT90uWSFzN2n0wVAEAAE7l5MlrCg+fr6tXb0qSmjQpq7lz28nbm7+2AAAAAABcQ/XqUni4fX31qrR5s9keOA7vTgAAAKdx8WKMwsPn6+zZ65KkWrWKacWKLvL39zFcBgAAAABA5jz2WOr67bfNdcCxGKoAAACncP16vFq2XKjDhy9LkipWLKh163oob15/w2UAAAAAAGRevXrS/ffb11euSHPmmO2BYzBUAQAATuH8+Rs6d85+hkrx4nm1cWMvFSqU23AVAAAAAABZN3Bg6vrDD6XoaHMtcAyGKgAAwCncc09Bbdv2hOrWLaGNG3spLCyf6SQAAAAAAO5KtWpS48ap+0mTzLXAMRiqAAAApxEWlk/btj2he+8tbDoFAAAAAIC75usrvfNO6n7lSmMpcBCGKgAAwJiVK39RYqI1zTGLxWKoBgAAAAAAx7NYpBEjUvfDh0snT5rrwd1hqAIAAIz46KMdat9+iTp0+FSxsYmmcwAAAAAAyDbt2qWut2yRnnvOWAruEkMVAACQ4xYt2q+hQ9dLktasOaxVq341XAQAAAAAQPbx9pbefDN1HxkprV5trgdZx1AFAADkqI0bf1OfPitls9n3L7/8kLp1q2o2CgAAAACAbNa8uf0slT+9/rq0ebO5HmQNQxUAAJBjfvjhtDp2/FRJScmSpCefvF9vvtnYcBUAAAAAADkjVy7pxRdT9z/8YK4FWcNQBQAA5IhDhy6qVauFiomx3z+lQ4fKmjKlFTemBwAAAAB4lM6dpQIF7Ov4eLMtyDyGKgAAINudOhWl8PD5unw5TpL0yCOltWBBB3l781cRAAAAAIBnsVikhg3t66Agsy3IPN7JAAAA2ery5ViFh8/X6dPRkqT77iuizz/vpoAAH8NlAAAAAACYUbiw/TEuzmwHMo+hCgAAyFYWi0X58wdIksqXL6D163sqKMjfcBUAAAAAAObky2d/vHbNZAWygqEKsofNJkV+ZboCAOAEChQI1KZNvdW3bw1FRPRSaGge00kAAAAAABjFUMV1cd0NZI/fPpd2jDVdAQBwErlz+2nWrLamMwAAAAAAcAp/DlV275ZOnZJKljSag0zgTBU43m+rpFXt0x4rWsdMCwAgx9lsNn3wwfe6fDnWdAoAAAAAAE6pSJHU9aZN5jqQeQxV4Hh7P067bzpVKtvKTAsAIMeNGbNVw4ZtVIMGs3TqVJTpHAAAAAAAnE6pUlLevPZ1crLZFmQOQxU4nvVm6vqBF6VqT5prAQDkqKlTd+qVVzZLkg4duqRvv400XAQAAAAAgHNq2tT+uH692Q5kDkMVZK96oyWLxXQFACAHLFt2UIMGrU3ZjxvXRN27VzNYBAAAAACA87p0yf548qTZDmQOQxUAAHDXvvrquHr2XCGbzb5/4YV6euGF+majAAAAAABwYq1bp64TEsx1IHMYqgAAgLuya9dZtW27WAkJVklS37419M47TQxXAQAAAADg3GrVSl3HxprrQOYwVAEAAFl2+PBltWixQDdu2H+k5rHHKmjatNaycOlHAAAAAADuKE8e0wXICoYqcKwjK6TTW0xXAABywIULMQoPn6eLF+0/TvPQQ6X06aed5OPDXy8AAAAAAMiMLbyl6jJ41wOOtenp1LWXj2ThtxgAuKsCBQLVuHEZSVK1aoW1enV3BQb6Gq4CAAAAAMA1/PUiDwcOmOtA5viYDoCbibuUuq71guTNm2sA4K58fLw0Y0YbVaxYUH36/Ev58gWYTgIAAAAAwKWEhUknT0rR0aZLkFEMVZA9ijwoNRhjugIAkM0sFov+85+HTGcAAAAAAOCSqlWzD1W++MJ0CTKKazMhe3CDYgBwO8nJNj3/fIT27//ddAoAAAAAAG4hODh1HRMj2WzmWpAxDFUAAMA/stlseuGFCI0f/50aNpytbdsiTScBAAAAAODyGjVKXT/8sNSkiXThgrke/DOGKgAA4B+NG7dNEyZ8L0mKjo7X77/HGC4CAAAAAMD1/etfafdRUVLLltKKFWZ68M8YqgAAgDuaMWO3Xnrpy5T9//7XSh06VDZYBAAAAACAe7BYpJEjpXr10h4fM0a6fNlME+6MoQoAALitlSt/0VNPrUnZjxnTWE8+WdNgEQAAAAAA7qV9e2nSJOmTT6T770893rWrdP68uS7cGkMVAABwS1u2nFS3bsuUnGy/S96wYbX10ksPGa4CAAAAAMA93X+/NHVq6v7aNWn0aGM5uA2GKgAAIJ2ffjqv1q0XKT7eKknq2bOaxo9vJovFYrgMAAAAAAD3ZbFIb72Vut+xQ7pyxVwP0mOoAgAA0rh27aaaN1+g6Oh4SVKLFuU1a1ZbeXkxUAEAAAAAILs1ayatXp26f+89cy1Ij6EKAABII1++AL38sv0yX3XrltDSpZ3l6+ttuAoAAAAAAM9RtGjqOiJCGjxYstnM9SAVQxUAAJDOM8/U1sqVXbVmTQ/lzu1nOgcAAAAAAI/z8sup6x07pLg4cy1IxVAFAADcUtu2lVSgQKDpDAAAAAAAPFLLllLv3qn7yEhzLUjFUAUAAA+XlJSs7t2Xa+XKX0ynAAAAAACAPwQESIMGpe579ZJiY831wI6hCgAAHsxms2ngwDVavPiAOnb8VLNn7zWdBAAAAAAA/uDjIxUpkro/etRcC+wYqgAA4MFefvlLzZixR5Lk7W1RiRJBhosAAAAAAMCfvLykxYtT9/36SYcPm+sBQxUAADzWhAnf6e23t0mSLBZpwYIOatKkrOEqAAAAAADwV3nySGXKpO5ffNFcCxiqAADgkebN+0n/938RKfuPP26pzp3vNVgEAAAAAABuZ+jQ1PXp09KoUVJExO2fj+zDUAUAAA+zdu1h9ev3ecr+9dcf0aBBD5gLAgAAAAAAd9SggbRqVep+/Xrp5ZfN9XgyhioAAHiQbdsi1bnzUlmtNknSkCEPaNSohoarAAAAAADAPylWTKpePe2xHTvMtHgyhioAAHiI2NhEdeq0VHFxSZKkrl3v1aRJLWSxWAyXAQAAAACAjJg6Vfrqq9T94MHmWjwVQxUAADxErly+WrCgg/Lk8VN4eDnNndteXl4MVAAAAAAAcBW+vlJQkDR8uOkSz8VQBQAAD9K4cRlt2/aEli/vIj8/b9M5AAAAAAAgC5o2TV0vXCglJ5tr8TQMVQAAcGNJSen/VlW9eqjy5PEzUAMAAAAAAByhYMHU9YQJ0q5d5lo8DUMVAADcVHx8kpo1m6/XX/9aNpvNdA4AAAAAAHAQi0W6777U/aBBUmysuR5PwlAFAAA3ZLUmq1evz/TVV8f12mvfaOTIr/75RQAAAAAAwCVYLNK0aVLbtqnHXn5ZOnvWXJOnYKgCAICbsdlsGjJknZYtOyjJfoP6Nm0qGq4CAAAAAACO9sor0vTpkre39O23Ups20sGDpqvcG0MVOM65H0wXAAAkvfrq15o61X4xVR8fLy1f3kV16pQwXAUAAAAAABzNYpFq1JD+7/9Sj02fbizHIzBUgWPEnJcWP2S6AgA83ocf/qA33tiSsp8zp52aNy9vsAgAAAAAAGS3Ll2kAgXs6/Pnzba4O4YqcIyrR6TkpNR9qUfNtQCAh1q0aL+GDt2Qsv/gg+bq0aOawSIAAAAAAJBTOna0Px4+LO3aZbbFnTFUgeOVaCjVf9N0BQB4lI0bf1OfPitT9iNHNtDQobXNBQEAAAAAgBxVtGjq+umnpatXzbW4M4YqcLwite0X8wMA5IjERKuGDFmnpKRkSdJTT92vN95oZLgKAAAAAADkpGrVJB+f1H3PntKPP0o2m7kmd8RQBQAAF+fr662NG3upbNn86tChsiZPbiULw20AAAAAADxKmTLS+vWp+wsXpEGDpPHjpZgYc13uhqEKAABuoFy5Avruu/5asKCDvL351zsAAAAAAJ4of35p2rS0xxYvllq0kLZvN9PkbnjXBQAAFxQVdVNWa3KaY4UL51ZAgM9tXgEAAAAAADzBffdJ06dL99+feiw2Vho+XEpIMNflLhiqAADgYm7cSFB4+Hx16bJMN28mmc4BAAAAAABOpkYN6eOPpaVLpQoV7MeSkuwfuDsMVQAAcCEJCVZ17Pipduw4oxUrDumJJz43nQQAAAAAAJyQr6/9PiuzZqUea9VKiooy1+QOGKoAAOAikpNtevzxlYqIOCpJypcvQCNGPGS4CgAAAAAAODMvL8nnj6uFX78uffihdPGidOWK2S5XxVAFAAAXYLPZ9Oyz67V48QFJUkCAj1av7q5q1UINlwEAAAAAAGfm6yu99VbqfuVK+43rO3fmHitZwVAFAAAX8OabW/TRRz9Kkry9LVq6tLMeeqiU4SoAAAAAAOAKHn1UGjw47bGoKGnXLjM9royhCgAATu5//9up//7365T9zJlt9dhjFcwFAQAAAAAAl9O9uxQebn/808mT5npclY/pALiJhOumCwDALS1bdlCDB69N2b/3XlP16fMvg0UAAAAAAMAVBQZKY8bY10eOSDt32u+3gszhHxnu3oWfpM9ama4AALeTnGzTxInfy2az7198sZ7+7//qmY0CAAAAAAAuLzjYdIHrYqiCu3d8bdp9cGkjGQDgbry8LFq/vqeaNCmrJ56oobffbmI6CQAAAAAAuIE/f4Bz3LjUNTKGy3/h7sRekLa/mrov0VC6t6+xHABwN3nz+mvNmu7y9vaSxWIxnQMAAAAAANxAQEDq+vffpSJFzLW4Gs5Uwd3ZNkpKTkrdP/iS5JvLXA8AuLizZ6/r2rWbaY75+/vIx4d/ZQMAAAAAAMfo1y91feaMuQ5XxDs0uDtRx1PXPrmkYvXNtQCAi7tyJU7h4fPUsOEsnTt33XQOAAAAAABwU2XKpK65MEbmMFSB4zx5QvIPMl0BAC4pNjZRrVsv0s8/X9T+/RfUu/dnppMAAAAAAIAbK1bM/vjUU9LVq2ZbXAlDFTiOT8A/PwcAkE5iolWdOy/V9u2nJEmFC+fW//73mOEqAAAAAADgznLnTl3Pn2+uw9UwVMHdiTpmugAAXFpysk1PPLFK69YdkSQFBflrw4aeKl++gOEyAAAAAADgzgYNSl3PmSMlJ5trcSUMVZB1G56Qrh01XQEALstms+n55yM0f/4+SZK/v7c+/7yb7ruvqOEyAAAAAADg7ho2lLp1S91/+625FlfCUAVZ9+vi1HWuwpJPoLkWAHBB48Zt0/vvfy9J8vKyaNGijnrkkdJmowAAAAAAgMfo2zd1vXjxbZ+Gv2Cogqyz/eV8sDafSV4+5loAwMXMmLFbL730Zcp+6tTH1L59ZYNFAAAAAADA04SE2M9YkaTTpyWbzWyPK2CogrtXqIZUvJ7pCgBwGTabTVu3Rqbsx4xprAED7jdYBAAAAAAAPNWbb9ofz56VZs402+IKjA9VJk+erDJlyiggIEA1a9bU1q1b7/j8+Ph4jRw5UmFhYfL391e5cuU0k/+lAQAuxGKxaObMtnr22dp67rk6eumlh0wnAQAAAAAAD5Url3Tfffb155+bbXEFRq/XtGTJEg0bNkyTJ09W/fr1NXXqVLVo0UIHDx5UqVKlbvmaLl266Pfff9eMGTNUvnx5XbhwQUlJSTlcDgDA3fHysuj995tJsg9ZAAAAAAAATGnXTtqzx362SlKS5MOdHm7L6D+aCRMmqH///howYIAkaeLEidq4caOmTJmisWPHpnv+hg0b9M033+jYsWMqUKCAJKl06dI5mYw/2WySNd50BQC4jKNHryghwarKlQulHGOYAgAAAAAAnEGdOqnrM2eksDBzLc7O2OW/EhIStGvXLoWHh6c5Hh4eru3bt9/yNatWrVKtWrU0btw4FS9eXBUqVNDzzz+vuLi4236d+Ph4RUdHp/nAXUq2Sksamq4AAJdx7tx1hYfPV4MGs7RjxxnTOQAAAAAAAGkULChVqmRfnzhhNMXpGRuqXLp0SVarVaGhoWmOh4aG6vz587d8zbFjx/Ttt9/qwIED+uyzzzRx4kQtW7ZMQ4YMue3XGTt2rIKDg1M+SpYs6dDvw+MkW6WtL0lnvk09lruIuR4AcHLXrt1UixYLdOzYVV2+HKehQ9fLZrOZzgIAAAAAAEjjz4tCHT9uNMPpGb9R/d8vfWKz2W57OZTk5GRZLBYtWLBADz74oFq2bKkJEyZo9uzZtz1bZcSIEYqKikr5OHXqlMO/B49ybK208720xx5+10wLADi5uLhEtW27WD/99LskKSwsWMuXd+GyXwAAAAAAwOmUKWN/ZKhyZ8buqRISEiJvb+90Z6VcuHAh3dkrfypatKiKFy+u4ODglGOVK1eWzWbT6dOndc8996R7jb+/v/z9/R0b78mu/pp232GdFFLVTAsAOLGkpGR167ZcW7aclCSFhORSRERvFS8eZLgMAAAAAAAgPYYqGWPsTBU/Pz/VrFlTmzZtSnN806ZNqlev3i1fU79+fZ09e1Y3btxIOXb48GF5eXmpRIkS2dqLW3h4vFSmhekKAHA6NptNTz21WqtW2QfRefL4af36nqpQoaDhMgAAAAAAgFv78/JfJ05IXLn89oxe/mv48OGaPn26Zs6cqUOHDum5555TZGSkBg4cKMl+6a4+ffqkPL9Hjx4qWLCg+vXrp4MHD2rLli164YUX9MQTTygwMNDUt+G5gkubLgAApzRixJeaNWuvJMnPz1srV3ZVrVrFzEYBAAAAAADcQcmSksUixcZKb7xhf0R6xi7/JUldu3bV5cuXNXr0aJ07d05Vq1bVunXrFBYWJkk6d+6cIiMjU56fJ08ebdq0Sc8884xq1aqlggULqkuXLnrzzTdNfQsAAKQxfvx2vfPONkn2v4gsWNBBjz5a1nAVAAAAAADAnfn62gcrkZHSqlVS2bJSr16mq5yP0aGKJA0ePFiDBw++5a/Nnj073bFKlSqlu2QYAADOaPLkVurUqYrpDAAAAAAAgAzp2lV69137euJEhiq3YnyoAgCAO/m//6unkJBcOn06WgMH1jKdAwAAAAAAkGFdu0rR0dLUqfZ9UpLkwxQhDf5xAADgYI8/XsN0AgAAAAAAQJb07y/NnSvFxUlnzkh/3K0Df8jSjeqTkpL0xRdfaOrUqbp+/bok6ezZs7px44ZD4wAAcHb79/+udeuOmM4AAAAAAABwCC+v1EHKiRNGU5xSpocqJ0+eVLVq1dS2bVsNGTJEFy9elCSNGzdOzz//vMMDAQBwVidOXFOzZvPVps0izZv3k+kcAAAAAAAAhyhd2v54/LjRDKeU6aHKs88+q1q1aunq1asKDAxMOd6+fXt9+eWXDo0DAMBZXbgQo/DweTp37oasVps+/vhHWa3JprMAAAAAAADu2p9DFc5USS/TQ5Vvv/1Wr7zyivz8/NIcDwsL05kzZxwWBgCAs4qOjleLFgt05MgVSVKlSiFas6aHvL2zdFVNAAAAAAAAp1KmjP1xzRopNtZsi7PJ9Ls/ycnJslqt6Y6fPn1aefPmdUgUAADO6ubNJLVrt1i7d5+TJJUoEaSNG3spJCSX4TIAAAAAAADHqFgxdf3YY1JiorkWZ5PpoUrTpk01ceLElL3FYtGNGzf06quvqmXLlo5sAwDAqVityerVa4U2bz4hSSpQIFAREb1UqlSw2TAAAAAAAAAHKlFC+vPuH9HR0pEjZnucSaaHKu+//76++eYbValSRTdv3lSPHj1UunRpnTlzRu+88052NAIAYJzNZtOQIeu0fPkhSVKuXL5at66HKlcuZLgMAAAAAADA8T7+OHWdzG1kU/hk9gXFihXT3r17tXjxYu3atUvJycnq37+/evbsmebG9QAAuJPXX/9GU6fukiT5+HhpxYouql27hOEqAAAAAACA7FG9ulSsmHT2rOkS55LpocqWLVtUr1499evXT/369Us5npSUpC1btqhhw4YODQQAwBnUrVtCuXL5Ki4uUXPntlOzZuVNJwEAAAAAAGSrmzftj0lJZjucSaYv/9WoUSNduXIl3fGoqCg1atTIIVEAADibZs3K66uv+uh//3tM3btXM50DAAAAAACQ7f4cBezfb7bDmWR6qGKz2WSxWNIdv3z5snLnzu2QKAAAnFHt2iX01FM1TWcAAAAAAADkqA8+MF3gPDJ8+a8OHTpIkiwWi/r27St/f/+UX7Nardq3b5/q1avn+EIAAAz4/vvT2rr1pJ5/vt4tf5gAAAAAAADA3d1/v7R7t3197pxUtKjZHmeQ4aFKcHCwJPuZKnnz5k1zU3o/Pz/VqVNHTz75pOMLAQDIYQcPXlSrVgt15Uqczp27offeC5eXF4MVAAAAAADgWUaMkDp3tq+XLpWGDjXb4wwyPFSZNWuWJKl06dJ6/vnnudSXp0qKM10AANkqMjJKzZrN15Ur9j/vfvrpdyUlJcvPz9twGQAAAAAAQM4qUyZ1PXcuQxUpC/dUefXVVxmoeKojK6Xtr5quAIBsc+lSrMLD5+n06WhJUs2aRbVyZVcGKgAAAAAAwGN165a6jooy1+EsMnymyl8tW7ZMn376qSIjI5WQkJDm13b/eYE1uJdzO6RV7dMey80F9AC4jxs3EtSy5QL9+utlSdI99xTQunU9lTev/z+8EgAAAAAAwH316SMtXmxf/20c4JEyfabKpEmT1K9fPxUuXFh79uzRgw8+qIIFC+rYsWNq0aJFdjTCGWzom3Zf+2WpaB0jKQDgaPHxSerQYYl+/PGsJKlYsbyKiOitwoU5MxMAAAAAAHi2woVT14sWmetwFpkeqkyePFmffPKJPvroI/n5+enFF1/Upk2bNHToUEVx7o/7unE6dX1PB+mhtyQLN20G4Pqs1mQ9/vhKbdp0TJKUL1+ANm7spdKl85kNAwAAAAAAcDK//mq6wLxMD1UiIyNVr149SVJgYKCuX78uSerdu7cWMaZyf/75pDbLTVcAgMOMGrVZS5b8LEkKDPTRmjXdVbVq4X94FQAAAAAAgOfo18/+yM/ZZ2GoUqRIEV2+bL/efFhYmL7//ntJ0vHjx2Wz2RxbB+eTp5jpAgBwqL59aygsLFje3hYtXdpZ9euXMp0EAAAAAADgVP68BNgf4wCPlumhSuPGjbV69WpJUv/+/fXcc8+padOm6tq1q9q3b/8PrwYAwLlUqFBQ27f317JlXdSqVQXTOQAAAAAAAE7nr+dTXLhgrsMZ+GT2BZ988omSk5MlSQMHDlSBAgX07bffqnXr1ho4cKDDAwEAyG7FiuVVu3aVTGcAAAAAAAA4pZo1U9fLlkmDB5trMS3TQxUvLy95eaWe4NKlSxd16dJFknTmzBkVL17ccXUAADjYF18c07x5+zRtWmv5+XmbzgEAAAAAAHB65cqlrn/+2VyHM8j05b9u5fz583rmmWdUvnx5R3w6AACyxY8/nlG7dos1d+5Pat16kWJiEkwnAQAAAAAAuIR27eyPwcFGM4zL8FDl2rVr6tmzpwoVKqRixYpp0qRJSk5O1n//+1+VLVtW33//vWbOnJmdrTDl8i9SwnXTFQBwV3799ZJatlyomJhESVJgoI/8/TN9wiYAAAAAAIBHKl3a/hgRYTTDuAy/m/Tyyy9ry5Ytevzxx7VhwwY999xz2rBhg27evKn169fr4Ycfzs5OmBIfJS14IHXvE2iuBQCy6MyZaIWHz9elS7GSpIYNw7RoUUf5+DjkhE0AAAAAAAC3FxRkusA5ZHiosnbtWs2aNUtNmjTR4MGDVb58eVWoUEETJ07MxjwYF3VcSryRun9whLkWAMiCK1fiFB4+X5GRUZKkf/0rVKtWdVNgoK/hMgAAAAAAANfx0EOmC5xDhn9E9+zZs6pSpYokqWzZsgoICNCAAQOyLQxOqEpvqUJH0xUAkGExMQl67LGFOnjwoiSpbNn82rChl4KDAwyXAQAAAAAAwBVleKiSnJwsX9/Un+r19vZW7ty5syUKTsqX/70BuI7ERKs6d16q7747LUkKDc2tiIheKlIkj+EyAAAAAAAA17ZqlekCczJ8+S+bzaa+ffvK399fknTz5k0NHDgw3WBlxYoVji2EWTar6QIAyJI339yi9et/kyQFBflrw4ZeKleugOEqAAAAAAAA1/TXUcDo0VLr1pLFYq7HlAyfqfL444+rcOHCCg4OVnBwsHr16qVixYql7P/8gBuJPiXNr2W6AgCyZPjwumrYMEz+/t5ataqbatQoYjoJAIBbmjx5ssqUKaOAgADVrFlTW7duvePzP/74Y1WuXFmBgYGqWLGi5s6dm+45165d05AhQ1S0aFEFBASocuXKWrduXXZ9CwAAAPAA/v5S9+6p+9hYcy0mZfhMlVmzZmVnB5zR0b+dw5W7qJkOAMiC4OAAbdzYS7t3n1O9eiVN5wAAcEtLlizRsGHDNHnyZNWvX19Tp05VixYtdPDgQZUqVSrd86dMmaIRI0Zo2rRpeuCBB7Rjxw49+eSTyp8/v1q3bi1JSkhIUNOmTVW4cGEtW7ZMJUqU0KlTp5Q3b96c/vYAAADgZgYPlhYtsq9PnJDuvddojhEWm81mMx2Rk6KjoxUcHKyoqCgFBQWZznFuuz+QNg+zr3OFSo8fkHKFGE0CgDuxWpPl7Z3hkzABwGPwd2DnVbt2bd1///2aMmVKyrHKlSurXbt2Gjt2bLrn16tXT/Xr19e7776bcmzYsGHauXOnvv32W0nS//73P7377rv65Zdf0twXMzP4PQMAAIDbqfXHxY3q1ZMmTTLb4iiZ+fsv7zwhYxpNZKACwKmtWHFIderM0O+/3zCdAgBAhiQkJGjXrl0KDw9Pczw8PFzbt2+/5Wvi4+MVEBCQ5lhgYKB27NihxMRESdKqVatUt25dDRkyRKGhoapatarGjBkjq/X290uMj49XdHR0mg8AAADgTjz1RGiGKgAAl7d583F1775cO3eeVf36M3XxYozpJACAm4uJidGoUaNUr149lS9fXmXLlk3zkRGXLl2S1WpVaGhomuOhoaE6f/78LV/TrFkzTZ8+Xbt27ZLNZtPOnTs1c+ZMJSYm6tKlS5KkY8eOadmyZbJarVq3bp1eeeUVjR8/Xm+99dZtW8aOHZvmXpklS3LpTAAAANza8OH2x40bpZ9/NttiQobvqQIAgDPas+ec2rZdrIQE+0/f1q9fSgUL5jJcBQBwdwMGDNA333yj3r17q2jRorJYLFn+XH9/rc1mu+3nGzVqlM6fP686derIZrMpNDRUffv21bhx4+Tt7S1JSk5OVuHChfXJJ5/I29tbNWvW1NmzZ/Xuu+/qv//97y0/74gRIzT8z/86lv3yBwxWAAAAcCuBganrH3/0vPuqMFQBALis3367oubNF+j69QRJUqtW92j69Nby8sr6G1sAAGTE+vXrtXbtWtWvXz/LnyMkJETe3t7pzkq5cOFCurNX/hQYGKiZM2dq6tSp+v3331W0aFF98sknyps3r0JC7JfrLVq0qHx9fVOGLJL9Pi3nz59XQkKC/Pz80n1ef39/+fv7Z/l7AQAAgOdo1kz68yToffvMtpiQpct/zZs3T/Xr11exYsV08uRJSdLEiRP1+eefOzQOAIDbOXfuusLD5+nCBfulvurVK6lPP+0sX1/vf3glAAB3L3/+/CpQoMBdfQ4/Pz/VrFlTmzZtSnN806ZNqlev3h1f6+vrqxIlSsjb21uLFy/WY489Ji8v+3/e1a9fX7/99puSk5NTnn/48GEVLVr0lgMVAAAAIDNy5ZJat7avt2yREhLM9uS0TA9VpkyZouHDh6tly5a6du1ays0O8+XLp4kTJzq6DyZdOmC6AABu6dq1m2refIGOH78mSapatbDWrOmuXLl8zYYBADzGG2+8of/+97+KjY29q88zfPhwTZ8+XTNnztShQ4f03HPPKTIyUgMHDpRkvyxXnz59Up5/+PBhzZ8/X0eOHNGOHTvUrVs3HThwQGPGjEl5zqBBg3T58mU9++yzOnz4sNauXasxY8ZoyJAhd9UKAAAA/KlOndT1kSPmOkzI9OW/PvzwQ02bNk3t2rXT22+/nXK8Vq1aev755x0aB4N+niPtn/6XA1xKB4BziItLVJs2i7Rv3++SpLCwYG3Y0FP58wf+wysBAHCc8ePH6+jRowoNDVXp0qXl65t2sL979+4MfZ6uXbvq8uXLGj16tM6dO6eqVatq3bp1CgsLkySdO3dOkZGRKc+3Wq0aP368fv31V/n6+qpRo0bavn27SpcunfKckiVLKiIiQs8995yqV6+u4sWL69lnn9V//vOfu//GAQAAAEnh4dLIkfb15ctmW3Japocqx48f13333ZfuuL+/v2JiYhwSBSdwdHXafbE6t34eAOSwKVN2autW+5tLISG5FBHRW8WLBxmuAgB4mnbt2jnscw0ePFiDBw++5a/Nnj07zb5y5cras2fPP37OunXr6vvvv3dEHgAAAJCOxSJVriwdOmRfe5JMD1XKlCmjvXv3pvzk1J/Wr1+vKlWqOCwMptlSl712SUFht38qAOSgZ5+trd9+u6J58/Zpw4aeqlChoOkkAIAHevXVV00nAAAAAEb9cUs/3bxptiOnZXqo8sILL2jIkCG6efOmbDabduzYoUWLFmns2LGaPn36P38CuJ7cRUwXAEAKb28vffxxS/3f/9VVuXJ3d4NgAADu1q5du3To0CFZLBZVqVLllmf1AwAAAO4oLs7+OGKE1LSp2ZaclOmhSr9+/ZSUlKQXX3xRsbGx6tGjh4oXL64PPvhA3bp1y45GAICHi46OV1CQf8reYrEwUAEAGHXhwgV169ZNX3/9tfLlyyebzaaoqCg1atRIixcvVqFChUwnAgAAAMgGXll50ZNPPqmTJ0/qwoULOn/+vE6dOqX+/fs7ug0AAM2Zs1cVKnyo3bvPmU4BACDFM888o+joaP3888+6cuWKrl69qgMHDig6OlpDhw41nQcAAABku7feSl3bbLd/nrvJ9FDl9ddf19GjRyVJISEhKly4sMOjAACQpNWrf1X//qv0++8xeuSR2Tp+/KrpJAAAJEkbNmzQlClTVLly5ZRjVapU0ccff6z169cbLAMAAAByhqeenJ3pocry5ctVoUIF1alTRx999JEuXryYHV0AAA/37beR6tJlmaxW+4869O1bQ6VL5zMbBQDAH5KTk+Xr65vuuK+vr5KTkw0UAQAAAOYkJJguyDmZHqrs27dP+/btU+PGjTVhwgQVL15cLVu21MKFCxUbG5sdjQAAD7Nv3+967LGFunkzSZLUvXtVTZzYXBaLxXAZAAB2jRs31rPPPquzZ8+mHDtz5oyee+45PfroowbLAAAAgJwRFJS6vnzZXEdOy9I9Ve69916NGTNGx44d0+bNm1WmTBkNGzZMRYoUcXQfAMDDHD9+Vc2bz1dUVLwkqVmzcpo9u528vBioAACcx0cffaTr16+rdOnSKleunMqXL68yZcro+vXr+vDDD03nAQAAANnO6y/ThUuXzHXkNJ+7/QS5c+dWYGCg/Pz8dP36dUc0AQA81O+/31B4+HydO3dDklS7dnEtX95Ffn7ehssAAEirZMmS2r17tzZt2qRffvlFNptNVapUUZMmTUynAQAAADnu00+l6tVNV+SMLA1Vjh8/roULF2rBggU6fPiwGjZsqNdee02dO3d2dB8AwENER8erRYsF+u23K5KkypVDtHZtD+XO7We4DACA22vatKmaNm1qOgMAAAAwIlcuKTZW8rnr0zdcR6a/1bp162rHjh2qVq2a+vXrpx49eqh48eLZ0QYA8CBr1hzWnj3nJUklSwZp48ZeKlgwl+EqAABSTZo0SU899ZQCAgI0adKkOz536NChOVQFAAAAmDNwoDRhgmfdqD7TQ5VGjRpp+vTpuvfee7OjBwDgoXr0qKaYmAS98spmRUT0VsmSwaaTAABI4/3331fPnj0VEBCg999//7bPs1gsDFUAAADgEfz97Y+JiWY7clKmhypjxozJjg4AAPTkkzXVtWtVBQX5m04BACCd48eP33INAAAAeCpfX/tjfLzZjpyUoaHK8OHD9cYbbyh37twaPnz4HZ87YcIEh4QBANzf0aNXVK5cgTTHGKgAAFyR1WrV/v37FRYWpvz585vOAQAAAHKEl5fpgpyXoaHKnj17lPjH+Tt79uzJ1iAAgGf44IPv9cILmzRvXnt17VrVdA4AAJkybNgwVatWTf3795fValXDhg313XffKVeuXFqzZo0eeeQR04kAAABAjvn+e9MFOSdDQ5XNmzffcg0AQFYsWLBPw4ZtlCR1775cVaoUUrVqoYarAADIuGXLlqlXr16SpNWrV+vEiRP65ZdfNHfuXI0cOVLbtm0zXAgAAABkv6tXTRfkvEyfnPPEE0/o+vXr6Y7HxMToiSeecEgUAMB9bdjwm/r2/TxlP2pUQwYqAACXc+nSJRUpUkSStG7dOnXu3FkVKlRQ//79tX//fsN1AAAAQM4oX97+GOpBb+1keqgyZ84cxcXFpTseFxenuXPnOiQKAOCevv/+tDp2/FRJScmSpIEDa+q11x4xGwUAQBaEhobq4MGDslqt2rBhg5o0aSJJio2Nlbe3t+E6AAAAIGf4+dkfz50z25GTMnT5L0mKjo6WzWaTzWbT9evXFRAQkPJrVqtV69atU+HChbMlEjls/wzpyArTFQDczMGDF9Wq1ULFxtrv0dWpUxV99FFLWSwWw2UAAGRev3791KVLFxUtWlQWi0VNmzaVJP3www+qVKmS4ToAAAAg59lskie8zZPhoUq+fPlksVhksVhUoUKFdL9usVj0+uuvOzQOhnw3OnXt7Sf55TXXAsAtREZGKTx8nq5csZ/p+OijZTR/fnt5e2f6hEkAAJzCa6+9pqpVq+rUqVPq3Lmz/P39JUne3t566aWXDNcBAAAAOSMsLHV9+LBUsaK5lpyS4aHK5s2bZbPZ1LhxYy1fvlwFChRI+TU/Pz+FhYWpWLFi2RKJHJYUm7puNouhCoC7culSrMLD5+nMGfv9uGrWLKrPPusqf/8M/ysIAACn1KlTp3THHn/8cQMlAAAAgBkhIanrTz+VRo0y15JTMvyO1sMPPyxJOn78uEqVKsXlWjxBvnJS5R6mKwC4uP37f9fJk1GSpAoVCmr9+p7Km9ffcBUAAJk3adIkPfXUUwoICNCkSZPu+NyhQ4fmUBUAAADgHIKDTRfkjAwNVfbt26eqVavKy8tLUVFR2r9//22fW716dYfFAQBcX6NGZbRpU28NHLhGa9b0UKFCuU0nAQCQJe+//7569uypgIAAvf/++7d9nsViYagCAAAAj9G7tzRvnumKnJOhoUqNGjV0/vx5FS5cWDVq1JDFYpHNZkv3PIvFIqvV6vBIAIBre+ihUvrpp4HcQwUA4NKOHz9+yzUAAAAAz5Ghocrx48dVqFChlDUAALdjs9n01VfH9eijZdMcZ6ACAAAAAAAAV5ehd7jCwsJS7qESFhZ2xw8AgGcbPfobNWkyTyNGfHHLsxoBAHAHnTp10ttvv53u+LvvvqvOnTsbKAIAAADM+uYb0wU5I9M/NjxnzhytXbs2Zf/iiy8qX758qlevnk6ePOnQOACAa5k8+Ue99pr936Bvv71N33132nARAADZ45tvvlGrVq3SHW/evLm2bNlioAgAAAAw49df7Y8nT0qe8PO1mR6qjBkzRoGBgZKk7777Th999JHGjRunkJAQPffccw4PBAC4hk8//Vn//ve6lP377zdTvXolDRYBAJB9bty4IT8/v3THfX19FR0dbaAIAAAAMKNp09T1rl3mOnJKpocqp06dUvny5SVJK1euVKdOnfTUU09p7Nix2rp1q8MDAQDOb9Omo+rVa0XKTyOMGPGQhg2rYzYKAIBsVLVqVS1ZsiTd8cWLF6tKlSoGigAAAAAz/jpUmTbNXEdOydCN6v8qT548unz5skqVKqWIiIiUs1MCAgIUFxfn8EAAgHP78cczat9+iRITkyVJ/fvfp7feamy4CgCA7DVq1Ch17NhRR48eVePG9n/vffnll1q0aJGWLl1quA4AAADIOXnySIGBUlycZ5ypkumhStOmTTVgwADdd999Onz4cMp1hH/++WeVLl3a0X0AACf2yy+X1KLFAsXEJEqS2rWrpP/97zFZLBbDZQAAZK82bdpo5cqVGjNmjJYtW6bAwEBVr15dX3zxhR5++GHTeQAAAECOeuYZadw40xU5I9NDlY8//livvPKKTp06peXLl6tgwYKSpF27dql79+4ODwQAOKfTp6MVHj5Ply/bz1J8+OEwLVrUUT4+mb6yJAAALqlVq1a3vFk9AAAA4GkqVTJdkHMyPVTJly+fPvroo3THX3/9dYcEAQBcw40bCSn3UKlRo4g+/7ybAgIy/a8VAABc1rVr17Rs2TIdO3ZMzz//vAoUKKDdu3crNDRUxYsXN50HAAAA5JiSJVPXN27YLwnmrrL07te1a9c0Y8YMHTp0SBaLRZUrV1b//v0VHBzs6D4AgJOqVClE27c/ocGD12n69NYKDg4wnQQAQI7Zt2+fmjRpouDgYJ04cUIDBgxQgQIF9Nlnn+nkyZOaO3eu6UQAAAAgx/x1NJCQYK4jJ2T6Gi07d+5UuXLl9P777+vKlSu6dOmS3n//fZUrV067d+/OjkYAgJMqWTJYq1d3V2ioG//4AQAAtzB8+HD17dtXR44cUUBA6g8WtGjRQlu2bDFYBgAAAJh1iwtduZVMD1Wee+45tWnTRidOnNCKFSv02Wef6fjx43rsscc0bNiwbEgEADiD5GSbPvlklxITraZTAAAw7scff9TTTz+d7njx4sV1/vx5A0UAAACAORZL6jo62lxHTsjSmSr/+c9/5OOTeuUwHx8fvfjii9q5c6dD4wAAzsFms2n48I16+uk1at9+iWJjE00nAQBgVEBAgKJv8V+Lv/76qwoVKmSgCAAAADDHYpGGDrWvjx0z25LdMj1UCQoKUmRkZLrjp06dUt68eR0SBQBwLmPHfqsPPvhBkrR+/W/64YfThosAADCrbdu2Gj16tBIT7T9oYLFYFBkZqZdeekkdO3Y0XAcAAADkvJgY++MtxgduJdNDla5du6p///5asmSJTp06pdOnT2vx4sUaMGCAunfvnh2NAACDpk3bpZEjv0rZT5/eWo0alTFYBACAee+9954uXryowoULKy4uTg8//LDKly+vvHnz6q233jKdBwAAAOS4EiVS18nJ5jqym88/PyWt9957TxaLRX369FFSUpIkydfXV4MGDdLbb7/t8EDksD0fSXGXTFcAcBIrVhzSwIFrU/bvvNNE/frdZ7AIAADnEBQUpG+//VZfffWVdu/ereTkZN1///1q0qSJ6TQAAADAiGrVUtfJyZJXpk/pcA2ZHqr4+fnpgw8+0NixY3X06FHZbDaVL19euXLlyo4+5KQb56Svhqbuvf3NtQAwbvPm4+refbmSk22SpP/7v7p64YV6hqsAADAvKSlJAQEB2rt3rxo3bqzGjRubTgIAAACMK1DAdEHOyPCsKDY2VkOGDFHx4sVVuHBhDRgwQEWLFlX16tUZqLiL+KuSbKn7+58zlgLArN27z6lt28VKSLBKkvr0+ZfGjWsqi8ViuAwAAPN8fHwUFhYmq9VqOgUAAABwGr6+qevdu811ZLcMD1VeffVVzZ49W61atVK3bt20adMmDRo0KDvbYNK9faXqA0xXADDgt9+uqHnz+bp+PUGS9NhjFTR9emt5eTFQAQDgT6+88opGjBihK1eumE4BAAAAnEJgYOp69GhzHdktw5f/WrFihWbMmKFu3bpJknr16qX69evLarXK29s72wJhiMVNL3gH4B8VKBCoe+4pqIsXY1W/fkktWdJJvr78OQ8AwF9NmjRJv/32m4oVK6awsDDlzp07za/vducfzQMAAABuIyhIio6Wzp83XZJ9MjxUOXXqlBo0aJCyf/DBB+Xj46OzZ8+qZMmS2RIHAMh5BQoEatOm3nr55S/16qsPK1cu339+EQAAHqZdu3ayWCyy2Wz//GQAAADAQ7z7rvT00/a1zSa545XkMzxUsVqt8vPzS/tiHx8lJSU5PAoAYFauXL6aOLG56QwAAJxObGysXnjhBa1cuVKJiYl69NFH9eGHHyokJMR0GgAAAGBcxYqp6/79pZkzzbVklwwPVWw2m/r27St/f/+UYzdv3tTAgQPTnOq+YsUKxxYCALJVUlKyXnnlKz3/fD2FhOQynQMAgFP7816TPXv2VGBgoBYuXKhBgwZp6dKlptMAAAAA4/LkkQoXli5ckA4cMF2TPTI8VHn88cfTHevVq5dDYwAAOSs52aYBA1ZpzpyftHLlL4qI6K1SpYJNZwEA4LT+fq/Jnj17cq9JAAAA4C/efFN66in7/VXcUYaHKrNmzcrODgCAAf/5zybNmfOTJOn48Ws6fvwqQxUAAO6Ae00CAAAAd/bnMMXLy2xHdnHTbwuZZk2Utv3XdAWAHPTuu9v03nvfSZK8vCxatKijHn64tNkoAACcHPeaBAAAADLmyhX7zerdTYbPVIGbO7ZWOrI8de/la64FQLabNWuPXnzxi5T9lCmt1KFDZYNFAAC4Bu41CQAAANzZX/5arHPnpGLFzLVkB4YqkKKOS6vapz1WuaeZFgDZbtWqX/Xkk6tT9m++2UhPPVXTYBEAAK6De00CAAAAd1awYOp69mzp5ZeNpWQLhiqQdk5Iu28+RyrR4NbPBeDStmw5qa5dl8lqtZ97OXTog3r5Zf7/DgBARnGvSQAAAODO/nq13BUr3G+owj1VPN25H6S9H6XuA0Ok8u2M5QDIPr/9dkVt2izSzZv2a7736FFN77/fXBaLxXAZAAAAAAAA3EnnzqYLsk+Whirz5s1T/fr1VaxYMZ08eVKSNHHiRH3++ecOjUM2i46UFtZJe6z3Xsk/yEgOgOxVunQ+depURZLUvHl5zZrVVl5eDFQAAAAAAADgWG3apK6tVnMd2SHTQ5UpU6Zo+PDhatmypa5duybrH/9E8uXLp4kTJzq6D9klKV6a/7d7KNzTQcpb3EwPgGzn4+OladNaa/Lkllq2rLP8/LxNJwEAAAAAAMAN3XNP6joqylxHdsj0UOXDDz/UtGnTNHLkSHl7p74hV6tWLe3fv9+hcchGN05LcZdS92FNpTbLzfUAyBEWi0WDBj2g3Ln9/vnJAAAAAAAAQBb4+Ehef0wfEhPNtjhapocqx48f13333ZfuuL+/v2JiYhwShRzmm1tqt8p0BQAHu3kzSd27L9e+fb+bTgEAAAAAAICHyZfP/nj1qtEMh8v0UKVMmTLau3dvuuPr169XlSpVHNGEnFa+neQTYLoCgAMlJSWrR4/lWrz4gBo2nKVvv400nQQAAAAAAAAPcuOG/XHbNrMdjuaT2Re88MILGjJkiG7evCmbzaYdO3Zo0aJFGjt2rKZPn54djQCATLDZbBo0aI0+++wXSfYBC/dPAQAAAAAAQE5KSLA/HjpktsPRMj1U6devn5KSkvTiiy8qNjZWPXr0UPHixfXBBx+oW7du2dGI7JBsNV0AIJu88spXmj59jyTJ19dLn33WVQ8+WNxwFQAAAAAAADxJ4cLShQtSgQKmSxwr05f/kqQnn3xSJ0+e1IULF3T+/HmdOnVK/fv3d3QbstPZv5xzlYc3WwF3MXHi9xoz5ltJksUizZvXXk2bljNcBQAAAAAAAE/TvLn90d/fbIejZfpMlb8KCQlxVAdy2uGlqevy7c11AHCY+fP36bnnNqbsP/qopbp2rWqwCAAAAAAAAJ7K+4+r0VssZjscLdNDlTJlyshyh38Kx44du6sgZDNbsnR4uXR8vX2ft5RUtLbZJgB3bd26I+rX7/OU/auvPqzBgx8wWAQAAAAAAAC4n0wPVYYNG5Zmn5iYqD179mjDhg164YUXHNWF7LJ3ivTVv1P3FTq636gQ8DCnT0erU6dPlZSULEkaPLiWXn31YcNVAAAAAAAAgPvJ9FDl2WefveXxjz/+WDt37rzrIGSz339Mu6/cy0wHAIcpUSJI48Y11dCh69W5872aNKnFHc8oBAAAAAAAAJA1WbpR/a20aNFCy5cvd9SnQ05otUgKvd90BQAH+Pe/H1RERG/NndtO3t4O+6MdAAAAAAAAyJJk+0VVtHKl0QyHu6sb1f/VsmXLVKBAAUd9OuSEwveZLgCQRTabLd3ZKE2alDVUAwAAAAAAAKQVGWl/jI012+FomR6q3HfffWneyLPZbDp//rwuXryoyZMnOzQOAJDe9evxatFigYYPr6sOHSqbzgEAAAAAAADSadRI2rzZvk5IkPz8zPY4SqaHKu3atUuz9/LyUqFChfTII4+oUqVKjuoCANxCfHyS2rdfom3bTum7705rzpx26tWruuksAAAAAAAAII2HH05d37ghucuFrjI1VElKSlLp0qXVrFkzFSlSJLuaAAC3YLUmq3fvz/Tll8clScHB/rrvPv4sBgAAAAAAgPP565XrlyyRBg0y1+JImbqbsY+PjwYNGqT4+Pjs6gEA3ILNZtMzz6zX0qUHJUm5cvlq7doeuvfewobLAAAAAAAAgPRy5Updz5hhrsPRMjVUkaTatWtrz5492dECALiN11//RlOm7JQk+fh4admyzqpbt6ThKgAAAAAAAOD2WrUyXeB4mb6nyuDBg/V///d/On36tGrWrKncuXOn+fXq1bm2PwA40scf79Drr3+Tsp89u61atLjHYBEAAAAAAADwz2rUkNaulYKCTJc4ToaHKk888YQmTpyorl27SpKGDh2a8msWi0U2m00Wi0VWq9XxlQDgoZYsOaBnnlmfsp84sZl69mR4DQAAAAAAAOdXrpz9MTrabIcjZXioMmfOHL399ts6fvx4dvYAAP5w5UqcnnxytWw2+/7llx/Ss8/WMRsFAAAAAAAAZNBfb1a/eLHUrZu5FkfJ8FDF9se7emFhYdkWAwBIVaBAoD7/vJvatl2srl3v1ZtvNjadBAAAAAAAAGRYxYqp6/fekzp0kPz8zPU4QqbuqWL561gJAJDtGjUqo507n1LZsvn5MxgAAAAAAAAuxc/Pfl+VvXvt+0OHpH/9y2TR3cvUUKVChQr/+KbelStX7ioI2cyaaLoAwB3ExCQod+604/oKFQoaqgEAAAAAAADuzrRp0gMP2NdxcWZbHCFTQ5XXX39dwcHB2dWC7LbnY+mXhaYrANzG5cuxatBgljp3rqLXXnuEM1MAAAAAAADg8iwWqVgx6exZacsWqY6L3zI4U0OVbt26qXDhwtnVguy2Z1Lq2uItBRQw1wIgjZiYBLVqtVCHDl3S6NFb5OPjpVGjHjadBQAAAAAAANy1s2ftjxER0osvmm25W14ZfSI/Me0GrAmp6yb/k3IVMtcCIEVCglUdO36qH344I0kqUiSPevasbrgKAAAAAAAAcIzGjU0XOE6Ghyo2my07O5CTchWWqg8wXQFAUnKyTX37rtTGjUclScHB/tq4sZfKls1vuAwAAAAAAABwjPr17Y+5cpntcIQMX/4rOTk5OzsAwOPYbDYNG7ZBixYdkCQFBPho9eruql491HAZAAAAAAAA4Dh/DlPOnpUuXJBc+S4jGT5TBQDgWGPGbNWHH+6QJHl7W/Tpp53UoEGY4SoAAAAAAADAscL+8pbXvn3mOhyBoQoAGDB16k698srmlP306W3UunVFg0UAAAAAAABA9qhQIXUdFWWuwxGMD1UmT56sMmXKKCAgQDVr1tTWrVsz9Lpt27bJx8dHNWrUyN5AdxB9SppxjxR9wnQJAEkxMQl6440tKft3322qvn1rmAsCAAAAAAAAstnDD9sfT58223G3jA5VlixZomHDhmnkyJHas2ePGjRooBYtWigyMvKOr4uKilKfPn306KOP5lCpC7t2VJpWSrr2W+ox/3zGcgBIuXP7aevWfipfvoBeeKGenn++nukkAAAAAAAAIFvFxdkf164123G3jA5VJkyYoP79+2vAgAGqXLmyJk6cqJIlS2rKlCl3fN3TTz+tHj16qG7dujlU6sLOfZ/+WMNxOd8BII0yZfJrx44BeuedJqZTAAAAAAAAgGz3583pK7r4FfCNDVUSEhK0a9cuhYeHpzkeHh6u7du33/Z1s2bN0tGjR/Xqq69m6OvEx8crOjo6zYfHKttaGp4slW9rugTwOGfPXldSUnKaY/nzB8pisRgqAgAAAAAAAHLOn3fy8PU1mnHXjA1VLl26JKvVqtDQ0DTHQ0NDdf78+Vu+5siRI3rppZe0YMEC+fj4ZOjrjB07VsHBwSkfJUuWvOt2l1U6XOINXCDHnT17XfXrz1Tnzkt182aS6RwAAAAAAADAmF27TBfcHeM3qv/7T2nbbLZb/uS21WpVjx499Prrr6tChQoZ/vwjRoxQVFRUysepU6fuutllRJ+S1vUyXQF4tKtX49Ss2XydOHFNK1f+omHDNphOAgAAAAAAAHJcbKz9MSZGstnMttyNjJ3ukQ1CQkLk7e2d7qyUCxcupDt7RZKuX7+unTt3as+ePfr3v/8tSUpOTpbNZpOPj48iIiLUuHHjdK/z9/eXv79/9nwTzu6nyWn33gFmOgAPFRubqNatF+nAgQuSpNKl8+m//33YcBUAAAAAAACQ86pWTV0fOiRVqWKu5W4YO1PFz89PNWvW1KZNm9Ic37Rpk+rVq5fu+UFBQdq/f7/27t2b8jFw4EBVrFhRe/fuVe3atXMq3TWc2CTteDt1b/HmXipADkpMtKpLl6Xats1+dlzhwrkVEdFLxYrlNVwGAAAAAAAA5Lx7701df/eduY67ZexMFUkaPny4evfurVq1aqlu3br65JNPFBkZqYEDB0qyX7rrzJkzmjt3rry8vFT1r6MsSYULF1ZAQEC645C07ZW0+ycOS7kKmWkBPExysk0DBqzW2rVHJEl58/pp/fqeuueegobLAAAAAAAAADO8vVPXe/aY67hbRocqXbt21eXLlzV69GidO3dOVatW1bp16xQWFiZJOnfunCIjI00muq74qNT1vwZJ+cqaawE8iM1m04svbtLcuT9Jkvz8vPX55910//1FDZcBAAAAAAAAZgUFSdHR0vffmy7JOovN5sq3hMm86OhoBQcHKyoqSkFBQaZzss/MStLVXyX/fNK/r5quATzGuHHb9J//fCFJ8vKyaOnSzurQobLhKgCAp/OYvwPDYfg9AwAAgOwwbZo0dap9vXOn2Za/yszff43dUwUA3E1CglXLlx9K2f/vf60YqAAAAAAAAAB/aNEide2qp3swVAEAB/Hz89aXX/ZR06Zl9dZbjfXkkzVNJwEAAAAAAABOIzAwde1MZ6pkhtF7qgCAu8mTx0/r1vWUt7fFdAoAAAAAAADgVPLnT13v3Ss98ICxlCzjTBUAuAsHD17U1atxaY75+HjJYmGoAgAAAAAAAPyVl5dUtap9PXWqdOSI2Z6sYKgCAFl07NhVNW48Rw0bztaZM9GmcwAAAAAAAACnFxqaur5wwVxHVjFUAYAsOH/+hpo2nafff4/RgQMXNHx4hOkkAAAAAAAAwOm98ELqOirKXEdWMVQBgEyKirqp5s3n69ixq5KkKlUKacqUVoarAAAAAAAAAOcXEpK6PnHCWEaWMVQBgEy4eTNJbdos1k8//S5JKlUqWBs39lKBAoGGywAAAAAAAADXkDev/fG338x2ZAVDFXdkS5au/mq6AnA7SUnJ6t59ubZsOSlJCgnJpYiIXipRIshwGQAAAAAAAOA6iha1P5Yta7YjKxiquBubTVrysOkKwO3YbDYNHLhGK1f+IknKndtX69b1UMWKIf/wSgAAAAAAAAB/VauW/dFiMduRFQxV3E3cJenMt6n7YBcc9QFO6OWXv9SMGXskSb6+Xlq5spseeKC44SoAAAAAAADA9SQn2x+vXjXbkRUMVdyOLe225QIzGYAbSU626cyZ65Ls0/P58zuoSRMGlgAAAAAAAEBWXLxof9y922xHVviYDkA2KtdGKljJdAXg8ry8LJo9u50KFcqlcuUKqEuXe00nAQAAAAAAAC4rONj+WLGi2Y6sYKgCABng5WXR+PHNTGcAAAAAAAAALq90afujjwtOKLj8FwDcwvffn9ahQxdNZwAAAAAAAABwIgxVAOBvDhy4oBYtFuihh2bphx9Om84BAAAAAAAA3JLVarog8xiqAMBfnDhxTc2azde1azd15Uqcxo791nQSAAAAAAAA4FaSkuyPERFmO7KCoQoA/OHChRiFh8/T2bPXJUkPPFBM8+a1N1wFAAAAAAAAuJfr9rffVKGC2Y6sYKgCAJKuX49Xy5YLdOTIFUlSxYoFtXZtD+XN62+4DAAAAAAAAHAvlSvbHw8fNtuRFQxVAHi8+PgktW+/RLt2nZMkFS+eVxERvVWoUG7DZQAAAAAAAID7CQhIXZ89a64jKxiquBubzXQB4FKs1mT16vWZvvzyuCQpf/4ARUT0VqlSwYbLAAAAAAAAAPdUq1bqeudOcx1ZwVDFnSRcl2bfa7oCcBk2m03//vc6LVt2UJKUK5ev1q7toSpVChkuAwAAAAAAANyXn1/q+quvzHVkBUMVd3LyS+nm5dR97iLmWgAXYLFYdM89BSVJPj5eWr68i+rWLWm4CgAAAAAAAPAc335ruiBzfEwHwIGSE9Pua79spgNwIcOH11WhQrnk7e2l5s3Lm84BAAAAAAAAPELPntKCBfZ1crLk5SKngDBUcVcPj5eCwkxXAC6hd+9/mU4AAAAAAAAAPMqAAalDldhYKU8esz0Z5SKzHwBwjIiIo1q9+lfTGQAAAAAAAIBHCwgwXZA1DFUAeIwffjitDh2WqH37JZozZ6/pHAAAAAAAAMBjWSyp68uXb/88Z8NQBYBHOHToolq2XKiYmERZrTatWXNENpvNdBYAAAAAAADgkXz+cnOSxMTbP8/ZMFQB4PZOnYpSePh8XbkSJ0lq1Ki05s1rL8tfx+EAAAAAAAAAjIiONl2QcQxVALi1y5djFR4+X6dP2/9kvv/+olq5spsCAnz+4ZUAAAAAAAAAcsK1a6YLMo6hCgC3deNGglq1WqhffrkkSSpfvoDWr++poCB/w2UAAAAAAAAAihe3P/r5me3IDIYqANxSQoJVHTt+qh9+OCNJKlo0jyIieqlw4dyGywAAAAAAAABIUnCw6YLMY6gCwC099dRqRUQclSTlyxegjRt7qUyZ/IarAAAAAAAAALgyhioA3FLPntWUO7evAgJ8tGZNd1WrFmo6CQAAAAAAAICL407N7iThuukCwGk0bVpOX331uC5dilX9+qVM5wAAAAAAAAD4m7g4+6PNZrYjMxiquIO4y9Kmp6Ujy02XAE7lwQeLm04AAAAAAAAAcBvx8fbHFSukBg3MtmQUl/9yB18OST9QCeIn8+FZli8/qLFjt8rmSmNtAAAAAAAAwIOdPWt/3LrVdc5W4UwVd3D5YNp9/Tekcm3MtAAGfPXVcfXosUIJCVZduBCj8eObycvLYjoLAAAAAAAAwB306SPNnWu6InM4U8XdPBMt1XlF8vYzXQLkiF27zqpt28VKSLBKkqKi4mVhngIAAAAAAAA4vd69TRdkHkMVd+ITKPnlNV0B5JjDhy+rRYsFunEjQZLUpk1FffJJa1mYqgAAAAAAAADIBgxVALikM2eiFR4+TxcvxkqSGjQopcWLO8rHhz/WAAAAAAAAAGQP3n0E4HKuXIlTs2bzdfJklCSpevVQrVrVXYGBvobLAAAAAAAAAGTFzZumCzKGoQoAlxIbm6jWrRfp558vSpLKlMmnDRt6Kl++AMNlAAAAAAAAADIjODh1fe2asYxMYagCwKUMHrxW27efkiSFhuZWRERvFS3KvYQAAAAAAAAAV+PlJfn7m67IHIYqAFzKyJENVLp0PgUF+Wv9+p4qX76A6SQAAAAAAAAAHsLHdAAAZMY99xTUtm1P6MSJa7rvvqKmcwAAAAAAAAB4EIYqAJyezWaTxWJJ2RcrllfFinHJLwAAAAAAAMDVxcfbH48fl4q6wM9Qc/kvAE5t+vTd6tZtueLjk0ynAAAAAAAAAMgmGzaYLsgYhiqubst/pEv7TVcA2WLlyl/09NNr9OmnP6tVq4UMVgAAAAAAAAA3tW6d6YKMYajiyhJjpJ3vpe4DuGE33Mc335xQt27LlJxskyRVrx4qPz9vw1UAAAAAAAAAHKlDB9MFmcNQxZVZEyVbcuq+8UfmWgAH2rv3vNq0Waz4eKskqVev6nrvvfA091UBAAAAAAAA4Po6djRdkDkMVVxV0k1pSYPUfenm0j3tjOUAjnL06BU1bz5f0dH2O1S1aFFeM2e2kZcXAxUAAAAAAADA3eTKlbpOTDTXkVEMVVzV6W+kSwdS94Eh5loABzl37rrCw+fr999jJEl165bQ0qWd5evLZb8AAAAAAAAAdxQamrqOizPXkVEMVVxVUnzafe2RZjoAB7l27aZatFigY8euSpLuvbeQ1qzpody5/QyXAQAAZK/JkyerTJkyCggIUM2aNbV169Y7Pv/jjz9W5cqVFRgYqIoVK2ru3Lm3fe7ixYtlsVjUrl07B1cDAAAAjuHlYlMKH9MBcICHxkgFK5muAO7Kyy9/qZ9++l2SVKpUsDZu7KUCBQINVwEAAGSvJUuWaNiwYZo8ebLq16+vqVOnqkWLFjp48KBKlSqV7vlTpkzRiBEjNG3aND3wwAPasWOHnnzySeXPn1+tW7dO89yTJ0/q+eefV4MGDdJ9HgAAAABZ42IzIADuauzYR/XII6UVEpJLERG9VLx4kOkkAACAbDdhwgT1799fAwYMUOXKlTVx4kSVLFlSU6ZMueXz582bp6efflpdu3ZV2bJl1a1bN/Xv31/vvPNOmudZrVb17NlTr7/+usqWLZsT3woAAACQJZa/3Er58mVzHRnFUAWAUwgODtD69T21dWs/VazIPYIAAID7S0hI0K5duxQeHp7meHh4uLZv337L18THxysgICDNscDAQO3YsUOJf7mr5+jRo1WoUCH1798/Qy3x8fGKjo5O8wEAAADkBO+/3E6ZG9UDwB3cvJmUZh8Q4KNKlRioAAAAz3Dp0iVZrVaF/vXOnJJCQ0N1/vz5W76mWbNmmj59unbt2iWbzaadO3dq5syZSkxM1KVLlyRJ27Zt04wZMzRt2rQMt4wdO1bBwcEpHyVLlsz6NwYAAABkUogLvSXIUMUVxZyX1vUwXQHclfHjt6tOnek6f/6G6RQAAACjLH+93oEkm82W7tifRo0apRYtWqhOnTry9fVV27Zt1bdvX0mSt7e3rl+/rl69emnatGkKycR/mY4YMUJRUVEpH6dOncry9wMAAAC4M4Yqrmj/DCkxJnXvE3D75wJOaO7cn/T885v000+/66GHZur69XjTSQAAADkuJCRE3t7e6c5KuXDhQrqzV/4UGBiomTNnKjY2VidOnFBkZKRKly6tvHnzKiQkREePHtWJEyfUunVr+fj4yMfHR3PnztWqVavk4+Ojo0eP3vLz+vv7KygoKM0HAAAAgPQYqriim1fS7it0NtMBZMGaNYf1xBOfp+z79PmX8ub1N1gEAABghp+fn2rWrKlNmzalOb5p0ybVq1fvjq/19fVViRIl5O3trcWLF+uxxx6Tl5eXKlWqpP3792vv3r0pH23atFGjRo20d+9eLusFAAAA3CUf0wG4S922SXlLmK4AMuTbbyPVufNSWa02SdK///2ARo1qaLgKAADAnOHDh6t3796qVauW6tatq08++USRkZEaOHCgJPtluc6cOaO5c+dKkg4fPqwdO3aodu3aunr1qiZMmKADBw5ozpw5kqSAgABVrVo1zdfIly+fJKU7DgAAADiLP24PqGvXjGZkCEMVV3T9tOkCINP27/9drVsvSrk5fbduVfXBBy1ue71wAAAAT9C1a1ddvnxZo0eP1rlz51S1alWtW7dOYWFhkqRz584pMjIy5flWq1Xjx4/Xr7/+Kl9fXzVq1Ejbt29X6dKlDX0HAAAAgONcv2664J9ZbDabzXREToqOjlZwcLCioqJc8zrBB2ZJG59I3XfbJhW/86UBANNOnLimevVm6Nw5+03pw8PLafXq7vLz8zZcBgCAZ3D5vwMjx/F7BgAAADmpc2fp+HFpwgSpoYEL22Tm77/cU8XV7JuWug4sJIXca64FyIALF2LUtOm8lIHKgw8W1/LlXRioAAAAAAAAAJAk5c5tuiDjGKq4muTE1PXj+yT/YHMtQAZ89NEO/fbbFUlSpUohWru2h/Lk8TNcBQAAAAAAAACZxz1VXJXFW8pdxHQF8I9ee+0RXb0ap5Urf9XGjb0UEpLLdBIAAAAAAAAAZAlnqgDIVl5eFk2a1EK7dj2lUqU4swoAAAAAAACA62KoAsChbDabzp+/keaYxWJR4cIudGFEAAAAAAAAALgFhiqu4rdV0lfPStEnTZcAd/Tqq1+revUp2rXrrOkUAAAAAAAAAHAo7qniCq4clj5vm/aYxWKmBbiDDz/8QW+8sUWS1LjxXP322zMqVIgzVAAAAAAAAAC4B85UcQVRx9IfK98uxzOAO1m0aL+GDt2Qsn/jjUYMVAAAAAAAAAC4Fc5UcTXVBkj3PSOFVDNdAqTYsOE39emzMmX/yisNNHRobXNBAAAAAAAAAJANGKq4mjwlpELVTVcAKX744bQ6dvxUSUnJkqSnnrpfo0c3MlwFAAAAAAAAAI7H5b8AZNmhQxfVsuVCxcYmSpI6dqysyZNbycI9fwAAAAAAAAC4IYYqALIkMjJK4eHzdeVKnCSpceMyWrCgg7y9+WMFAAAAAAAAgHvi3U8AWbJx4286fTpaknT//UX12Wdd5e/PFQUBAAAAAAAAuC/eAXV2Npv0y0LTFUA6Tz5ZUxaLRePHf6f163sqKMjfdBIAAAAAAAAAZCuGKs7u4Dz7x598Asy1AH8zYMD96tPnX/Lz8zadAgAAAAAAAADZjst/Obsrv6TdV+hspgMeLznZpt27z6U7zkAFAAAAAAAAgKdgqOJKOn0h5StrugIeyGaz6dln16t27elatGi/6RwAAAAAAAAAMIKhiiux8D8XzHjzzS366KMflZSUrMcfX6nIyCjTSQAAAAAAAACQ43iX3tnFXjBdAA83ZcqP+u9/v07ZT5/eRqVKBZsLAgAAAAAAAABDGKo4s72TpQMzTFfAgy1d+rOGDFmXsh8/Plx9+vzLYBEAAAAAAAAAmMNQxZn9sjjtPm9JMx3wSF98cUw9e66QzWbfv/RSfQ0fXtdsFAAAAAAAAAAYxFDFmdmsqeuW86X85c21wKPs3HlW7dsvUWJisiTpiSdqaMyYRw1XAQAAAAAAAIBZDFVcRaXupgvgIQ4fvqwWLRboxo0ESVLbthU1dWprWSwWw2UAAAAAAAAAYBZDFQBpnDhxTTEx9oFKw4ZhWrSoo3x8+KMCAAAAAAAAAHinFEAa4eHltGlTbzVqVFqrVnVTYKCv6SQAAAAAAAAAcAo+pgMAOJ/69Uvpyy/7cMkvAAAAAAAAAPgLzlRxVr8sls5uN10BD5CYaNXy5QfTHWegAgAAAAAAAABpMVRxRrZk6YuBqXtvf3MtcGvJyTY98cQqdeq0VP/5zybZbDbTSQAAAAAAAADgtBiqOCObTYqPSt3Xflmy8D8VHMtms+n55yM0f/4+SdIHH/ygX365ZLgKAAAAAAAAgKeJ+uPt8Bs3zHZkBO/UO7ti9aW6/zVdATf0zjvb9P7730uSvLwsWry4kypXLmS4CgAAAAAAAICnOXXK/njihNGMDGGoAnig6dN3a8SIL1P2n3zymNq1q2SwCAAAAAAAAICnKlXK/hgSYrYjIxiqAB7ms88O6emn16Ts3377UfXvf7/BIgAAAAAAAACerEIF0wUZx1AF8CBff31C3bsvV3Ky/Yb0w4fX0Ysv1jdcBQAAAAAAAACugaEK4CH27DmnNm0WKT7eKknq3bu63n03XBaLxXAZAAAAAAAAALgGhiqAhwgI8FG+fAGSpFat7tGMGW3k5cVABQAAAAAAAAAyiqEK4CEqVy6kbdueUN++NfTpp53l6+ttOgkAAAAAAAAAXIqP6QDcwo2zpgvgpkqWDNasWW1NZwAAAAAAAACAS+JMFWcTd0WaWd50BdxAXFyixozZqsREq+kUAAAAAAAAAHALDFWczcWfJGtC6r5wDWMpcF1JScnq2nWZRo78Sm3bLlZMTMI/vwgAAAAAAAAAcEcMVZxZ3lJSw3GmK+BibDabnnxytVavPixJ2ro1UseOXTVcBQAAAAAAAACuj6GKM6vcU/LNZboCLuall77Q7Nl7JUl+ft76/PNuqlYt1GwUAAAAAAAAALgBhiqAG3nvve0aN267JMlikRYs6KDGjcsYrgIAAAAAAAAA98BQBXATs2fv1QsvbErZT5nSSp06VTFYBAAAAAAAAADuhaEK4AZWr/5VAwasStm/8UYjPf10LYNFAAAAAAAAAOB+GKoALu6HH06rS5dlslptkqRnnnlQI0c2MFwFAAAAAAAAAO6HoQrg4ipWDFGtWsUkSd27V9XEic1lsVgMVwEAAAAAAACA+/ExHQDg7uTLF6CIiF4aP/47vfhifXl5MVABAAAAAAAAgOzAUAVwA4GBvnrllYamMwAAAAAAAADArXH5L8DFREfHq3//z3XxYozpFAAAAAAAAADwKAxVABdy82aS2rVbrJkz96pBg1k6efKa6SQAAAAAAAAA8BgMVQAXYbUmq2fPFdq8+YQk6eLFWMXGJpqNAgAAAAAAAAAPwlAFcAE2m02DBq3VihWHJEm5cvlq3boeqly5kOEyAAAAAAAAAPAcDFUAFzBq1GZNm7ZbkuTr66UVK7qodu0ShqsAAAAAAAAAwLMwVAGc3AcffK+33toqSbJYpDlz2qlZs/KGqwAAAAAAAADA8zBUAZzYggX7NGzYxpT9Bx80V/fu1QwWAQAAAAAAAIDnYqgCOKkffzyjvn0/T9mPGtVQzzxT22ARAAAAAAAAAHg2hiqAk7r//qLq16+GJOnpp2vq9dcfMZkDAAAAAAAAAB7Px3QAgFvz9vbS1KmPqVGj0urS5V5ZLBbTSQAAAAAAAADg0RiqAE7MYrFwDxUAAAAAAAAAcBJc/gtwEhcvxqhRoznau/e86RQAAAAAAAAAwC0wVAGcwPXr8WrZcqG+/vqEHn54trZvP2U6CQAAAAAAAADwNwxVAMPi45PUocOn2rnzrCQpTx4/FSuW13AVAAAAAAAAAODvGKoABlmtyXr88ZX64otjkqR8+QK0cWMvlS6dz2wYAAAAAAAAACAdhiqAITabTUOHrteSJT9LkgIDfbR2bQ9VrVrYcBkAAAAAAAAA4FYYqgCGjB79jSZP3ilJ8va2aNmyLqpXr6ThKgAAAAAAAADA7RgfqkyePFllypRRQECAatasqa1bt972uStWrFDTpk1VqFAhBQUFqW7dutq4cWMO1gKOMXnyj3rttW9S9rNmtVXLlvcYLAIAAAAAAAAA/BOjQ5UlS5Zo2LBhGjlypPbs2aMGDRqoRYsWioyMvOXzt2zZoqZNm2rdunXatWuXGjVqpNatW2vPnj05XJ6NEm+YLkA2O3jwov7973Up+wkTwtW7978MFgEAAAAAAAAAMsLoUGXChAnq37+/BgwYoMqVK2vixIkqWbKkpkyZcsvnT5w4US+++KIeeOAB3XPPPRozZozuuecerV69OofLs8nve6SVbUxXIJtVqVJIH3/cUhaL9NJL9fXcc3VNJwEAAAAAAAAAMsDH1BdOSEjQrl279NJLL6U5Hh4eru3bt2focyQnJ+v69esqUKDAbZ8THx+v+Pj4lH10dHTWgnPCsb8Nh4LCzHQg2w0a9IBq1iymBx4oZjoFAAAAAAAAAJBBxs5UuXTpkqxWq0JDQ9McDw0N1fnz5zP0OcaPH6+YmBh16dLlts8ZO3asgoODUz5KlnTiG4HbklPXJR+RqvQ2lgLHSky0pjv24IPFZbFYDNQAAAAAAAAAALLC+I3q//6mss1my9AbzYsWLdJrr72mJUuWqHDhwrd93ogRIxQVFZXycerUqbtuzhEPviT55jJdAQc4fTpa9947WcuWHTSdAgAAAAAAAAC4C8aGKiEhIfL29k53VsqFCxfSnb3yd0uWLFH//v316aefqkmTJnd8rr+/v4KCgtJ8ADnlypU4NWs2X0eOXFGXLku1dOnPppMAAAAAAAAAAFlkbKji5+enmjVratOmTWmOb9q0SfXq1bvt6xYtWqS+fftq4cKFatWqVXZnAlkWE5Ogxx5bqIMHL0qSypbNr4YNuU8OAAAAAAAAALgqYzeql6Thw4erd+/eqlWrlurWratPPvlEkZGRGjhwoCT7pbvOnDmjuXPnSrIPVPr06aMPPvhAderUSTnLJTAwUMHBwca+D+DvEhOt6tx5qb777rQkqUiRPIqI6K3Q0DyGywAAAAAAAAAAWWV0qNK1a1ddvnxZo0eP1rlz51S1alWtW7dOYWH2n+Y/d+6cIiMjU54/depUJSUlaciQIRoyZEjK8ccff1yzZ8/O6XzglpKTberX73OtX/+bJCk42F8bNvRU2bL5DZcBAAAAAAAAAO6G0aGKJA0ePFiDBw++5a/9fVDy9ddfZ38QcBdsNpuGD9+oBQv2S5ICAny0enV3/etfRQyXAQAAAAAAAADulrF7qgDu6O23v9UHH/wgSfL2tmjJkk5q0ID7qAAAAAAAAACAO2CoAjhIZGSURo/ekrKfNq212rSpaLAIAAAAAAAAAOBIDFUABylVKlgbNvRUUJC/3nmnifr1u890EgAAAAAAAADAgYzfUwVwJw8/XFoHDw5WsWJ5TacAAAAAAAAAAByMM1WAu3DpUmy6Y8WLB8lisRioAQAAAAAAAABkJ4YqQBYdOXJZ9947WaNGfSWbzWY6BwAAAAAAAACQzRiqAFlw9ux1hYfP14ULMXrzza2aOPF700kAAAAAAAAAgGzGUAXIpGvXbqp58/k6ceKaJKlq1cLq27eG0SYAAAAAAAAAQPZjqAJkQlxcolq3XqT9+y9IksLCgrVxYy/lzx9ouAwAAAAAAAAAkN0YqgAZlJSUrK5dl+nbbyMlSYUK5dKmTb1VrFhew2UAAAAAAAAAgJzAUAXIAJvNpgEDVmn16sOSpLx5/bRhQy/dc09Bw2UAAAAAAAAAgJzCUAXIgP/85wvNmfOTJMnPz1srV3bT/fcXNVwFAAAAAAAAAMhJDFWAf3DpUqzmz98nSbJYpIULO6hx4zKGqwAAAAAAAAAAOY2hCvAPQkJyafv2/rrnngKaMqWVOnasYjoJAAAAAAAAAGCAj+kAwBWULp1PP/00UIGBvqZTAAAAAAAAAACGcKYKcAuHDl1UUlJymmMMVAAAAAAAAADAszFUAf7mp5/Oq27dGerY8VPFxSWazgEAAAAAAAAAOAmGKs7i+Abpu9dNV3i8Y8euqnnzBYqKiteqVb/qjTe2mE4CAAAAAAAAADgJhirOYs+Haff++c10eLDff7+h8PB5On/+hiSpdu3iGjmygeEqAAAAAAAAAICzYKjiDK4dlY6vS93X+LdU5AFzPR4oKuqmmjdfoKNHr0qSKlcO0dq1PZQ7t5/hMgAAAAAAAACAs2Co4gxWtkm7bzhOsljMtHigmzeT1LbtYu3de16SVLJkkDZu7KWCBXMZLgMAAAAAAAAAOBOGKs7gyi+p6xINJd9Acy0eJikpWd27L9c335yUJBUsGKiIiN4qWTLYcBkAAAAAAAAAwNkwVHE2nb4wXeAxbDabBg1ao5Ur7UOt3Ll9tW5dT1WqFGK4DAAAAAAAAADgjBiqOJPQWpK3r+kKjxETk6j9+y9Iknx9vfTZZ1314IPFDVcBAAAAAAAAAJwVQxV4rDx5/PTFF33UokV5zZvXXk2bljOdBAAAAAAAAABwYj6mAwCT8uTx09q1PWSxWEynAAAAAAAAAACcHGeqwKNs3XpSV67EpTnGQAUAAAAAAAAAkBEMVeAxvvvulJo1m68GDWbp9Olo0zkAAAAAAAAAABfDUAUe4eefL6hVq4WKi0vSwYMXNW7cNtNJAAAAAAAAAAAXw1AFbu/kyWtq1my+rl69KUl69NEyevfdpoarAAAAAAAAAACuhqEK3NrFizEKD5+vM2euS5Jq1iyqzz7rKn9/H8NlAAAAAAAAAABXw1AFbuv69Xi1bLlQhw9fliRVqFBQ69f3VN68/obLAAAAAAAAAACuiKEK3FJ8fJLat1+inTvPSpKKFcuriIheKlQot+EyAAAAAAAAAICrYqgCt2O1Jqt378/05ZfHJUn58wcoIqKXwsLymQ0DAAAAAAAAALg0hipwOzablCuXryQpMNBHa9b00L33FjZcBQAAAAAAAABwddytG27Hx8dLs2a1VZEiedSwYZjq1StpOgkAAAAAAAAA4AYYqsAtWSwWvf12E9MZAAAAAAAAAAA3wuW/4BY+++yQfv75gukMAAAAAAAAAIAbY6gClxcRcVRduy5Tgwaz9N13p0znAAAAAAAAAADcFEMVuLQdO86oQ4clSkxM1tWrN7Vw4X7TSQAAAAAAAAAAN8VQBS7rl18uqWXLBYqJSZQktW9fSe+/39xwFQAAAAAAAADAXTFUgUs6fTpa4eHzdPlynCTpkUdKa+HCjvLx4bc0AAAAAAAAACB78A40XM7ly7EKD5+nU6eiJUn33VdEn3/eTQEBPobLAAAAAAAAAADujKEKXEpMTIJatVqoQ4cuSZLKlcuv9et7KijI33AZAAAAAAAAAMDdMVSBy7Bak9Wp01L98MMZSVKRInkUEdFboaF5DJcBAAAAAAAAADwBQxW4DG9vLzVvXk6SFBzsr40be6ls2fyGqwAAAAAAAAAAnoKbUMClPPtsHYWE5FKpUsGqXj3UdA4AAAAAAAAAwIMwVIHL6dmzuukEAAAAAAAAAIAH4vJfcGozZuzWqlW/ms4AAAAAAAAAAIChCpzXihWH9NRTa9ShwxLNnr3XdA4AAAAAAAAAwMNx+S84pc2bj6t79+VKTrZJkg4evGi4CAAAAAAAAADg6ThTBU5n9+5zatt2sRISrJKkvn1r6J13mhiuAgAAAAAAAAB4OoYqcCpHjlxW8+bzdf16giTpsccqaNq01rJYLIbLAAAAAAAAAACejqEKnMbZs9cVHj5fFy/GSpIeeqiUlizpJB8ffpsCAAAAAAAAAMzj3Wo4hatX49Ss2XydOHFNklStWmGtXt1duXL5mg0DAAAAAAAAAOAPDFVgnM1mU6dOS3XgwAVJUunS+bRxYy/lyxdguAwAAAAAAAAAgFQMVWCcxWLRiBEPKXduXxUunFsREb1UtGhe01kAAAAAAAAAAKThYzoAkKQmTcpq8+bH5e3tpXvuKWg6BwAAAAAAAACAdBiqwGk88EBx0wkAAAAAAAAAANwWl/+CEePGbdObb26RzWYznQIAAAAAAAAAQIZwpgpy3MyZe/Sf/3whSbp4MUYTJzaXxWIxXAUAAAAAAAAAwJ1xpgpy1MqVv+jJJ1en7END8zBQAQAAAAAAAAC4BIYqyDFbtpxUt27LlJxsv+TXsGG1NWLEQ4arAAAAAAAAAADIGIYqyBE//XRerVsvUny8VZLUs2c1jR/fjLNUAAAAAAAAAAAug6EKst3Ro1fUrNl8RUfHS5JatCivWbPaysuLgQoAAAAAAAAAwHUwVEG2On/+hsLD5+v332MkSXXqlNDSpZ3l6+ttuAwAAAAAAAAAgMxhqIJs1bfvSh07dlWSVKVKIa1d20O5c/sZrgIAAAAAAAAAIPMYqiBbffxxS5Upk08lSwZp48ZeKlAg0HQSAAAAAAAAAABZ4mM6wKPZbNJXz0i2ZNMl2aZcuQLatu0JXb+eoBIlgkznAAAAAAAAAACQZQxVTLq0X9r7cereL6+5Fgex2WxKTrbJ2zv1JKiiRfOqaFGDUQAAAAAAAAAAOACX/zIp4Xra/YMjzHQ40MiRX6lbt+WKj08ynQIAAAAAAAAAgENxpoqzqPl/Uummpivuyvvvf6exY7+VJF27dlMbN/aSl5fFcBUAAAAAAAAAAI7BmSpwiHnzftLw4REp+/btKzFQAQAAAAAAAAC4FYYquGtr1x5Wv36fp+xfe+1hDR78gMEiAAAAAAAAAAAcj6EK7sr27afUufNSWa02SdLgwbX03/8+bLgKAAAAAAAAAADHY6iCLDtw4IJatVqouDj7Tem7dLlXkya1kMXCZb8AAACAjJo8ebLKlCmjgIAA1axZU1u3br3j8z/++GNVrlxZgYGBqlixoubOnZvm16dNm6YGDRoof/78yp8/v5o0aaIdO3Zk57cAAAAAeAyGKsiSEyeuqVmz+bp27aYkqUmTspo7t528vfktBQAAAGTUkiVLNGzYMI0cOVJ79uxRgwYN1KJFC0VGRt7y+VOmTNGIESP02muv6eeff9brr7+uIUOGaPXq1SnP+frrr9W9e3dt3rxZ3333nUqVKqXw8HCdOXMmp74tAAAAwG3xDjiy5KWXvtDZs9clSQ88UEwrVnSRv7+P4SoAAADAtUyYMEH9+/fXgAEDVLlyZU2cOFElS5bUlClTbvn8efPm6emnn1bXrl1VtmxZdevWTf3799c777yT8pwFCxZo8ODBqlGjhipVqqRp06YpOTlZX375ZU59WwAAAIDb4l1wZMknn7TWxYuxOnMmWmvX9lDevP6mkwAAAACXkpCQoF27dumll15Kczw8PFzbt2+/5Wvi4+MVEBCQ5lhgYKB27NihxMRE+fr6pntNbGysEhMTVaBAgdu2xMfHKz4+PmUfHR2dmW8FAAAA8BicqYIsCQry17p1PbR58+MqVCi36RwAAADA5Vy6dElWq1WhoaFpjoeGhur8+fO3fE2zZs00ffp07dq1SzabTTt37tTMmTOVmJioS5cu3fI1L730kooXL64mTZrctmXs2LEKDg5O+ShZsmTWvzEAAADAjTFUQYZYrcmKjo5Pc8zf30dFi+Y1VAQAAAC4B4vFkmZvs9nSHfvTqFGj1KJFC9WpU0e+vr5q27at+vbtK0ny9vZO9/xx48Zp0aJFWrFiRbozXP5qxIgRioqKSvk4depU1r8hAAAAwI0xVME/stlsGjJknRo0mKVz566bzgEAAADcQkhIiLy9vdOdlXLhwoV0Z6/8KTAwUDNnzlRsbKxOnDihyMhIlS5dWnnz5lVISEia57733nsaM2aMIiIiVL169Tu2+Pv7KygoKM0HAAAAgPQYqphkTTBdkCGvvfa1pk7dpX37flejRnOUkGA1nQQAAAC4PD8/P9WsWVObNm1Kc3zTpk2qV6/eHV/r6+urEiVKyNvbW4sXL9Zjjz0mL6/U/7x799139cYbb2jDhg2qVatWtvQDAAAAnogb1Zty5bC0tLHpin/00Uc7NHr0lpT9qFEN5eeX/rICAAAAADJv+PDh6t27t2rVqqW6devqk08+UWRkpAYOHCjJflmuM2fOaO7cuZKkw4cPa8eOHapdu7auXr2qCRMm6MCBA5ozZ07K5xw3bpxGjRqlhQsXqnTp0ilnwuTJk0d58uTJ+W8SAAAAcCMMVUw5uirtvkAFMx13sGjRfg0duj5lP3FiM/XseefLBgAAAADIuK5du+ry5csaPXq0zp07p6pVq2rdunUKCwuTJJ07d06RkZEpz7darRo/frx+/fVX+fr6qlGjRtq+fbtKly6d8pzJkycrISFBnTp1SvO1Xn31Vb322ms58W0BAAAAbouhiinJianrezpIVfuba7mFjRt/U58+K2Wz2fcjRzbQs8/WMRsFAAAAuKHBgwdr8ODBt/y12bNnp9lXrlxZe/bsuePnO3HihIPKAAAAAPwd91RxBlUel7yc55JaP/xwWh07fqqkpGRJ0pNP3q833mhkuAoAAAAAAAAAALMYqiCNQ4cuqlWrhYqJsZ9J06FDZU2Z0koWi8VwGQAAAAAAAAAAZjFUQRrTpu3W5ctxkqRHHimtBQs6yNub3yYAAAAAAAAAAHBPFaTx3nvhSky0atu2U/r8824KCOC3CAAAAAAAAAAAEkMV/I2Xl0WTJrVQTEyi8uTxM50DAAAAAAAAAIDT4LpOHi4hwarjx6+mOWaxWBioAAAAAAAAAADwNwxVPFhysk19+67Ugw9O148/njGdAwAAAAAAAACAU2Oo4qFsNpuGDdugRYsO6NKlWLVqtVAxMQmmswAAAAAAAAAAcFoMVUyJOWf0y7/11lZ9+OEOSZK3t0UzZrRR7txc8gsAAAAAAAAAgNthqGLC/hnSng+NffmpU3dq1KjNKfsZM9qodeuKxnoAAAAAAAAAAHAFDFVMOLI87T64TI596WXLDmrQoLUp+3ffbarHH6+RY18fAAAAAAAAAABXxVDFBFty6vrRyVKhajnyZb/66rh69lwhm82+f/HFenr++Xo58rUBAAAAAAAAAHB1DFVMq9wjR77Mrl1n1bbtYiUkWCVJ/frV0NtvN8mRrw0AAAAAAAAAgDtgqOIhvv/+tG7cSJAktWlTUZ980loWi8VwFQAAAAAAAAAArsPHdAByxpAhDypXLl/Nm7dPixd3lI8P8zQAAAAAAAAAADKDoYoH6dfvPj3+eA15eXGGCgAAAAAAAAAAmcXpCm4qNjZRW7eeTHecgQoAAAAAAAAAAFnDUMUNJSZa1aXLUjVuPFcLFuwznQMAAAAAAAAAwP+3d+dxOaX//8Bfd8vdpkUJpVQkyWCoQXySRormI8PYRmPfjWEYfLJN+djGlm1sM1KY7MkwzChbsnxmkhBCaGHUWMaQilTX7w+/ztetu1V14349H4/78eicc51z3ue+Tuec636f65z3ApMq75mCAoHhw/fjwIEk5OUV4MsvD+Lhw2xVh0VERERERERERERE9M5jUuU9IoTAlCmR2Lz5AgBAR0cTe/f2g5mZvoojIyIiIiIiIiIiIiJ69zGp8h5ZvPg0goL+B+Dlu1O2bfsMHTvaqjYoIiIiIiIiIiIiIqL3BJMq74mNG+Pxn/8clobXr/83evRoosKIiIiIiIiIiIiIiIjeL0yqvAd+/vkqRozYLw3Pn/8xhg9vpcKIiIiIiIiIiIiIiIjeP0yqvONOnEhF3767UVAgAABff90G/v7/UnFURERERERERERERETvHyZV3nHZ2S+goSEDAHzxRXMsXeoNmUym4qiIiIiIiIiIiIiIiN4/TKq847p0sceRIwPh59cMGzf6SgkWIiIiIiIiIiIiIiKqXFqqDoDenKurNVxdrVUdBhERERERERERERHRe409Vd4x//zzDCEh8aoOg4iIiIiIiIiIiIhI7bCnyjskJ+cFunffjhMnUnHlyn0sWtSZ708hIiIiIiIiIiIiIqom7KnyjsjLK0C/fuE4cSIVABAaegF372aqOCoiIiIiIiIiIiIiIvXBpMo7QAiBkSP3Y9++awCAGjXk+PVXP9SrZ6TiyIiIiIiIiIiIiIiI1AeTKu+AadOOICTkPABALtfE3r194eJiqdqgiIiIiIiIiIiIiIjUDJMqb7mlS09j4cJTAACZDPjppx7o1KmBiqMiIiIiIiIiIiIiIlI/TKpUt4I84M6JMhXdvPkCJk+OkoZXr/ZB795NqyoyIiIiIiIiIiIiIiIqAZMq1e3nnkBeTqnFDh5MwtChP0vDs2d3xJgxH1VhYEREREREREREREREVBImVapb2uH/+9vQGtCuobRYgwY1pRfRjxv3EWbN6lAd0RERERERERERERERUTG0VB2AWusVBWhoKp3k6FgLp04NxZo1sZg792PIZLJqDo6IiIiIiIiIiIiIiF7FpIqqmDcHTBuXWMTKygjz53eqpoCIiIiIiIiIiIiIiKgkfPzXW+LevSxMnRqF3Nx8VYdCRERERERERERERERKsKfKW+DJk+fo2jUM586l4+LFvxAe3gcGBnJVh0VERERERERERERERK9gTxUVe/YsDz167MC5c+kAgEuX7uHvv3NUHBUREREREREREREREb2OSRUVys8vwBdf7MHRo8kAAFNTPURGDoC1tbGKIyMiIiIiIiIiIiIiotcxqaIiQgBffnkQ4eGJAAB9fW0cONAfTk7mKo6MiIiIiIiIiIiIiIiUUXlSZc2aNbCzs4Ouri6cnZ0RExNTYvno6Gg4OztDV1cXDRo0wLp166op0soVENEE69fHAQC0tDQQHt4HbdtaqTgqIiIiIiIiIiIiIiIqjkqTKjt27MDXX3+NGTNmID4+Hm5ubujatSvS0tKUlk9OToaPjw/c3NwQHx+P6dOnY/z48QgPD6/myN/Mypg2mPNzE2l406ZP0aWLvQojIiIiIiIiIiIiIiKi0qg0qRIUFIRhw4Zh+PDhaNKkCZYvXw5ra2usXbtWafl169ahfv36WL58OZo0aYLhw4dj6NChWLJkSTVHXnH7LzXEhJ+7SsMrVnRB//7NVBgRERERERERERERERGVhcqSKrm5uYiLi4OXl5fCeC8vL5w+fVrpPGfOnClS3tvbG2fPnsWLFy+UzvP8+XM8efJE4aNKbg3uwM0uFQAwc6Ybxo9vo9J4iIiIiIiIiIiIiIiobFSWVHnw4AHy8/NRp04dhfF16tRBRkaG0nkyMjKUls/Ly8ODBw+UzrNgwQIYGxtLH2tr68rZgAoy0X+OQyO3YN3gePz3vx4qjYWIiIiIiIiIiIiISNVsbIAWLYBatVQdSem0VB2ATCZTGBZCFBlXWnll4wtNmzYNkyZNkoafPHmi2sTK0CToQWCUhjZQwnYSEREREREREREREamDMWNUHUHZqSypUqtWLWhqahbplXLv3r0ivVEK1a1bV2l5LS0tmJmZKZ1HR0cHOjo6lRN0ZTCsp+oIiIiIyiQ/P7/Yx2sS0dtFU1MTWlpaJd6cRFTZhBDIy8tDfn6+qkMhIqoy2tra0NTUVHUYRET0FlFZUkUul8PZ2RlRUVHo0aOHND4qKgrdu3dXOo+rqyv279+vMC4yMhIuLi7Q1tau0niJiIjUydOnT3Hnzh2pRygRvf309fVhYWEBuVyu6lBIDeTm5iI9PR3Z2dmqDoWIqErJZDJYWVmhRo0aqg6FiIjeEip9/NekSZMwYMAAuLi4wNXVFT/88APS0tIwevRoAC8f3fXnn39i8+bNAIDRo0fj+++/x6RJkzBixAicOXMGwcHB2LZtmyo3g4iI6L2Sn5+PO3fuQF9fH+bm5rzznegtJ4RAbm4u7t+/j+TkZDRq1AgaGip7dSKpgYKCAiQnJ0NTUxOWlpaQy+U8VxDRe0kIgfv37+POnTto1KgRe6wQEREAFSdV+vbti4cPH+K///0v0tPT8cEHH+DgwYOwsbEBAKSnpyMtLU0qb2dnh4MHD2LixIlYvXo1LC0tsXLlSnz22Weq2gQiIqL3zosXLyCEgLm5OfT09FQdDhGVgZ6eHrS1tZGamorc3Fzo6uqqOiR6j+Xm5qKgoADW1tbQ19dXdThERFXK3NwcKSkpePHiBZMqREQE4C14Uf3YsWMxduxYpdNCQ0OLjHN3d8e5c+eqOCoiIiLiXcdE7xb2TqHqxn2OiNQBr4mJiOh1vAomIiIiIiIiIiIiIiIqAyZViIiIiIiIiIiIiIiIyoBJFSIiIiI19/DhQ9SuXRspKSmqDoWK0atXLwQFBak6DCKqAFtbWyxfvrzSy74PZDIZ9u7dCwBISUmBTCbD+fPnVRpTZcrNzYW9vT1OnTql6lCoGJMnT8b48eNVHQYREb1jmFQhIiKi98LgwYMhk8kgk8mgpaWF+vXrY8yYMXj06FGRsqdPn4aPjw9q1qwJXV1dNGvWDEuXLkV+fn6RsseOHYOPjw/MzMygr68PJycnfPPNN/jzzz+rY7OqxYIFC9CtWzfY2tqqOpQqEx0dDWdnZ+jq6qJBgwZYt25dqfMcOXIE7dq1g6GhISwsLPCf//wHeXl5CmUSEhLg7u4OPT091KtXD//9738hhJCmnzx5Eu3bt4eZmRn09PTg6OiIZcuWFVlXeHg4nJycoKOjAycnJ0RERChM//bbbzFv3jw8efKkgt8AEb16ntDW1kaDBg0wefJkZGVlVel6Y2NjMXLkyEov+yY6duwofRdyuRwNGzbEtGnT8Pz58ypftzr54YcfYGNjg/bt26s6lCpT2nlQmXPnzqFz584wMTGBmZkZRo4ciadPnyqUiY2NRadOnWBiYoKaNWvCy8urSMJNCIElS5bAwcEBOjo6sLa2xvz586Xpx48fl/bzVz9Xr16VykydOhUhISFITk5+8y+DiIjUBpMqRERE9N7o0qUL0tPTkZKSgg0bNmD//v0YO3asQpmIiAi4u7vDysoKx44dw9WrVzFhwgTMmzcP/fr1U/ghYP369fD09ETdunURHh6OK1euYN26dXj8j52ifAAAMetJREFU+DGWLl1abduVm5tbZcvOyclBcHAwhg8f/kbLqcoY31RycjJ8fHzg5uaG+Ph4TJ8+HePHj0d4eHix81y8eBE+Pj7o0qUL4uPjsX37duzbtw/+/v5SmSdPnqBz586wtLREbGwsVq1ahSVLlij0KDEwMMC4ceNw4sQJJCYmYubMmZg5cyZ++OEHqcyZM2fQt29fDBgwABcuXMCAAQPQp08f/P7771KZ5s2bw9bWFmFhYZX87RCpl8LzxK1btzB37lysWbMGkydPVlr2xYsXlbJOc3Nz6OvrV3rZNzVixAikp6fjxo0bWLRoEVavXo3AwMBqWffborLquDirVq164/NrVcf4JspyHnzd3bt34enpCXt7e/z+++/47bffcPnyZQwePFgqk5mZCW9vb9SvXx+///47Tp48CSMjI3h7eyt8HxMmTMCGDRuwZMkSXL16Ffv370fr1q2LrPPatWtIT0+XPo0aNZKm1a5dG15eXmW62YKIiEgi1Mzjx48FAPH48WNVh0JERPRWysnJEVeuXBE5OTmqDqVcBg0aJLp3764wbtKkScLU1FQafvr0qTAzMxM9e/YsMv++ffsEALF9+3YhhBC3b98WcrlcfP3110rX9+jRo2JjefTokRgxYoSoXbu20NHREU2bNhX79+8XQggREBAgWrRooVB+2bJlwsbGpsi2zJ8/X1hYWAgbGxvh7+8v2rRpU2RdzZo1E99++600vHHjRuHo6Ch0dHRE48aNxerVq4uNUwghwsPDRa1atRTG5eXliaFDhwpbW1uhq6srHBwcxPLlyxXKKItRCCHu3Lkj+vTpI0xMTISpqanw9fUVycnJ0nx//PGH8PT0FGZmZsLIyEh06NBBxMXFlRjjm5o6dapwdHRUGDdq1CjRtm3bYueZNm2acHFxURgXEREhdHV1xZMnT4QQQqxZs0YYGxuLZ8+eSWUWLFggLC0tRUFBQbHL7tGjh/jiiy+k4T59+oguXboolPH29hb9+vVTGBcYGCjc3NyKXW5J/7u8BqbyKmmfeX1fKygQIjtbNZ8S/tWKUHaeGD58uKhbt64Q4v+Oz8HBwcLOzk7IZDJRUFAg/vnnHzFixAhhbm4uDA0NhYeHhzh//rzCcn7++Wfh7OwsdHR0hJmZmejRo4c0zcbGRixbtkwaDggIENbW1kIulwsLCwvx1VdfFVs2NTVV+Pr6CgMDA2FoaCh69+4tMjIyFJbVokULsXnzZmFjYyOMjIxE3759peNUcdzd3cWECRMUxvXs2VO0atVKGi4oKBALFy4UdnZ2QldXVzRv3lzs2rVLYZ5Lly4JHx8fYWhoKGrUqCH+9a9/iRs3bgghyna8ByAiIiKEEEIkJycLACI+Pr7YuJ89eyamTJkirKyshFwuF/b29mLDhg1CCCFCQkKEsbGxQvmIiAjx6s8eyup43bp1wtLSUuTn5yvM261bNzFw4EBpeN++faJVq1ZCR0dH2NnZicDAQPHixYtiY42LixMaGhpF/oemTp0qGjVqJPT09ISdnZ2YOXOmyM3NLTHGsuyHN27cEL6+vqJ27drCwMBAuLi4iKioqGLjqwwVOQ+uX79e1K5dW+H7jo+PFwBEUlKSEEKI2NhYAUCkpaVJZS5evCgASPvXlStXhJaWlrh69Wqx8R07dkwAKPGaTQghQkNDhbW1dbHT39VrYyIiKp/ytJm0VJPKISIionfKTy5AVkb1r9egLvDF2QrNeuvWLfz222/Q1taWxkVGRuLhw4dK70ru1q0bHBwcsG3bNvTt2xe7du1Cbm4upk6dqnT5JiYmSscXFBSga9euyMzMxE8//YSGDRviypUr0NTULFf8R44cgZGREaKioqTeM9999x1u3ryJhg0bAgAuX76MhIQE7N69GwDw448/IiAgAN9//z1atmyJ+Ph4jBgxAgYGBhg0aJDS9Zw4cQIuLi5FtsHKygo7d+5ErVq1cPr0aYwcORIWFhbo06dPsTFmZ2fDw8MDbm5uOHHiBLS0tDB37lx06dIFFy9ehFwuR2ZmJgYNGoSVK1cCAJYuXQofHx8kJSXB0NBQaYxhYWEYNWpUid/X+vXr4efnp3TamTNn4OXlpTDO29sbwcHBePHihcI+Uuj58+fQ1dVVGKenp4dnz54hLi4OHTt2xJkzZ+Du7g4dHR2F5U6bNg0pKSmws7Mrstz4+HicPn0ac+fOVYhv4sSJReJ7/b0KrVu3xoIFC/D8+XOFdRKp2rNngJubatYdEwPo6VV8fj09PYU732/cuIGdO3ciPDxcOm5/8sknMDU1xcGDB2FsbIz169ejU6dOuH79OkxNTXHgwAH07NkTM2bMwJYtW5Cbm4sDBw4oXd/u3buxbNkybN++HU2bNkVGRgYuXLigtKwQAp9++ikMDAwQHR2NvLw8jB07Fn379sXx48elcjdv3sTevXvxyy+/4NGjR+jTpw++++47zJs3r8zfw4ULF3Dq1CmFx0DOnDkTe/bswdq1a9GoUSOcOHECX3zxBczNzeHu7o4///wTHTp0QMeOHXH06FEYGRnh1KlT0mMSK3K8L83AgQNx5swZrFy5Ei1atEBycjIePHhQrmW8Xsf16tXD+PHjcezYMXTq1AkA8OjRIxw6dAj79+8HABw6dAhffPEFVq5cCTc3N9y8eVN6TFtAQIDS9Zw4cQIODg4wMjJSGG9oaIjQ0FBYWloiISEBI0aMgKGhocL1RkX2w6dPn8LHxwdz586Frq4uNm3ahG7duuHatWuoX7++0hhjYmLQtWvXEr+v6dOnY/r06UqnVeQ8+Pz5c8jlcmho/N+DU/T+/z/xyZMnYW9vj8aNG6NWrVoIDg7G9OnTkZ+fj+DgYDRt2hQ2NjYAgP3796NBgwb45Zdf0KVLFwgh4OnpiUWLFsHU1FRhnS1btsSzZ8/g5OSEmTNnwsPDQ2F669atcfv2baSmpkrLJyIiKgmTKkRERFS6rAzg6dv/DpFffvkFNWrUQH5+Pp49ewYACo+guH79OgCgSZMmSud3dHSUyiQlJcHIyAgWFhbliuHw4cP4448/kJiYCAcHBwBAgwYNyr0tBgYG2LBhA+RyuTSuefPm2Lp1K2bNmgXgZbLho48+ktYzZ84cLF26FD179gQA2NnZ4cqVK1i/fn2xSZWUlBRYWloqjNPW1sbs2bOlYTs7O5w+fRo7d+5USKq8HuPGjRuhoaGBDRs2QCaTAQBCQkJgYmKC48ePw8vLCx9//LHCutavX4+aNWsiOjoa//73v5XG6OvrizZt2pT4fdWpU6fYaRkZGUWm16lTB3l5eXjw4IHSOi5Mamzbtg19+vRBRkaGlAhJT0+Xlvv6e2gK15ORkaHwY5KVlRXu37+PvLw8BAYGKjwOprj4MjIUE5n16tXD8+fPkZGRwR99iCrBH3/8ga1bt0o/pAMvH2W4ZcsWmJubAwCOHj2KhIQE3Lt3T/rheMmSJdi7dy92796NkSNHSo+PfPW42aJFC6XrTEtLQ926deHp6QltbW3Ur19f6eOKgJfnk4sXLyI5ORnW1tYAgC1btqBp06aIjY3FRx99BOBlIjw0NFRKVAwYMABHjhwpNamyZs0abNiwAS9evEBubi40NDSwevVqAEBWVhaCgoJw9OhRuLq6Anh5Ljt58iTWr18Pd3d3rF69GsbGxti+fbuUnC48HwGo0PG+JNevX8fOnTsRFRUFT09PKabyer2OgZePhXt1X9i1axdMTU2l4Xnz5sHf3186lzZo0ABz5szB1KlTi02qKDu/Ai+TVYVsbW3xzTffYMeOHQpJlYrshy1atFDY7+bOnYuIiAjs27cP48aNUxqji4tLkfeUvO71BMWrynMeLPTxxx9j0qRJWLx4MSZMmICsrCwpaVN4fjU0NMTx48fRvXt3zJkzB8DLfevQoUPQ0nr5M9atW7eQmpqKXbt2YfPmzcjPz8fEiRPRq1cvHD16FABgYWGBH374Ac7Oznj+/Dm2bNmCTp064fjx4+jQoYMUU7169QC8rDOeX4mIqCyYVCEiIqLSGdR9J9br4eGBtWvXIjs7Gxs2bMD169fx1VdfFSkninmBqhBCSga8+nd5nD9/HlZWVgo/LFVEs2bNFBIqAODn54eNGzdi1qxZEEJg27Zt+PrrrwEA9+/fx+3btzFs2DCMGDFCmicvLw/GxsbFricnJ6dIjwwAWLduHTZs2IDU1FTk5OQgNzcXH374YYkxxsXF4caNG0XuQH727Blu3rwJALh37x6+/fZbHD16FH/99Rfy8/ORnZ2NtLS0YmM0NDSs8F3NhV6vy8J9oLg69vLywuLFizF69GgMGDAAOjo6mDVrFk6ePKnQ66isy42JicHTp0/xv//9D/7+/rC3t8fnn39e4nJeH1d4J292dnap20tUnXR1X/YYUdW6y6Mw+Z6Xl4cXL16ge/fuWLVqlTTdxsZG4cf2uLg4PH36FGZmZgrLycnJkY5r58+fVzjulqR3795Yvnw5GjRogC5dusDHxwfdunWTfih+VWJiIqytraWECgA4OTnBxMQEiYmJUlLF1tZW4RhpYWGBe/fuASja0+/XX3+F2//vVuTn54cZM2bgyZMnWLhwIYyMjPDZZ58BAK5cuYJnz56hc+fOCjHl5uaiZcuW0na7ubkp7e0HVOx4X5Lz589DU1MT7u7uFZq/0Ot1DLz8LkaOHIk1a9ZAR0cHYWFh6Nevn3S8j4uLQ2xsrEKiqvAGjuzsbKXvwSnu/Lp7924sX74cN27cwNOnT5GXl1ekN0tF9sOsrCzMnj0bv/zyC+7evYu8vDzk5OSU+H3r6enB3t6+2OllUd7za9OmTbFp0yZMmjQJ06ZNg6amJsaPH486depI33dOTg6GDh2K9u3bY9u2bcjPz8eSJUvg4+OD2NhY6OnpoaCgAM+fP8fmzZula67g4GA4Ozvj2rVraNy4sfQp5Orqitu3b2PJkiUKSRWeX4mIqLyYVCEiIqLSVfARXNXNwMBA+nFg5cqV8PDwwOzZsxXucgRe/lDVrl27IvNfvXoVTk5OUtnHjx8jPT29XL1V9Ep5Do2GhkaRpI6yl9AaGBgUGde/f3/4+/vj3LlzyMnJwe3bt9GvXz8AL+9UBl4+Auz1Xh0lPXqsVq1aePTokcK4nTt3YuLEiVi6dClcXV1haGiIxYsXK7w4XVmMBQUFcHZ2Vvoy9cIfhwYPHoz79+9j+fLlsLGxgY6ODlxdXUt80f2bPv6rbt26RXp93Lt3D1paWkV+oHrVpEmTMHHiRKSnp6NmzZpISUnBtGnTpDtvi1suULTnTOE8zZo1w19//YXAwEApqVLccl5fxt9//w0ARX4MJFI1mezNHsFVnQqT79ra2rC0tCySEFB2XLOwsFB43FahwsdAlnbcf5W1tTWuXbuGqKgoHD58GGPHjsXixYsRHR1dJJbikvuvj399PplMJp0TXu/pV3hHPgAYGxtL58yffvoJTZs2RXBwMIYNGybNf+DAAYV5AEg9JUrb7ooc70tSlefXbt26oaCgAAcOHMBHH32EmJgYhZ6uBQUFmD17ttQT9FXKEifAy/NrQkKCwrj//e9/Uq8mb29vqafP0qVLS4yxLPvhlClTcOjQISxZsgT29vbQ09NDr169Svy+3/TxX+U5D76qf//+6N+/P/766y8YGBhAJpMhKChIOldu3boVKSkpOHPmjPSYsK1bt6JmzZr4+eef0a9fP1hYWEBLS0vhJpbCnshpaWkKyZRXtW3bFj/99JPCOJ5fiYiovJhUISIiovdWQEAAunbtijFjxsDS0hJeXl4wNTXF0qVLiyRV9u3bh6SkJCkB06tXL/j7+2PRokVYtmxZkWX/888/St+r0rx5c9y5cwfXr19X2lvF3NwcGRkZCj+KlfbojUJWVlbo0KEDwsLCkJOTA09PT+lHizp16qBevXq4detWsckFZVq2bFnkx4WYmBi0a9cOY8eOlcYV3glbklatWmHHjh2oXbt2kbtuX132mjVr4OPjAwC4fft2qc/Df9PHf7m6ukrPxS8UGRkJFxeXYu+wLiSTyaTHt2zbtg3W1tZo1aqVtNzp06cjNzdX6rETGRkJS0vLIo9DeZUQAs+fP1eILyoqSuG9KpGRkUX20UuXLsHKygq1atUqMWYiKt6ryfeyaNWqFTIyMqClpVXs/3Xz5s1x5MgRDBkypEzL1NPTg6+vL3x9ffHll1/C0dERCQkJ0rGlkJOTE9LS0nD79m2pt8qVK1fw+PHjYh9j+bqy9vTT1tbG9OnTMW3aNHz++edwcnKCjo4O0tLSiu0Z0rx5c2zatKnYd1NV5HhfkmbNmqGgoADR0dHS479eZW5ujszMTGRlZUlJibKeX/X09NCzZ0+EhYXhxo0bcHBwgLOzszS9VatWuHbtWrn2nZYtW2Lt2rUK5/tTp07BxsYGM2bMkMqlpqaWuqyy7IcxMTEYPHgwevToAQB4+vQpUlJSSlzumz7+q6LnwUKF5+6NGzdCV1dX6hmVnZ0NDQ0NheRh4XBhwq99+/bIy8tTeNdc4SNcS3qEV3x8fJGbZS5dugRtbW00bdq01JiJiIgAQKP0IkRERETvpo4dO6Jp06aYP38+gJc/pq1fvx4///wzRo4ciYsXLyIlJQXBwcEYPHgwevXqJb0zxNraGsuWLcOKFSswbNgwREdHIzU1FadOncKoUaOk5Mvr3N3d0aFDB3z22WeIiopCcnIyfv31V/z2229STPfv38eiRYtw8+ZNrF69Gr/++muZt8nPzw/bt2/Hrl278MUXXyhMCwwMxIIFC7BixQpcv34dCQkJCAkJUbjb9nXe3t64fPmyQm8Ve3t7nD17FocOHcL169cxa9YsxMbGlim2WrVqoXv37oiJiUFycjKio6MxYcIE3LlzR1r2li1bkJiYiN9//x1+fn6l3n1saGgIe3v7Ej8l/Wg4evRopKamYtKkSUhMTMTGjRsRHByMyZMnS2UiIiLg6OioMN/ixYuRkJCAy5cvY86cOfjuu++wcuVKqedP//79oaOjg8GDB+PSpUuIiIjA/PnzMWnSJOmHoNWrV2P//v1ISkpCUlISQkJCsGTJEoW6mzBhAiIjI7Fw4UJcvXoVCxcuxOHDh6VHuxWKiYmBl5dXqfVARJXH09MTrq6u+PTTT3Ho0CGkpKTg9OnTmDlzJs6efdmLMyAgANu2bUNAQAASExORkJCARYsWKV1eaGgogoODcenSJdy6dQtbtmyBnp6e0h+BPT090bx5c/j5+eHcuXP4448/MHDgQLi7u8PFxaXSt7V///6QyWRYs2YNDA0NMXnyZEycOBGbNm3CzZs3ER8fj9WrV2PTpk0AgHHjxuHJkyfo168fzp49i6SkJGzZsgXXrl0DULHjfUlsbW0xaNAgDB06FHv37kVycjKOHz+OnTt3AgDatGkDfX19TJ8+HTdu3MDWrVsRGhpa5uX7+fnhwIED2LhxY5Hz67fffovNmzcjMDAQly9fRmJiInbs2KHwfpTXeXh4ICsrC5cvX5bG2dvbIy0tDdu3b8fNmzexcuVKRERElBpbWfZDe3t77NmzB+fPn8eFCxfQv39/KQFRnMLHf5X0KSmpUpbz4B9//AFHR0f8+ef/vZvv+++/x7lz53D9+nWsXr0a48aNw4IFC6SbVTp37oxHjx7hyy+/RGJiIi5fvowhQ4ZAS0tLesm8p6cnWrVqhaFDhyI+Ph5xcXEYNWoUOnfuLN3Usnz5cuzduxdJSUm4fPkypk2bhvDw8CLvmImJiYGbm9sb7Z9ERKRmhJp5/PixACAeP36s6lCIiIjeSjk5OeLKlSsiJydH1aGUy6BBg0T37t2LjA8LCxNyuVykpaVJ406cOCG6dOkijI2NhVwuF05OTmLJkiUiLy+vyPxRUVHC29tb1KxZU+jq6gpHR0cxefJkcffu3WJjefjwoRgyZIgwMzMTurq64oMPPhC//PKLNH3t2rXC2tpaGBgYiIEDB4p58+YJGxubUrdFCCEePXokdHR0hL6+vsjMzFS6vR9++KGQy+WiZs2aokOHDmLPnj3FxiqEEG3bthXr1q2Thp89eyYGDx4sjI2NhYmJiRgzZozw9/cXLVq0KDXG9PR0MXDgQFGrVi2ho6MjGjRoIEaMGCFde507d064uLgIHR0d0ahRI7Fr1y5hY2Mjli1bVmKMb+r48eOiZcuWQi6XC1tbW7F27VqF6SEhIeL1S2MPDw9hbGwsdHV1RZs2bcTBgweLLPfixYvCzc1N6OjoiLp164rAwEBRUFAgTV+5cqVo2rSp0NfXF0ZGRqJly5ZizZo1Ij8/X2E5u3btEo0bNxba2trC0dFRhIeHK0zPyckRRkZG4syZM8VuY0n/u7wGpvIqaZ95384ThQICAhSOc4WePHkivvrqK2FpaSm0tbWFtbW18PPzUzivhIeHS8feWrVqiZ49e0rTXj3GRUREiDZt2ggjIyNhYGAg2rZtKw4fPqy0rBBCpKamCl9fX2FgYCAMDQ1F7969RUZGRokxL1u2TOGcooy7u7uYMGFCkfHz5s0T5ubmIjMzUxQUFIgVK1ZIxyZzc3Ph7e0toqOjpfIXLlwQXl5eQl9fXxgaGgo3Nzdx8+ZNIUTZjvcAREREhBBCiOTkZAFAxMfHFxt3Tk6OmDhxorCwsBByuVzY29uLjRs3StMjIiKEvb290NXVFf/+97/FDz/8oHBsL66OhRAiLy9PWFhYCADSNrzqt99+E+3atRN6enrCyMhItG7dWvzwww/FxiqEEP369RP+/v4K46ZMmSLMzMxEjRo1RN++fcWyZcuEsbFxqTGWth8mJycLDw8PoaenJ6ytrcX3339fbD1XptLOg8eOHRMARHJysjRuwIABwtTUVMjlctG8eXOxefPmIsuNjIwU7du3F8bGxqJmzZri448/LnIO/PPPP0XPnj1FjRo1RJ06dcTgwYPFw4cPpekLFy4UDRs2FLq6uqJmzZriX//6lzhw4ECRdTk4OIht27YVu43v6jGPiIjKpzxtJpkQxbyp9T315MkTGBsb4/Hjx8U+loKIiEidPXv2DMnJybCzsyv2OeH0fjl48CAmT56MS5cuSc8up7fL6tWr8fPPPyMyMrLYMiX97/IamMqrpH2G5wmisklISICnpydu3LhRpsewUfU7cOAApkyZgosXL0JLS/kT8nnMIyJSD+VpM7HVTERERKTmfHx8MGrUKIVHc9DbRVtbG6tWrVJ1GEREVA7NmjXDokWLSn23CalOVlYWQkJCik2oEBERKcOzBhERERFhwoQJqg6BSjBy5EhVh0BERBUwaNAgVYdAJSh8lx4REVF5sKcKERERERERERERERFRGTCpQkREREREREREREREVAZMqhAREZFSQghVh0BE5cD/Wapu3OeISB3wWEdERK9jUoWIiIgUaGpqAgByc3NVHAkRlUd2djaAly+1J6pKhftY4T5HRPQ+K7wmLrxGJiIi4ovqiYiISIGWlhb09fVx//59aGtrQ0OD92AQvc2EEMjOzsa9e/dgYmLCH32oymlqasLExAT37t0DAOjr60Mmk6k4KiKiyldQUID79+9DX18fWlr8CY2IiF7iGYGIiIgUyGQyWFhYIDk5GampqaoOh4jKyMTEBHXr1lV1GKQmCve1wsQKEdH7SkNDA/Xr12fymIiIJEyqEBERURFyuRyNGjXiI8CI3hHa2trsoULVqjABX7t2bbx48ULV4RARVRm5XM6e20REpIBJFSIiIlJKQ0MDurq6qg6DiIjeYpqamkzoEREREZFaYaqdiIiIiIiIiIiIiIioDJhUISIiIiIiIiIiIiIiKgMmVYiIiIiIiIiIiIiIiMpA7d6pIoQAADx58kTFkRARERERVY/Ca9/Ca2Gi0rDdRERERETqpDxtJrVLqmRmZgIArK2tVRwJEREREVH1yszMhLGxsarDoHcA201EREREpI7K0maSCTW7Xa2goAB3796FoaEhZDJZta//yZMnsLa2xu3bt2FkZFTt6yfV4z6g3lj/6o31r95Y/+pN1fUvhEBmZiYsLS2hocEnAFPp2G4iVWL9qzfWv3pj/as31r96U3X9l6fNpHY9VTQ0NGBlZaXqMGBkZMSDg5rjPqDeWP/qjfWv3lj/6k2V9c8eKlQebDfR24D1r95Y/+qN9a/eWP/q7V1oM/E2NSIiIiIiIiIiIiIiojJgUoWIiIiIiIiIiIiIiKgMmFSpZjo6OggICICOjo6qQyEV4T6g3lj/6o31r95Y/+qN9U9UPvyfUW+sf/XG+ldvrH/1xvpXb+9S/avdi+qJiIiIiIiIiIiIiIgqgj1ViIiIiIiIiIiIiIiIyoBJFSIiIiIiIiIiIiIiojJgUoWIiIiIiIiIiIiIiKgMmFQhIiIiIiIiIiIiIiIqAyZVqsCaNWtgZ2cHXV1dODs7IyYmpsTy0dHRcHZ2hq6uLho0aIB169ZVU6RUFcpT/3v27EHnzp1hbm4OIyMjuLq64tChQ9UYLVW28v7/Fzp16hS0tLTw4YcfVm2AVOXKuw88f/4cM2bMgI2NDXR0dNCwYUNs3LixmqKlylbe+g8LC0OLFi2gr68PCwsLDBkyBA8fPqymaKmynDhxAt26dYOlpSVkMhn27t1b6jy8/iNiu0ndsd2k3thuUm9sM6k3tpnU1/vUbmJSpZLt2LEDX3/9NWbMmIH4+Hi4ubmha9euSEtLU1o+OTkZPj4+cHNzQ3x8PKZPn47x48cjPDy8miOnylDe+j9x4gQ6d+6MgwcPIi4uDh4eHujWrRvi4+OrOXKqDOWt/0KPHz/GwIED0alTp2qKlKpKRfaBPn364MiRIwgODsa1a9ewbds2ODo6VmPUVFnKW/8nT57EwIEDMWzYMFy+fBm7du1CbGwshg8fXs2R05vKyspCixYt8P3335epPK//iNhuUndsN6k3tpvUG9tM6o1tJvX2XrWbBFWq1q1bi9GjRyuMc3R0FP7+/krLT506VTg6OiqMGzVqlGjbtm2VxUhVp7z1r4yTk5OYPXt2ZYdG1aCi9d+3b18xc+ZMERAQIFq0aFGFEVJVK+8+8OuvvwpjY2Px8OHD6giPqlh563/x4sWiQYMGCuNWrlwprKysqixGqnoARERERIlleP1HxHaTumO7Sb2x3aTe2GZSb2wzUaF3vd3EniqVKDc3F3FxcfDy8lIY7+XlhdOnTyud58yZM0XKe3t74+zZs3jx4kWVxUqVryL1/7qCggJkZmbC1NS0KkKkKlTR+g8JCcHNmzcREBBQ1SFSFavIPrBv3z64uLhg0aJFqFevHhwcHDB58mTk5ORUR8hUiSpS/+3atcOdO3dw8OBBCCHw119/Yffu3fjkk0+qI2RSIV7/kbpju0m9sd2k3thuUm9sM6k3tpmovN7m6z8tla79PfPgwQPk5+ejTp06CuPr1KmDjIwMpfNkZGQoLZ+Xl4cHDx7AwsKiyuKlylWR+n/d0qVLkZWVhT59+lRFiFSFKlL/SUlJ8Pf3R0xMDLS0eDh+11VkH7h16xZOnjwJXV1dRERE4MGDBxg7diz+/vtvPiP4HVOR+m/Xrh3CwsLQt29fPHv2DHl5efD19cWqVauqI2RSIV7/kbpju0m9sd2k3thuUm9sM6k3tpmovN7m6z/2VKkCMplMYVgIUWRcaeWVjad3Q3nrv9C2bdsQGBiIHTt2oHbt2lUVHlWxstZ/fn4++vfvj9mzZ8PBwaG6wqNqUJ5jQEFBAWQyGcLCwtC6dWv4+PggKCgIoaGhvPPqHVWe+r9y5QrGjx+Pb7/9FnFxcfjtt9+QnJyM0aNHV0eopGK8/iNiu0ndsd2k3thuUm9sM6k3tpmoPN7W6z+m+CtRrVq1oKmpWSS7eu/evSJZtUJ169ZVWl5LSwtmZmZVFitVvorUf6EdO3Zg2LBh2LVrFzw9PasyTKoi5a3/zMxMnD17FvHx8Rg3bhyAlxeLQghoaWkhMjISH3/8cbXETpWjIscACwsL1KtXD8bGxtK4Jk2aQAiBO3fuoFGjRlUaM1WeitT/ggUL0L59e0yZMgUA0Lx5cxgYGMDNzQ1z587lXdfvMV7/kbpju0m9sd2k3thuUm9sM6k3tpmovN7m6z/2VKlEcrkczs7OiIqKUhgfFRWFdu3aKZ3H1dW1SPnIyEi4uLhAW1u7ymKlyleR+gde3mk1ePBgbN26lc+EfIeVt/6NjIyQkJCA8+fPS5/Ro0ejcePGOH/+PNq0aVNdoVMlqcgxoH379rh79y6ePn0qjbt+/To0NDRgZWVVpfFS5apI/WdnZ0NDQ/FSTFNTE8D/3X1D7yde/5G6Y7tJvbHdpN7YblJvbDOpN7aZqLze6uu/Sn7xvdrbvn270NbWFsHBweLKlSvi66+/FgYGBiIlJUUIIYS/v78YMGCAVP7WrVtCX19fTJw4UVy5ckUEBwcLbW1tsXv3blVtAr2B8tb/1q1bhZaWlli9erVIT0+XPv/884+qNoHeQHnr/3UBAQGiRYsW1RQtVYXy7gOZmZnCyspK9OrVS1y+fFlER0eLRo0aieHDh6tqE+gNlLf+Q0JChJaWllizZo24efOmOHnypHBxcRGtW7dW1SZQBWVmZor4+HgRHx8vAIigoCARHx8vUlNThRC8/iNShu0m9cZ2k3pju0m9sc2k3thmUm/vU7uJSZUqsHr1amFjYyPkcrlo1aqViI6OlqYNGjRIuLu7K5Q/fvy4aNmypZDL5cLW1lasXbu2miOmylSe+nd3dxcAinwGDRpU/YFTpSjv//+r2Dh4P5R3H0hMTBSenp5CT09PWFlZiUmTJons7OxqjpoqS3nrf+XKlcLJyUno6ekJCwsL4efnJ+7cuVPNUdObOnbsWInnc17/ESnHdpN6Y7tJvbHdpN7YZlJvbDOpr/ep3SQTgn2liIiIiIiIiIiIiIiISsN3qhAREREREREREREREZUBkypERERERERERERERERlwKQKERERERERERERERFRGTCpQkREREREREREREREVAZMqhAREREREREREREREZUBkypERERERERERERERERlwKQKERERERERERERERFRGTCpQkREREREREREREREVAZMqhARvUNCQ0NhYmKi6jAqzNbWFsuXLy+xTGBgID788MNqiYeIiIiIiOh98Xp7SyaTYe/evSqLh4jofcWkChFRNRs8eDBkMlmRz40bN1QdGkJDQxVisrCwQJ8+fZCcnFwpy4+NjcXIkSOlYWUX+ZMnT8aRI0cqZX3FeX0769Spg27duuHy5cvlXs67nOQiIiIiIqLK8Wo7T0tLC/Xr18eYMWPw6NEjVYdGRESVjEkVIiIV6NKlC9LT0xU+dnZ2qg4LAGBkZIT09HTcvXsXW7duxfnz5+Hr64v8/Pw3Xra5uTn09fVLLFOjRg2YmZm98bpK8+p2HjhwAFlZWfjkk0+Qm5tb5esmIiIiIqL3T2E7LyUlBRs2bMD+/fsxduxYVYdFRESVjEkVIiIV0NHRQd26dRU+mpqaCAoKQrNmzWBgYABra2uMHTsWT58+LXY5Fy5cgIeHBwwNDWFkZARnZ2ecPXtWmn769Gl06NABenp6sLa2xvjx45GVlVVibDKZDHXr1oWFhQU8PDwQEBCAS5cuST1p1q5di4YNG0Iul6Nx48bYsmWLwvyBgYGoX78+dHR0YGlpifHjx0vTXu2ObmtrCwDo0aMHZDKZNPzq478OHToEXV1d/PPPPwrrGD9+PNzd3SttO11cXDBx4kSkpqbi2rVrUpmS6uP48eMYMmQIHj9+LN2RFhgYCADIzc3F1KlTUa9ePRgYGKBNmzY4fvx4ifEQEREREdG7rbCdZ2VlBS8vL/Tt2xeRkZHS9JCQEDRp0gS6urpwdHTEmjVrFOa/c+cO+vXrB1NTUxgYGMDFxQW///47AODmzZvo3r076tSpgxo1auCjjz7C4cOHq3X7iIjoJSZViIjeIhoaGli5ciUuXbqETZs24ejRo5g6dWqx5f38/GBlZYXY2FjExcXB398f2traAICEhAR4e3ujZ8+euHjxInbs2IGTJ09i3Lhx5YpJT08PAPDixQtERERgwoQJ+Oabb3Dp0iWMGjUKQ4YMwbFjxwAAu3fvxrJly7B+/XokJSVh7969aNasmdLlxsbGAnjZsEhPT5eGX+Xp6QkTExOEh4dL4/Lz87Fz5074+flV2nb+888/2Lp1KwBI3x9Qcn20a9cOy5cvl3q8pKenY/LkyQCAIUOG4NSpU9i+fTsuXryI3r17o0uXLkhKSipzTERERERE9O66desWfvvtN6l98eOPP2LGjBmYN28eEhMTMX/+fMyaNQubNm0CADx9+hTu7u64e/cu9u3bhwsXLmDq1KkoKCiQpvv4+ODw4cOIj4+Ht7c3unXrhrS0NJVtIxGR2hJERFStBg0aJDQ1NYWBgYH06dWrl9KyO3fuFGZmZtJwSEiIMDY2loYNDQ1FaGio0nkHDBggRo4cqTAuJiZGaGhoiJycHKXzvL7827dvi7Zt2worKyvx/Plz0a5dOzFixAiFeXr37i18fHyEEEIsXbpUODg4iNzcXKXLt7GxEcuWLZOGAYiIiAiFMgEBAaJFixbS8Pjx48XHH38sDR86dEjI5XLx999/v9F2AhAGBgZCX19fABAAhK+vr9LyhUqrDyGEuHHjhpDJZOLPP/9UGN+pUycxbdq0EpdPRERERETvplfbebq6ulIbIygoSAghhLW1tdi6davCPHPmzBGurq5CCCHWr18vDA0NxcOHD8u8TicnJ7Fq1SppuCztLSIienNaKsznEBGpLQ8PD6xdu1YaNjAwAAAcO3YM8+fPx5UrV/DkyRPk5eXh2bNnyMrKksq8atKkSRg+fDi2bNkCT09P9O7dGw0bNgQAxMXF4caNGwgLC5PKCyFQUFCA5ORkNGnSRGlsjx8/Ro0aNSCEQHZ2Nlq1aoU9e/ZALpcjMTFR4UXzANC+fXusWLECANC7d28sX74cDRo0QJcuXeDj44Nu3bpBS6vipxs/Pz+4urri7t27sLS0RFhYGHx8fFCzZs032k5DQ0OcO3cOeXl5iI6OxuLFi7Fu3TqFMuWtDwA4d+4chBBwcHBQGP/8+fNqeVcMERERERGpRmE7Lzs7Gxs2bMD169fx1Vdf4f79+7h9+zaGDRuGESNGSOXz8vJgbGwMADh//jxatmwJU1NTpcvOysrC7Nmz8csvv+Du3bvIy8tDTk4Oe6oQEakAkypERCpgYGAAe3t7hXGpqanw8fHB6NGjMWfOHJiamuLkyZMYNmwYXrx4oXQ5gYGB6N+/Pw4cOIBff/0VAQEB2L59O3r06IGCggKMGjVK4Z0mherXr19sbIXJBg0NDdSpU6dI8kAmkykMCyGkcdbW1rh27RqioqJw+PBhjB07FosXL0Z0dLTCY7XKo3Xr1mjYsCG2b9+OMWPGICIiAiEhIdL0im6nhoaGVAeOjo7IyMhA3759ceLECQAVq4/CeDQ1NREXFwdNTU2FaTVq1CjXthMRERER0bvj1XbeypUr4eHhgdmzZ0uPJv7xxx/Rpk0bhXkK2wyFj10uzpQpU3Do0CEsWbIE9vb20NPTQ69evZCbm1sFW0JERCVhUoWI6C1x9uxZ5OXlYenSpdDQePnKq507d5Y6n4ODAxwcHDBx4kR8/vnnCAkJQY8ePdCqVStcvny5SPKmNK8mG17XpEkTnDx5EgMHDpTGnT59WqE3iJ6eHnx9feHr64svv/wSjo6OSEhIQKtWrYosT1tbG/n5+aXG1L9/f4SFhcHKygoaGhr45JNPpGkV3c7XTZw4EUFBQYiIiECPHj3KVB9yubxI/C1btkR+fj7u3bsHNze3N4qJiIiIiIjeXQEBAejatSvGjBmDevXq4datW9K7IV/XvHlzbNiwAX///bfS3ioxMTEYPHgwevToAeDlO1ZSUlKqMnwiIioGX1RPRPSWaNiwIfLy8rBq1SrcunULW7ZsKfI4qlfl5ORg3LhxOH78OFJTU3Hq1CnExsZKCY7//Oc/OHPmDL788kucP38eSUlJ2LdvH7766qsKxzhlyhSEhoZi3bp1SEpKQlBQEPbs2SO9oD00NBTBwcG4dOmStA16enqwsbFRujxbW1scOXIEGRkZePToUbHr9fPzw7lz5zBv3jz06tULurq60rTK2k4jIyMMHz4cAQEBEEKUqT5sbW3x9OlTHDlyBA8ePEB2djYcHBzg5+eHgQMHYs+ePUhOTkZsbCwWLlyIgwcPlismIiIiIiJ6d3Xs2BFNmzbF/PnzERgYiAULFmDFihW4fv06EhISEBISgqCgIADA559/jrp16+LTTz/FqVOncOvWLYSHh+PMmTMAAHt7e+zZswfnz5/HhQsX0L9/f+kl9kREVL2YVCEiekt8+OGHCAoKwsKFC/HBBx8gLCwMCxYsKLa8pqYmHj58iIEDB8LBwQF9+vRB165dMXv2bAAv73SKjo5GUlIS3Nzc0LJlS8yaNQsWFhYVjvHTTz/FihUrsHjxYjRt2hTr169HSEgIOnbsCAAwMTHBjz/+iPbt26N58+Y4cuQI9u/fX+y7RJYuXYqoqChYW1ujZcuWxa63UaNG+Oijj3Dx4sUid3ZV5nZOmDABiYmJ2LVrV5nqo127dhg9ejT69u0Lc3NzLFq0CAAQEhKCgQMH4ptvvkHjxo3h6+uL33//HdbW1uWOiYiIiIiI3l2TJk3Cjz/+CG9vb2zYsAGhoaFo1qwZ3N3dERoaCjs7OwAve8FHRkaidu3a8PHxQbNmzfDdd99JjwdbtmwZatasiXbt2qFbt27w9vZW+jQAIiKqejIhhFB1EERERERERERERERERG879lQhIiIiIiIiIiIiIiIqAyZViIiIiIiIiIiIiIiIyoBJFSIiIiIiIiIiIiIiojJgUoWIiIiIiIiIiIiIiKgMmFQhIiIiIiIiIiIiIiIqAyZViIiIiIiIiIiIiIiIyoBJFSIiIiIiIiIiIiIiojJgUoWIiIiIiIiIiIiIiKgMmFQhIiIiIiIiIiIiIiIqAyZViIiIiIiIiIiIiIiIyoBJFSIiIiIiIiIiIiIiojL4fyrDTbxvvVRXAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAHFCAYAAADCA+LKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABF8UlEQVR4nO3deVhV5fr/8c8WYQuIOwEBMS2nzClTLMROoTnknE1aJmlRag5FannMUhsOqA1WUmqOZZZZZtMx0tIsjxOanNRssDDzBKKGqIiAuH5/9HN/2wIusL1cSO9X17quWOvez3r2TuL2vp9n4TAMwxAAAICNqtg9AQAAABISAABgOxISAABgOxISAABgOxISAABgOxISAABgOxISAABgOxISAABgOxISAABgOxISnNU333yju+++W/Xr11e1atVUvXp1tWnTRtOmTdPvv/9u6b23bdum2NhYuVwuORwOvfDCC16/h8Ph0OTJk70+rpmFCxfK4XDI4XDoiy++KHbdMAw1atRIDodDHTp0OKd7vPLKK1q4cGG5XvPFF1+UOqfzweFwaOTIkWeN6dChwzl/JoMHD1b16tVN444fP67Jkyef9XP45ptvFB8fr4YNG8rf31/+/v5q3Lixhg4dqi1btnjETp482f3f2+FwqEqVKqpdu7Z69Oih//znPx6xe/bscceV9mfznnvucccAlUVVuyeAimvOnDkaPny4mjRpoocffljNmjVTYWGhtmzZolmzZmnDhg1avny5Zfe/5557lJubqyVLlqhmzZq69NJLvX6PDRs26OKLL/b6uGUVFBSkefPmFfsBu3btWv30008KCgo657FfeeUVhYaGavDgwWV+TZs2bbRhwwY1a9bsnO9rtVdeecXyexw/flxPPPGEJJWY/MyePVsjR45UkyZN9OCDD6p58+ZyOBzatWuX3nrrLV111VXavXu3GjZs6PG6lJQUuVwunTp1Snv37tW0adPUoUMHbdq0SW3atPGIDQoK0sKFCzVx4kRVqfJ/f3c8duyY3nnnHdWoUUNHjhzx/psH7GIAJVi/fr3h4+NjdOvWzThx4kSx6/n5+cYHH3xg6RyqVq1q3H///Zbewy4LFiwwJBn33nuv4e/vb+Tk5HhcHzhwoBETE2M0b97ciI2NPad7lOe1BQUFRmFh4Tndx5skGSNGjLBs/EGDBhmBgYGmcQcOHDAkGZMmTSp2bd26dUaVKlWM3r17G/n5+SW+funSpcb//vc/99eTJk0yJBkHDhzwiPvpp58MScb48ePd59LT091/NiQZK1eu9HjN3LlzDX9/f2PgwIEG/wtHZULLBiVKTEyUw+HQq6++KqfTWey6n5+f+vTp4/761KlTmjZtmi6//HI5nU6FhYXprrvu0r59+zxe16FDB7Vo0UKpqam69tprFRAQoAYNGmjKlCk6deqUpP9rZ5w8eVIzZ870KE2fLn2f6fRr9uzZ4z63evVqdejQQSEhIfL391e9evV0yy236Pjx4+6YksriO3bs0I033qiaNWuqWrVquvLKK/Xaa695xJxubbz11luaMGGCIiMjVaNGDXXu3Fnff/992T5kSXfccYck6a233nKfy8nJ0bJly3TPPfeU+JonnnhC0dHRCg4OVo0aNdSmTRvNmzdPxp9+T+all16qnTt3au3ate7P73SF6fTcFy1apDFjxqhOnTpyOp3avXt3sZbNwYMHVbduXbVv316FhYXu8b/99lsFBgYqLi6uzO/VW0pq2ezbt0+33nqrgoKCdNFFF+nOO+9UamqqHA5HiW2r3bt3q0ePHqpevbrq1q2rMWPGKD8/X9IfLZNatWpJ+uOzPv35na40JSYmysfHR7Nnz5afn1+Jc7ztttsUGRlp+l5cLpckydfXt9i1Jk2aqH379po/f77H+fnz5+vmm292vxaoLEhIUExRUZFWr16tqKgo1a1bt0yvuf/++zVu3Dh16dJFH374oZ566imlpKSoffv2OnjwoEdsZmam7rzzTg0cOFAffvihunfvrvHjx+uNN96QJPXs2VMbNmyQJN16663asGGD++uy2rNnj3r27Ck/Pz/Nnz9fKSkpmjJligIDA1VQUFDq677//nu1b99eO3fu1EsvvaT33ntPzZo10+DBgzVt2rRi8Y8++qh++eUXzZ07V6+++qp+/PFH9e7dW0VFRWWaZ40aNXTrrbd6/NB56623VKVKFfXv37/U9zZ06FAtXbpU7733nm6++WaNGjVKTz31lDtm+fLlatCggVq3bu3+/M5sr40fP1579+7VrFmz9NFHHyksLKzYvUJDQ7VkyRKlpqZq3Lhxkv5oZ9x2222qV6+eZs2aVab3aaXc3Fx17NhRa9as0dSpU7V06VKFh4eX+vkVFhaqT58+6tSpkz744APdc889mj59uqZOnSpJql27tlJSUiRJ8fHx7s/v8ccfV1FRkdasWaO2bduqdu3a5Z5rUVGRTp48qYKCAu3evVsjRoyQ0+nUrbfeWmJ8fHy83n//fWVnZ0v648/n+vXrFR8fX+57AxWe3SUaVDyZmZmGJOP2228vU/yuXbsMScbw4cM9zm/atMmQZDz66KPuc7GxsYYkY9OmTR6xzZo1M2644QaPcyqhfH+69H2m0y2Q9PR0wzAM49133zUkGWlpaWedu84oy99+++2G0+k09u7d6xHXvXt3IyAgwDh8+LBhGIaxZs0aQ5LRo0cPj7ilS5cakowNGzac9b6n55uamuoea8eOHYZhGMZVV11lDB482DAM87ZLUVGRUVhYaDz55JNGSEiIcerUKfe10l57+n7XXXddqdfWrFnjcX7q1KmGJGP58uXGoEGDDH9/f+Obb74563s8FyX9Nz9TbGysx/t6+eWXDUnGJ5984hE3dOhQQ5KxYMEC97lBgwYZkoylS5d6xPbo0cNo0qSJ++vSWjZn+944efKkUVhY6D7+/N/i9J/bM48aNWoY7733nsc4p1s2zzzzjHH06FGjevXqRnJysmEYhvHwww8b9evXN06dOmWMGDGClg0qFSok+MvWrFkjScUWT1599dVq2rSpPv/8c4/zERERuvrqqz3OXXHFFfrll1+8Nqcrr7xSfn5+GjJkiF577TX9/PPPZXrd6tWr1alTp2KVocGDB+v48ePFKjV/bltJf7wPSeV6L7GxsWrYsKHmz5+v7du3KzU1tdR2zek5du7cWS6XSz4+PvL19dXEiRN16NAhZWVllfm+t9xyS5ljH374YfXs2VN33HGHXnvtNc2YMUMtW7Y0fd3Jkyc9DuNPbSVvWbt2rYKCgtStWzeP86fbYWdyOBzq3bu3xzlv/PmLioqSr6+v+3juueeKxXz22WdKTU3V5s2b9fHHH6tz5866/fbbS10cXr16dd12222aP3++Tp48qddff1133303u2tQKZGQoJjQ0FAFBAQoPT29TPGHDh2SpBJL2JGRke7rp4WEhBSLczqdysvLO4fZlqxhw4b67LPPFBYWphEjRqhhw4Zq2LChXnzxxbO+7tChQ6W+j9PX/+zM93J6vU153ovD4dDdd9+tN954Q7NmzdJll12ma6+9tsTYzZs3q2vXrpL+2AX1n//8R6mpqZowYUK571uelsPpNRQnTpxQREREmdaO7Nmzx+MHtK+vr9auXVvme5bVoUOHFB4eXux8SeckKSAgQNWqVfM453Q6deLECdN7hYaGyt/fv8Tk5c0331Rqaqo+/PDDUl/fqlUrtW3bVldddZV69uypd955R40aNdKIESNKfU18fLy+/vpr/etf/9KBAwfKtWsKuJCQkKAYHx8fderUSVu3bi22KLUkp38oZ2RkFLv222+/KTQ01GtzO/2D5PQCxNPOXKciSddee60++ugj5eTkaOPGjYqJiVFCQoKWLFlS6vghISGlvg9JXn0vfzZ48GAdPHhQs2bN0t13311q3JIlS+Tr66uPP/5Y/fr1U/v27dW2bdtzumd5/padkZGhESNG6Morr9ShQ4c0duxY09dERkYqNTXV44iKijqnuZ5NSEiI9u/fX+x8Zmam1+/l4+Oj66+/Xlu2bCn256RZs2Zq27ZtmSpHp1WpUkXNmzdXRkZGqdWta665Rk2aNNGTTz6pLl26lHldF3ChISFBicaPHy/DMHTfffeVuAi0sLBQH330kSTp+uuvlyT3otTTUlNTtWvXLnXq1Mlr8zq9U+Sbb77xOH96LiXx8fFRdHS0Xn75ZUnS119/XWpsp06dtHr1ancCctrrr7+ugIAAtWvX7hxnfnZ16tTRww8/rN69e2vQoEGlxjkcDlWtWlU+Pj7uc3l5eVq0aFGxWG9VnYqKinTHHXfI4XDok08+UVJSkmbMmKH33nvvrK/z8/NT27ZtPY6/8lyV0sTGxuro0aP65JNPPM6fLfE0c7ZK1/jx41VUVKRhw4Z57Dw6F0VFRdq+fbucTqdq1KhRatxjjz2m3r17a8yYMX/pfkBFxoPRUKKYmBjNnDlTw4cPV1RUlO6//341b95chYWF2rZtm1599VW1aNFCvXv3VpMmTTRkyBDNmDFDVapUUffu3bVnzx49/vjjqlu3rh566CGvzatHjx4KDg5WfHy8nnzySVWtWlULFy7Ur7/+6hE3a9YsrV69Wj179lS9evV04sQJ906Wzp07lzr+pEmT9PHHH6tjx46aOHGigoODtXjxYv373//WtGnTLN1qOWXKFNOYnj176vnnn9eAAQM0ZMgQHTp0SM8++2yJW7NbtmypJUuW6O2331aDBg1UrVq1cv3t/bRJkybpq6++0sqVKxUREaExY8Zo7dq1io+PV+vWrVW/fv1yj3k2P/30k959991i55s1a1biA9sGDRqk6dOna+DAgXr66afVqFEjffLJJ/r0008lyeOhYmUVFBSkSy65RB988IE6deqk4OBghYaG6tJLL9U111yjl19+WaNGjVKbNm00ZMgQNW/eXFWqVFFGRoaWLVsmSSUmGFu3bnX/Gdq/f7/mz5+v7777Tg899FCxNtKfDRw4UAMHDiz3+wAuKHavqkXFlpaWZgwaNMioV6+e4efnZwQGBhqtW7c2Jk6caGRlZbnjioqKjKlTpxqXXXaZ4evra4SGhhoDBw40fv31V4/xYmNjjebNmxe7z6BBg4xLLrnE45xK2XGxefNmo3379kZgYKBRp04dY9KkScbcuXM9dtls2LDBuOmmm4xLLrnEcDqdRkhIiBEbG2t8+OGHxe5x5k6K7du3G7179zZcLpfh5+dntGrVymOnhmH8326Ud955x+P86R0SZ8af6c+7bM6mpJ0y8+fPN5o0aWI4nU6jQYMGRlJSkjFv3jyP928YhrFnzx6ja9euRlBQkCHJ/fmWNvc/Xzu9y2blypVGlSpVin1Ghw4dMurVq2dcddVVpT4c7FyohJ0op4/Tczhzl41hGMbevXuNm2++2ahevboRFBRk3HLLLcaKFSsMSR4P8CvtwWgl7d767LPPjNatWxtOp9OQZAwaNMjjelpamnH33Xcb9evXN5xOp1GtWjWjUaNGxl133WV8/vnnJY7/5yM4ONiIjo425s+fbxQVFblj/7zL5mzYZYPKxmEYFix7BwCbJSYm6rHHHtPevXtt/fUAAMqGlg2AC15ycrIk6fLLL1dhYaFWr16tl156SQMHDiQZAS4QJCQALngBAQGaPn269uzZo/z8fNWrV0/jxo3TY489ZvfUAJQRLRsAAGA7tv0CAADbkZAAAADbkZAAAADbkZAAAADbVcpdNkdOnLJ7CkCFdLKINezAmYIDfcyD/iL/1iO9Mk7etmSvjFMRUSEBAAC2q5QVEgAAKhQHf/83Q0ICAIDVHA67Z1DhkZAAAGA1KiSm+IQAAIDtqJAAAGA1WjamSEgAALAaLRtTfEIAAMB2VEgAALAaLRtTJCQAAFiNlo0pPiEAAGA7KiQAAFiNlo0pEhIAAKxGy8YUnxAAALAdFRIAAKxGy8YUCQkAAFajZWOKhAQAAKtRITFFygYAAGxHhQQAAKvRsjFFQgIAgNVISEzxCQEAANtRIQEAwGpVWNRqhgoJAABWc1TxzlEOkydPlsPh8DgiIiLc1w3D0OTJkxUZGSl/f3916NBBO3fu9BgjPz9fo0aNUmhoqAIDA9WnTx/t27fPIyY7O1txcXFyuVxyuVyKi4vT4cOHy/0RkZAAAFBJNW/eXBkZGe5j+/bt7mvTpk3T888/r+TkZKWmpioiIkJdunTR0aNH3TEJCQlavny5lixZonXr1unYsWPq1auXioqK3DEDBgxQWlqaUlJSlJKSorS0NMXFxZV7rrRsAACwmk3PIalatapHVeQ0wzD0wgsvaMKECbr55pslSa+99prCw8P15ptvaujQocrJydG8efO0aNEide7cWZL0xhtvqG7duvrss890ww03aNeuXUpJSdHGjRsVHR0tSZozZ45iYmL0/fffq0mTJmWeKxUSAACs5qWWTX5+vo4cOeJx5Ofnl3rbH3/8UZGRkapfv75uv/12/fzzz5Kk9PR0ZWZmqmvXru5Yp9Op2NhYrV+/XpK0detWFRYWesRERkaqRYsW7pgNGzbI5XK5kxFJateunVwulzumrEhIAAC4QCQlJbnXapw+kpKSSoyNjo7W66+/rk8//VRz5sxRZmam2rdvr0OHDikzM1OSFB4e7vGa8PBw97XMzEz5+fmpZs2aZ40JCwsrdu+wsDB3TFnRsgEAwGpeatmMHz9eo0eP9jjndDpLjO3evbv731u2bKmYmBg1bNhQr732mtq1a/f/p+U5L8Mwip0705kxJcWXZZwzUSEBAMBqXmrZOJ1O1ahRw+MoLSE5U2BgoFq2bKkff/zRva7kzCpGVlaWu2oSERGhgoICZWdnnzVm//79xe514MCBYtUXMyQkAABYzeHwzvEX5Ofna9euXapdu7bq16+viIgIrVq1yn29oKBAa9euVfv27SVJUVFR8vX19YjJyMjQjh073DExMTHKycnR5s2b3TGbNm1STk6OO6asaNkAAFAJjR07Vr1791a9evWUlZWlp59+WkeOHNGgQYPkcDiUkJCgxMRENW7cWI0bN1ZiYqICAgI0YMAASZLL5VJ8fLzGjBmjkJAQBQcHa+zYsWrZsqV7103Tpk3VrVs33XfffZo9e7YkaciQIerVq1e5dthIJCQAAFjPht9ls2/fPt1xxx06ePCgatWqpXbt2mnjxo265JJLJEmPPPKI8vLyNHz4cGVnZys6OlorV65UUFCQe4zp06eratWq6tevn/Ly8tSpUyctXLhQPj4+7pjFixfrgQcecO/G6dOnj5KTk8s9X4dhGMZffM8VzpETp+yeAlAhnSyqdN/uwF8WHOhjHvQX+Xef7pVx8j55yCvjVESsIQEAALajZQMAgNVsaNlcaEhIAACwmk2Pjr+QkLIBAADbUSEBAMBqtGxMkZAAAGA1EhJTfEIAAMB2VEgAALAai1pNkZAAAGA1WjamSEgAALAaFRJTpGwAAMB2VEgAALAaLRtTJCQAAFiNlo0pUjYAAGA7KiQAAFjMQYXEFAkJAAAWIyExR8sGAADYjgoJAABWo0BiioQEAACL0bIxR8sGAADYjgoJAAAWo0JijoQEAACLkZCYIyEBAMBiJCTmWEMCAABsR4UEAACrUSAxRUICAIDFaNmYo2UDAABsR4UEAACLUSExR0ICAIDFSEjM0bIBAAC2o0ICAIDFqJCYIyEBAMBq5COmaNkAAADbUSEBAMBitGzMkZAAAGAxEhJzJCQAAFiMhMQca0gAAIDtqJAAAGA1CiSmSEgAALAYLRtztGwAAIDtqJAAAGAxKiTmSEgAALAYCYk5WjYAAMB2VEgAALAYFRJzJCQAAFiNfMQULRsAAGA7KiQAAFiMlo05EhIAACxGQmKOhAQAAIuRkJhjDQkAALAdFRIAAKxGgcQUCQkAABajZWOOlg0AALAdFRKUy4J5r2rN56v0S/rPcjqr6YorW2tkwhhdeml9d4xhGJoz62UtX7ZUR48cUfOWV+iR8Y+rYaPG7pih8Xfp6y2pHmN3uaG7Eqc9f97eC+BN772zRO+9s0QZGf+TJDVo0Ej3DLlfMddcJ0n64vNVen/ZUn333U7lHD6s195apsuaNPUYo6CgQDOmT9OqT1co/0S+2l7dTg+Pf1xh4RHn/f3Au6iQmKNCgnL5ekuqbus/QPMXLVHy7HkqOnlSo4bFK+/4cXfM6wvm6s1FC/XwPx/TwsVLFRISqpHD4pWbm+sxVt9bbtMnn3/pPh59/Inz/XYAr6kVFq7hDzykBW+8owVvvKOoq6L1yEMj9fNPP0qS8vLy1PLK1ho+anSpY7zwbJLWrvlcTyY9q1nzFynv+HGNffB+FRUVna+3AYs4HA6vHJUZFRKUy4yZczy+nvhkorp2vEa7du1Um6irZBiG3lr8uu6+d6iu79xVkjT56Sm64fp/6NMVH+vm2/q7X1utWjWFhtY6r/MHrHJtbEePr4eNTNB77y7Rju3fqEHDxureq48kKeO3/5X4+mNHj+qj95dp0lNTdXV0e0nSpH9NVd/u1yt10wa1a/8Pa98AYDNbKyT79u3ThAkT1LFjRzVt2lTNmjVTx44dNWHCBP366692Tg1ldOzYUUlSjRouSdL//rdPhw4eVLuYa9wxfn5+ahN1lb757zaP16as+FidY2PU76ZeeuG5acUqKMCFqqioSKs+XaETeXlqeUWrMr3mu107dfLkSV0d0959rlatMDVo2Fjbz/jewYWHCok52yok69atU/fu3VW3bl117dpVXbt2lWEYysrK0vvvv68ZM2bok08+0TXXXGM+GGxhGIamPztVV7aOUqPGl0mSDh08KEkKDgn1iA0OCVHmb7+5v+7Wo5ci61yskJBQ/bz7R7380nT9+MN3enn2/PP3BgAv2/3jDxoy+A4VFBTI3z9AU557SfUbNCrTaw8dOihfX193cn9acEiIDh06aMV0cT5V7lzCK2yrkDz00EO699579e233+qFF17Q+PHj9eijj+qFF17Qzp07FR8fr4SEBNNx8vPzdeTIEY8jPz/f+jcATUt6Srt//F5PT3222LUzE3nDMDxO3nRLP0W3a69GjS9T1+49NeW5F7V54wZ9t2un1dMGLHPJpZfqtbfe05zX3tJNt/XXUxMfVfrPu//SmIZhVPq/GeP8SEpKksPh8PjZahiGJk+erMjISPn7+6tDhw7audPz/8P5+fkaNWqUQkNDFRgYqD59+mjfvn0eMdnZ2YqLi5PL5ZLL5VJcXJwOHz5crvnZlpDs2LFDw4YNK/X60KFDtWPHDtNxkpKS3B/A6eP5Z6Z4c6oowTNJT+vLL9Zo5pzXFP6nHQAhoX9URk5XSk7L/v13hYSElDre5U2bqWpVX+395RdrJgycB76+fqpb7xI1bdZCw0eNVqPLmujtNxeV6bUhIaEqLCzUkSM5Huezf/9dwcGlf+/gwmB3yyY1NVWvvvqqrrjiCo/z06ZN0/PPP6/k5GSlpqYqIiJCXbp00dGjR90xCQkJWr58uZYsWaJ169bp2LFj6tWrl8di6wEDBigtLU0pKSlKSUlRWlqa4uLiyjVH2xKS2rVra/369aVe37Bhg2rXrm06zvjx45WTk+NxjH74n96cKv7EMAxNS3xKaz5fpZlzFqjOxRd7XK9T52KFhIZq08b/+29bWFigr7em6opWrUsd96fdP+rkyUKF1mKRKyoPwzBUWFhYptjLmzZX1apVtflP3zsHDxzQzz/9qJZn+d7BhcHOhOTYsWO68847NWfOHNWsWdN93jAMvfDCC5owYYJuvvlmtWjRQq+99pqOHz+uN998U5KUk5OjefPm6bnnnlPnzp3VunVrvfHGG9q+fbs+++wzSdKuXbuUkpKiuXPnKiYmRjExMZozZ44+/vhjff/992Wep21rSMaOHathw4Zp69at6tKli8LDw+VwOJSZmalVq1Zp7ty5euGFF0zHcTqdcjqdHueOnDhl0awxNfFJffrJv/XsC8kKCAzUwYMHJEnVqwepWrVqcjgcuuPOu7Rg3quqW+8S1a13iRbOe1XVqlXTDT16SZL2/bpXn/z7I11zbawuuqim0n/erReem6YmlzdVqyvb2Pn2gHM2c8Z0xVxzrcIjais3N1effbpC27amanryq5KknJzD2p+ZoYMHsiRJe/fskfRHZSQktJaqBwWpd99bNGP6M3K5LlINl0szpj+jho0a66roGLveFrzEW123/Pz8YssSSvo5+GcjRoxQz5491blzZz399NPu8+np6crMzFTXrl09xoqNjdX69es1dOhQbd26VYWFhR4xkZGRatGihdavX68bbrhBGzZskMvlUnR0tDumXbt2crlcWr9+vZo0aVKm92ZbQjJ8+HCFhIRo+vTpmj17trv04+Pjo6ioKL3++uvq16+fXdNDKZYtXSJJGhY/yOP8xCcT1fvGmyRJd919r/Lz8zU18Un3g9FmzJyrwMBASVJVX1+lbt6ot99cpOPHjys8orauuTZW9w0bLh8fn/P7hgAv+f33Q3ri8X/q0MEDql49SA0bX6bpya/q6nZ/7JpZt3aNnp48wR3/+PgxkqT4IcN177CRkqQHx/xTPj4+euyfo5Wfn6+2V7XT408k8n0Bt6SkJD3xhOczmyZNmqTJkyeXGL9kyRJ9/fXXSk1NLXYtMzNTkhQeHu5xPjw8XL/8//Z5Zmam/Pz8PCorp2NOvz4zM1NhYWHFxg8LC3PHlIWtzyHp37+/+vfvr8LCQh38/2sOQkND5evra+e0cBap/91lGuNwODTk/pEacv/IEq9HRNTWq/PL1lcHLhQTJj191us9+9yknn1uOmuM0+nUmHGPacy4x7w5NVQA3lqYPH78eI0e7flwvdKqI7/++qsefPBBrVy5UtWqVSvz3MqykPrMmJLiy7sgu0I8GM3X17dM60UAALgQeatlY9ae+bOtW7cqKytLUVFR7nNFRUX68ssvlZyc7F7fkZmZ6fEzOCsry101iYiIUEFBgbKzsz2qJFlZWWrfvr07Zv/+/cXuf+DAgWLVl7Ph0fEAAFRCnTp10vbt25WWluY+2rZtqzvvvFNpaWlq0KCBIiIitGrVKvdrCgoKtHbtWneyERUVJV9fX4+YjIwM7dixwx0TExOjnJwcbd682R2zadMm5eTkuGPKokJUSAAAqMzseJZMUFCQWrRo4XEuMDBQISEh7vMJCQlKTExU48aN1bhxYyUmJiogIEADBgyQJLlcLsXHx2vMmDEKCQlRcHCwxo4dq5YtW6pz586SpKZNm6pbt2667777NHv2bEnSkCFD1KtXrzIvaJVISAAAsFxFfbbdI488ory8PA0fPlzZ2dmKjo7WypUrFRQU5I6ZPn26qlatqn79+ikvL0+dOnXSwoULPRZbL168WA888IB7N06fPn2UnJxcrrk4DMMwvPO2Kg62/QIlO1lU6b7dgb8sOND6XUyX//NTr4zz3ZQbvDJORUSFBAAAi1WpUkFLJBUICQkAABarqC2bioRdNgAAwHZUSAAAsBi/sdkcCQkAABYjHzFHQgIAgMWokJhjDQkAALAdFRIAACxGhcQcCQkAABYjHzFHywYAANiOCgkAABajZWOOhAQAAIuRj5ijZQMAAGxHhQQAAIvRsjFHQgIAgMXIR8zRsgEAALajQgIAgMVo2ZgjIQEAwGLkI+ZISAAAsBgVEnOsIQEAALajQgIAgMUokJgjIQEAwGK0bMzRsgEAALajQgIAgMUokJgjIQEAwGK0bMzRsgEAALajQgIAgMUokJgjIQEAwGK0bMzRsgEAALajQgIAgMWokJgjIQEAwGLkI+ZISAAAsBgVEnOsIQEAALajQgIAgMUokJgjIQEAwGK0bMzRsgEAALajQgIAgMUokJgjIQEAwGJVyEhM0bIBAAC2o0ICAIDFKJCYIyEBAMBi7LIxR0ICAIDFqpCPmGINCQAAsB0VEgAALEbLxhwJCQAAFiMfMUfLBgAA2I4KCQAAFnOIEokZEhIAACzGLhtztGwAAIDtqJAAAGAxdtmYIyEBAMBi5CPmaNkAAADbUSEBAMBiVSiRmCIhAQDAYuQj5khIAACwGItazbGGBAAA2I4KCQAAFqNAYo6EBAAAi7Go1RwtGwAAKqGZM2fqiiuuUI0aNVSjRg3FxMTok08+cV83DEOTJ09WZGSk/P391aFDB+3cudNjjPz8fI0aNUqhoaEKDAxUnz59tG/fPo+Y7OxsxcXFyeVyyeVyKS4uTocPHy73fElIAACwmMNLR3lcfPHFmjJlirZs2aItW7bo+uuv14033uhOOqZNm6bnn39eycnJSk1NVUREhLp06aKjR4+6x0hISNDy5cu1ZMkSrVu3TseOHVOvXr1UVFTkjhkwYIDS0tKUkpKilJQUpaWlKS4urvyfkWEYRrlfVcEdOXHK7ikAFdLJokr37Q78ZcGBPpbf447X07wyzlt3XfmXXh8cHKxnnnlG99xzjyIjI5WQkKBx48ZJ+qMaEh4erqlTp2ro0KHKyclRrVq1tGjRIvXv31+S9Ntvv6lu3bpasWKFbrjhBu3atUvNmjXTxo0bFR0dLUnauHGjYmJi9N1336lJkyZlnhsVEgAALhD5+fk6cuSIx5Gfn2/6uqKiIi1ZskS5ubmKiYlRenq6MjMz1bVrV3eM0+lUbGys1q9fL0naunWrCgsLPWIiIyPVokULd8yGDRvkcrncyYgktWvXTi6Xyx1TViQkAABYrIrDO0dSUpJ7rcbpIykpqdT7bt++XdWrV5fT6dSwYcO0fPlyNWvWTJmZmZKk8PBwj/jw8HD3tczMTPn5+almzZpnjQkLCyt237CwMHdMWZVpl82HH35Y5gH79OlTrgkAAFDZeevBaOPHj9fo0aM9zjmdzlLjmzRporS0NB0+fFjLli3ToEGDtHbt2lLnZRiG6VzPjCkpvizjnKlMCUnfvn3LNJjD4fBY6AIAALzH6XSeNQE5k5+fnxo1aiRJatu2rVJTU/Xiiy+6141kZmaqdu3a7visrCx31SQiIkIFBQXKzs72qJJkZWWpffv27pj9+/cXu++BAweKVV/MlKllc+rUqTIdJCMAABTncHjn+KsMw1B+fr7q16+viIgIrVq1yn2toKBAa9eudScbUVFR8vX19YjJyMjQjh073DExMTHKycnR5s2b3TGbNm1STk6OO6aseDAaAAAWs+N32Tz66KPq3r276tatq6NHj2rJkiX64osvlJKSIofDoYSEBCUmJqpx48Zq3LixEhMTFRAQoAEDBkiSXC6X4uPjNWbMGIWEhCg4OFhjx45Vy5Yt1blzZ0lS06ZN1a1bN913332aPXu2JGnIkCHq1atXuXbYSOeYkOTm5mrt2rXau3evCgoKPK498MAD5zIkAACVVhUbHtS6f/9+xcXFKSMjQy6XS1dccYVSUlLUpUsXSdIjjzyivLw8DR8+XNnZ2YqOjtbKlSsVFBTkHmP69OmqWrWq+vXrp7y8PHXq1EkLFy6Uj8//bZVevHixHnjgAfdunD59+ig5Obnc8y33c0i2bdumHj166Pjx48rNzVVwcLAOHjyogIAAhYWF6eeffy73JLyN55AAJeM5JEBx5+M5JIPf+sYr4yy84wqvjFMRlXvb70MPPaTevXvr999/l7+/vzZu3KhffvlFUVFRevbZZ62YIwAAFzSHw+GVozIrd0KSlpamMWPGyMfHRz4+PsrPz1fdunU1bdo0Pfroo1bMEQCAC5odj46/0JQ7IfH19XVnaeHh4dq7d6+kPxa/nP53AACA8ij3otbWrVtry5Ytuuyyy9SxY0dNnDhRBw8e1KJFi9SyZUsr5ggAwAWtSiVvt3hDuSskiYmJ7oeoPPXUUwoJCdH999+vrKwsvfrqq16fIAAAF7qK8hySiqzcFZK2bdu6/71WrVpasWKFVycEAAD+fngwGgAAFqvsO2S8odwJSf369c/6wVaE55AAAFCRkI+YK3dCkpCQ4PF1YWGhtm3bppSUFD388MPemhcAAPgbKXdC8uCDD5Z4/uWXX9aWLVv+8oQAAKhs2GVjrty7bErTvXt3LVu2zFvDAQBQabDLxpzXFrW+++67Cg4O9tZwAABUGixqNXdOD0b78wdrGIYyMzN14MABvfLKK16dHAAA+Hsod0Jy4403eiQkVapUUa1atdShQwddfvnlXp3cufKr6rVOFFCphMeMtHsKQIWTty3Z8nvwU8lcuROSyZMnWzANAAAqL1o25sqdtPn4+CgrK6vY+UOHDsnHx8crkwIAAH8v5a6QGIZR4vn8/Hz5+fn95QkBAFDZVKFAYqrMCclLL70k6Y+y09y5c1W9enX3taKiIn355ZcVZg0JAAAVCQmJuTInJNOnT5f0R4Vk1qxZHu0ZPz8/XXrppZo1a5b3ZwgAACq9Mick6enpkqSOHTvqvffeU82aNS2bFAAAlQmLWs2Vew3JmjVrrJgHAACVFi0bc+XeZXPrrbdqypQpxc4/88wzuu2227wyKQAA8PdS7oRk7dq16tmzZ7Hz3bp105dffumVSQEAUJnwu2zMlbtlc+zYsRK39/r6+urIkSNemRQAAJUJv+3XXLkrJC1atNDbb79d7PySJUvUrFkzr0wKAIDKpIqXjsqs3BWSxx9/XLfccot++uknXX/99ZKkzz//XG+++abeffddr08QAABUfuVOSPr06aP3339fiYmJevfdd+Xv769WrVpp9erVqlGjhhVzBADggkbHxly5ExJJ6tmzp3th6+HDh7V48WIlJCTov//9r4qKirw6QQAALnSsITF3zi2p1atXa+DAgYqMjFRycrJ69OihLVu2eHNuAADgb6JcFZJ9+/Zp4cKFmj9/vnJzc9WvXz8VFhZq2bJlLGgFAKAUFEjMlblC0qNHDzVr1kzffvutZsyYod9++00zZsywcm4AAFQKVRzeOSqzMldIVq5cqQceeED333+/GjdubOWcAADA30yZKyRfffWVjh49qrZt2yo6OlrJyck6cOCAlXMDAKBSqOJweOWozMqckMTExGjOnDnKyMjQ0KFDtWTJEtWpU0enTp3SqlWrdPToUSvnCQDABYtHx5sr9y6bgIAA3XPPPVq3bp22b9+uMWPGaMqUKQoLC1OfPn2smCMAAKjk/tKTaJs0aaJp06Zp3759euutt7w1JwAAKhUWtZo7pwejncnHx0d9+/ZV3759vTEcAACVikOVPJvwAq8kJAAAoHSVvbrhDZX9lwcCAIALABUSAAAsRoXEHAkJAAAWc1T2PbteQMsGAADYjgoJAAAWo2VjjoQEAACL0bExR8sGAADYjgoJAAAWq+y/GM8bSEgAALAYa0jM0bIBAAC2o0ICAIDF6NiYIyEBAMBiVfjleqZISAAAsBgVEnOsIQEAALajQgIAgMXYZWOOhAQAAIvxHBJztGwAAIDtqJAAAGAxCiTmSEgAALAYLRtztGwAAIDtSEgAALCYw+GdozySkpJ01VVXKSgoSGFhYerbt6++//57jxjDMDR58mRFRkbK399fHTp00M6dOz1i8vPzNWrUKIWGhiowMFB9+vTRvn37PGKys7MVFxcnl8sll8uluLg4HT58uFzzJSEBAMBiVbx0lMfatWs1YsQIbdy4UatWrdLJkyfVtWtX5ebmumOmTZum559/XsnJyUpNTVVERIS6dOmio0ePumMSEhK0fPlyLVmyROvWrdOxY8fUq1cvFRUVuWMGDBigtLQ0paSkKCUlRWlpaYqLiyvXfB2GYRjlfI8V3omTds8AqJhqXjXS7ikAFU7etmTL77Ewda9Xxhl8Vb1zfu2BAwcUFhamtWvX6rrrrpNhGIqMjFRCQoLGjRsn6Y9qSHh4uKZOnaqhQ4cqJydHtWrV0qJFi9S/f39J0m+//aa6detqxYoVuuGGG7Rr1y41a9ZMGzduVHR0tCRp48aNiomJ0XfffacmTZqUaX5USAAAsJjD4fDKkZ+fryNHjngc+fn5ZZpDTk6OJCk4OFiSlJ6erszMTHXt2tUd43Q6FRsbq/Xr10uStm7dqsLCQo+YyMhItWjRwh2zYcMGuVwudzIiSe3atZPL5XLHlAUJCQAAFnN46UhKSnKv0zh9JCUlmd7fMAyNHj1a//jHP9SiRQtJUmZmpiQpPDzcIzY8PNx9LTMzU35+fqpZs+ZZY8LCwordMywszB1TFmz7BQDAYt7a9jt+/HiNHj3a45zT6TR93ciRI/XNN99o3bp1xa45zpibYRjFzp3pzJiS4ssyzp9RIQEA4ALhdDpVo0YNj8MsIRk1apQ+/PBDrVmzRhdffLH7fEREhCQVq2JkZWW5qyYREREqKChQdnb2WWP2799f7L4HDhwoVn05GxISAAAs5q2WTXkYhqGRI0fqvffe0+rVq1W/fn2P6/Xr11dERIRWrVrlPldQUKC1a9eqffv2kqSoqCj5+vp6xGRkZGjHjh3umJiYGOXk5Gjz5s3umE2bNiknJ8cdUxa0bAAAsJgdD2odMWKE3nzzTX3wwQcKCgpyV0JcLpf8/f3lcDiUkJCgxMRENW7cWI0bN1ZiYqICAgI0YMAAd2x8fLzGjBmjkJAQBQcHa+zYsWrZsqU6d+4sSWratKm6deum++67T7Nnz5YkDRkyRL169SrzDhuJhAQAgEpp5syZkqQOHTp4nF+wYIEGDx4sSXrkkUeUl5en4cOHKzs7W9HR0Vq5cqWCgoLc8dOnT1fVqlXVr18/5eXlqVOnTlq4cKF8fHzcMYsXL9YDDzzg3o3Tp08fJSeXbzs1zyEB/kZ4DglQ3Pl4Dslb2/7nlXHuaF3HK+NURFRIAACwGAs2zfEZAQAA21EhAQDAYuV5HsffFQkJAAAWIx0xR8sGAADYjgoJAAAWo2VjjoQEAACL0Y4wR0ICAIDFqJCYI2kDAAC2o0ICAIDFqI+YIyEBAMBidGzM0bIBAAC2o0ICAIDFqtC0MUVCAgCAxWjZmKNlAwAAbEeFBAAAizlo2ZgiIQEAwGK0bMzRsgEAALajQgIAgMXYZWOOhAQAAIvRsjFHQgIAgMVISMyxhgQAANiOCgkAABZj2685EhIAACxWhXzEFC0bAABgOyokAABYjJaNORISAAAsxi4bc7RsAACA7aiQAABgMVo25khIAACwGLtszNGyAQAAtqNCgr9s6ZI3tfTtt/Tb//4nSWrYqLGG3j9c/7g2VpI08+UZSvnk38rMzJSvr6+aNWuukQ8+pCuuaGXntIFzNmFoDz02rIfHucyDR1S/y6Pu67fd0EYXR9RUQWGRtu3aq8nJHyl1xy/u+PoXh2rKQzcppnUDOX2ratX6XRo99R1l/X7UHfPOC0PV6rI6qhUcpOwjx7Vm0/d67KUPlHEg5/y8UXgNLRtzDsMwDLsn4W0nTto9g7+XL9aslo+Pj+rWqydJ+uiD97Vw/jy9vWy5GjVqrBUff6TgkBBdfHFdncg/oTdeX6hVn6boo09WKTg42ObZ/73UvGqk3VOoFCYM7aGbOl+pnsNmuM8VnTJ0MPuYJKl/t7bKyj6q9H0H5e/01aiB1+vmzq3V4sYndDD7mAKq+Sl16Xht/+F/emrWCknSpOE9VbuWS9fd9ZxO/2951J0dtembdGUezFFk2EVKeugmSVLHwc+f53dcueVtS7b8Hut+zPbKOP9oXNMr41REJCSwxLUxV+uhsQ/r5ltuK3bt2LFjuiY6Sq/OW6jodjE2zO7vi4TEOyYM7aHeHa9Qu9unlCk+KLCastY9q+5DX9IXm39Qp3aX64Pk4aod+4iO5p6QJF0U5K+ML59Rj2EztGbT9yWO0zO2pZY+f59c0Qk6efKU197P3935SEj+46WE5JpKnJCwhgReVVRUpE9W/Ft5ecfVqlXrYtcLCwq07J23FRQUpMuaNLFhhoB3NKpXSz+v/Jd2fTxZr0+5W5fWCSkxzreqj+JvvkaHjx7X9h/+aGs6/arKMAzlF/zf355OFJxUUdEptb+yYYnj1KwRoNu7t9XG/6aTjKBSqtBrSH799VdNmjRJ8+fPLzUmPz9f+fn5HucMH6ecTqfV08Of/PjD94obcLsKCvIVEBCg6S+9rIaNGrmvr/1ijcaNHa0TJ/IUWquWZs2Zr5o1adfgwpS6Y4/ufXyRfvwlS2EhQfrnvd20ZuEYRd36L/2ekytJ6n5tC70+5W4FVPNV5sEj6jUsWYcO/3Ft8/Y9ys0r0L8evFETkz+UQw7968Eb5eNTRRGhNTzu9fQDN2rY7dcp0N+pTd+k6+YHZp3394u/rgpPRjNVoVs2//3vf9WmTRsVFRWVGjN58mQ98cQTHucmPD5Jj02cbPHs8GeFBQXKyMjQ0aNH9NmqlVq+7B3NW/iGOyk5fvy4Dh44oMOHs7Xs3aXavGmj3njrHYWElPy3SliDlo01Aqr5aedHkzX9tc/00hur3eciatVQ6EXVdffN7dXhqst0XdyzOvD/15l0ane5Xnq0vy6tE6JTpwwtTdmqyxtEKHXHHiUkLXWPHXJRoGrWCFS92sGaMLS7co7lkZR42flo2Wzcfdgr47RrdJFXxqmIbE1IPvzww7Ne//nnnzVmzJizJiRUSCqmIfGDdXHdepo4+ckSr/fu3lV9b75F8fcNPc8z+3sjIbHOxzNH6qdfD+jBxLdLvL79g4l67YONenb+So/zIRcF6uTJU8o5lqf0VYl6adHnmv765yWOUSfsIu3+9Gl1GPScNn2T7vX38HdFQlIx2Nqy6du3rxwOh86WEzlMylxOZ/Hkg0Wt9jMMQ4UFBWe9XnCW68CFxM+3qi6vH67/bNtdaoxDDjl9i/8v93QbJ/aqyxQWXF0fr91e+hiO/7sfLjB0bEzZ+qe6du3aevnll9W3b98Sr6elpSkqKur8Tgrl9tILz+sf116n8IgIHc/NVconK7QldbNemT1Xx48f19xXZ6lDx+sVWquWcg4f1ttL3tT+/ZnqckM3u6cOnJOkh27Sv7/crl8zshUWXF3j7u2moMBqWvzRJgVU89O4e2/Qv9duV+bBHAW7AjWk33WqE36R3lv1tXuMuD7t9H16pg5kH1P0FfX17MO3asbiNfrxlyxJUtvml6hti0u0fttPOnz0uC6tE6qJ9/fUT3sPUB25APEcEnO2JiRRUVH6+uuvS01IzKonqBgOHTqoCf98RAcOZKl6UJAuu6yJXpk9VzHtr1F+fr7S03/Whx8s1+HsbF100UVq3qKlFry+WI0aNbZ76sA5qRN+kV5PulshFwXqYPYxbd6+R7GDntPejGw5/aqqyaXhGtg7WiEXBer3nOPasvMXdb5nunb9nOke47JLw/TkqD4KdgXol99+17R5n7rXn0hSXn6hbry+lR4b1lOB/n7KPJijlet36a5/LlBBIWVgVD62riH56quvlJubq27dSv6bcm5urrZs2aLY2NhyjUvLBigZa0iA4s7HGpLNP3vn6bpXN3B5ZZyKyNYKybXXXnvW64GBgeVORgAAqGho2JjjwWgAAMB2LNUGAMBqlEhMkZAAAGAxdtmYIyEBAMBiPDneHGtIAACA7aiQAABgMQok5khIAACwGhmJKVo2AADAdlRIAACwGLtszJGQAABgMXbZmKNlAwAAbEeFBAAAi1EgMUdCAgCA1chITNGyAQAAtqNCAgCAxdhlY44KCQAAFnM4vHOU15dffqnevXsrMjJSDodD77//vsd1wzA0efJkRUZGyt/fXx06dNDOnTs9YvLz8zVq1CiFhoYqMDBQffr00b59+zxisrOzFRcXJ5fLJZfLpbi4OB0+fLhccyUhAQDAYg4vHeWVm5urVq1aKTk5ucTr06ZN0/PPP6/k5GSlpqYqIiJCXbp00dGjR90xCQkJWr58uZYsWaJ169bp2LFj6tWrl4qKitwxAwYMUFpamlJSUpSSkqK0tDTFxcWVa64OwzCMc3iPFdqJk3bPAKiYal410u4pABVO3raSf1h70459x7wyTouLq5/zax0Oh5YvX66+fftK+qM6EhkZqYSEBI0bN07SH9WQ8PBwTZ06VUOHDlVOTo5q1aqlRYsWqX///pKk3377TXXr1tWKFSt0ww03aNeuXWrWrJk2btyo6OhoSdLGjRsVExOj7777Tk2aNCnT/KiQAABgNS+VSPLz83XkyBGPIz8//5ymlJ6erszMTHXt2tV9zul0KjY2VuvXr5ckbd26VYWFhR4xkZGRatGihTtmw4YNcrlc7mREktq1ayeXy+WOKQsSEgAALObw0j9JSUnudRqnj6SkpHOaU2ZmpiQpPDzc43x4eLj7WmZmpvz8/FSzZs2zxoSFhRUbPywszB1TFuyyAQDgAjF+/HiNHj3a45zT6fxLYzrOWC1rGEaxc2c6M6ak+LKM82dUSAAAsJi3dtk4nU7VqFHD4zjXhCQiIkKSilUxsrKy3FWTiIgIFRQUKDs7+6wx+/fvLzb+gQMHilVfzoaEBAAAi9m1y+Zs6tevr4iICK1atcp9rqCgQGvXrlX79u0lSVFRUfL19fWIycjI0I4dO9wxMTExysnJ0ebNm90xmzZtUk5OjjumLGjZAABQSR07dky7d+92f52enq60tDQFBwerXr16SkhIUGJioho3bqzGjRsrMTFRAQEBGjBggCTJ5XIpPj5eY8aMUUhIiIKDgzV27Fi1bNlSnTt3liQ1bdpU3bp103333afZs2dLkoYMGaJevXqVeYeNREICAID1bHpQ65YtW9SxY0f316fXnwwaNEgLFy7UI488ory8PA0fPlzZ2dmKjo7WypUrFRQU5H7N9OnTVbVqVfXr1095eXnq1KmTFi5cKB8fH3fM4sWL9cADD7h34/Tp06fUZ5+UhueQAH8jPIcEKO58PIfku4zjXhnn8toBXhmnImINCQAAsB0tGwAALHYuv4fm74aEBAAAi5GPmCMhAQDAamQkplhDAgAAbEeFBAAAizkokZgiIQEAwGIsajVHywYAANiOCgkAABajQGKOhAQAAKuRkZiiZQMAAGxHhQQAAIuxy8YcCQkAABZjl405WjYAAMB2VEgAALAYBRJzJCQAAFiNjMQUCQkAABZjUas51pAAAADbUSEBAMBi7LIxR0ICAIDFyEfM0bIBAAC2o0ICAIDFaNmYIyEBAMByZCRmaNkAAADbUSEBAMBitGzMkZAAAGAx8hFztGwAAIDtqJAAAGAxWjbmSEgAALAYv8vGHAkJAABWIx8xxRoSAABgOyokAABYjAKJORISAAAsxqJWc7RsAACA7aiQAABgMXbZmCMhAQDAauQjpmjZAAAA21EhAQDAYhRIzJGQAABgMXbZmKNlAwAAbEeFBAAAi7HLxhwJCQAAFqNlY46WDQAAsB0JCQAAsB0tGwAALEbLxhwJCQAAFmNRqzlaNgAAwHZUSAAAsBgtG3MkJAAAWIx8xBwtGwAAYDsqJAAAWI0SiSkSEgAALMYuG3O0bAAAgO2okAAAYDF22ZgjIQEAwGLkI+ZISAAAsBoZiSnWkAAAANtRIQEAwGLssjFHQgIAgMVY1GqOlg0AALCdwzAMw+5JoHLKz89XUlKSxo8fL6fTafd0gAqD7w2gOBISWObIkSNyuVzKyclRjRo17J4OUGHwvQEUR8sGAADYjoQEAADYjoQEAADYjoQElnE6nZo0aRKL9oAz8L0BFMeiVgAAYDsqJAAAwHYkJAAAwHYkJAAAwHYkJAAAwHYkJLDMK6+8ovr166tatWqKiorSV199ZfeUAFt9+eWX6t27tyIjI+VwOPT+++/bPSWgwiAhgSXefvttJSQkaMKECdq2bZuuvfZade/eXXv37rV7aoBtcnNz1apVKyUnJ9s9FaDCYdsvLBEdHa02bdpo5syZ7nNNmzZV3759lZSUZOPMgIrB4XBo+fLl6tu3r91TASoEKiTwuoKCAm3dulVdu3b1ON+1a1etX7/eplkBACoyEhJ43cGDB1VUVKTw8HCP8+Hh4crMzLRpVgCAioyEBJZxOBweXxuGUewcAAASCQksEBoaKh8fn2LVkKysrGJVEwAAJBISWMDPz09RUVFatWqVx/lVq1apffv2Ns0KAFCRVbV7AqicRo8erbi4OLVt21YxMTF69dVXtXfvXg0bNszuqQG2OXbsmHbv3u3+Oj09XWlpaQoODla9evVsnBlgP7b9wjKvvPKKpk2bpoyMDLVo0ULTp0/XddddZ/e0ANt88cUX6tixY7HzgwYN0sKFC8//hIAKhIQEAADYjjUkAADAdiQkAADAdiQkAADAdiQkAADAdiQkAADAdiQkAADAdiQkAADAdiQkQCU0efJkXXnlle6vBw8erL59+573eezZs0cOh0NpaWnn/d4ALiwkJMB5NHjwYDkcDjkcDvn6+qpBgwYaO3ascnNzLb3viy++WOYngZJEALADv8sGOM+6deumBQsWqLCwUF999ZXuvfde5ebmaubMmR5xhYWF8vX19co9XS6XV8YBAKtQIQHOM6fTqYiICNWtW1cDBgzQnXfeqffff9/dZpk/f74aNGggp9MpwzCUk5OjIUOGKCwsTDVq1ND111+v//73vx5jTpkyReHh4QoKClJ8fLxOnDjhcf3Mls2pU6c0depUNWrUSE6nU/Xq1dO//vUvSVL9+vUlSa1bt5bD4VCHDh3cr1uwYIGaNm2qatWq6fLLL9crr7zicZ/NmzerdevWqlatmtq2batt27Z58ZMDUJlRIQFs5u/vr8LCQknS7t27tXTpUi1btkw+Pj6SpJ49eyo4OFgrVqyQy+XS7Nmz1alTJ/3www8KDg7W0qVLNWnSJL388su69tprtWjRIr300ktq0KBBqfccP3685syZo+nTp+sf//iHMjIy9N1330n6I6m4+uqr9dlnn6l58+by8/OTJM2ZM0eTJk1ScnKyWrdurW3btum+++5TYGCgBg0apNzcXPXq1UvXX3+93njjDaWnp+vBBx+0+NMDUGkYAM6bQYMGGTfeeKP7602bNhkhISFGv379jEmTJhm+vr5GVlaW+/rnn39u1KhRwzhx4oTHOA0bNjRmz55tGIZhxMTEGMOGDfO4Hh0dbbRq1arE+x45csRwOp3GnDlzSpxjenq6IcnYtm2bx/m6desab775pse5p556yoiJiTEMwzBmz55tBAcHG7m5ue7rM2fOLHEsADgTLRvgPPv4449VvXp1VatWTTExMbruuus0Y8YMSdIll1yiWrVquWO3bt2qY8eOKSQkRNWrV3cf6enp+umnnyRJu3btUkxMjMc9zvz6z3bt2qX8/Hx16tSpzHM+cOCAfv31V8XHx3vM4+mnn/aYR6tWrRQQEFCmeQDAn9GyAc6zjh07aubMmfL19VVkZKTHwtXAwECP2FOnTql27dr64osvio1z0UUXndP9/f39y/2aU6dOSfqjbRMdHe1x7XRryTCMc5oPAEgkJMB5FxgYqEaNGpUptk2bNsrMzFTVqlV16aWXlhjTtGlTbdy4UXfddZf73MaNG0sds3HjxvL399fnn3+ue++9t9j102tGioqK3OfCw8NVp04d/fzzz7rzzjtLHLdZs2ZatGiR8vLy3EnP2eYBAH9GywaowDp37qyYmBj17dtXn376qfbs2aP169frscce05YtWyRJDz74oObPn6/58+frhx9+0KRJk7Rz585Sx6xWrZrGjRunRx55RK+//rp++uknbdy4UfPmzZMkhYWFyd/fXykpKdq/f79ycnIk/fGwtaSkJL344ov64YcftH37di1YsEDPP/+8JGnAgAGqUqWK4uPj9e2332rFihV69tlnLf6EAFQWJCRABeZwOLRixQpdd911uueee3TZZZfp9ttv1549exQeHi5J6t+/vyZOnKhx48YpKipKv/zyi+6///6zjvv4449rzJgxmjhxopo2bar+/fsrKytLklS1alW99NJLmj17tiIjI3XjjTdKku69917NnTtXCxcuVMuWLRUbG6uFCxe6twlXr15dH330kb799lu1bt1aEyZM0NSpUy38dABUJg6Dxi8AALAZFRIAAGA7EhIAAGA7EhIAAGA7EhIAAGA7EhIAAGA7EhIAAGA7EhIAAGA7EhIAAGA7EhIAAGA7EhIAAGA7EhIAAGA7EhIAAGC7/wfuIeznWpgu5AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Classifcation Report - LightGBM\n",
      "**************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.40      0.54       515\n",
      "           1       0.95      0.99      0.97      5426\n",
      "\n",
      "    accuracy                           0.94      5941\n",
      "   macro avg       0.90      0.70      0.76      5941\n",
      "weighted avg       0.94      0.94      0.93      5941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_lgbm = lgb.LGBMClassifier(objective='binary',class_weight= {0: 1.0, 1: 1.0}, n_estimators= 76)\n",
    "\n",
    "best_lgbm.fit(X_tr_vec, y_tr)\n",
    "y_pr = best_lgbm.predict(X_te_vec)\n",
    "y_prob = best_lgbm.predict_proba(X_te_vec)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_te, y_prob[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "precision, recall, pthresholds = precision_recall_curve(y_te, y_prob[:, 1])\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "# Plot ROC curve\n",
    "\n",
    "ax[0].plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n",
    "ax[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax[0].set_xlabel('False Positive Rate')\n",
    "ax[0].set_ylabel('True Positive Rate')\n",
    "ax[0].set_title('Receiver Operating Characteristic (ROC) Curve - LightGBM')\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "ax[1].plot(recall, precision, color='b', alpha=0.8, label='Precision-Recall curve (area = %0.4f)' % pr_auc)\n",
    "ax[1].set_xlabel('Recall')\n",
    "ax[1].set_ylabel('Precision')\n",
    "ax[1].set_title('Precision-Recall Curve - LightGBM')\n",
    "ax[1].legend(loc='lower left')\n",
    "\n",
    "plt.show();\n",
    "# Classification report\n",
    "\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_te, y_pr), cmap='Blues', cbar=True, annot=True, xticklabels= True, yticklabels=True, fmt = 'd')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - LightGBM')\n",
    "\n",
    "plt.show();\n",
    "\n",
    "print('*'*50)\n",
    "print('Classifcation Report - LightGBM')\n",
    "print('*'*50)\n",
    "print(classification_report(y_te, y_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3622d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Classifcation Report - LinearSVC\n",
      "**************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.41      0.54       515\n",
      "           1       0.95      0.99      0.97      5426\n",
      "\n",
      "    accuracy                           0.94      5941\n",
      "   macro avg       0.86      0.70      0.75      5941\n",
      "weighted avg       0.93      0.94      0.93      5941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_svc = LinearSVC(dual='auto', C= 1, class_weight= {0: 1.0, 1: 1.0}, max_iter= 4264)\n",
    "\n",
    "clf = CalibratedClassifierCV(best_svc) \n",
    "clf.fit(X_tr_vec, y_tr)\n",
    "y_pr = clf.predict(X_te_vec)\n",
    "y_prob = clf.predict_proba(X_te_vec)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_te, y_prob[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "precision, recall, pthresholds = precision_recall_curve(y_te, y_prob[:, 1])\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "# Plot ROC curve\n",
    "\n",
    "ax[0].plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n",
    "ax[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax[0].set_xlabel('False Positive Rate')\n",
    "ax[0].set_ylabel('True Positive Rate')\n",
    "ax[0].set_title('Receiver Operating Characteristic (ROC) Curve - LinearSVC')\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "ax[1].plot(recall, precision, color='b', alpha=0.8, label='Precision-Recall curve (area = %0.4f)' % pr_auc)\n",
    "ax[1].set_xlabel('Recall')\n",
    "ax[1].set_ylabel('Precision')\n",
    "ax[1].set_title('Precision-Recall Curve - LinearSVC')\n",
    "ax[1].legend(loc='lower left')\n",
    "\n",
    "plt.show();\n",
    "# Classification report\n",
    "\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_te, y_pr), cmap='Blues', cbar=True, annot=True, xticklabels= True, yticklabels=True, fmt = 'd')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - LinearSVC')\n",
    "\n",
    "plt.show();\n",
    "\n",
    "print('*'*50)\n",
    "print('Classifcation Report - LinearSVC')\n",
    "print('*'*50)\n",
    "print(classification_report(y_te, y_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec64ec96",
   "metadata": {},
   "source": [
    "## What a show. LightGBM is marginally ahead in recall of the positive class, AOC ROC, and AOC Precision-Recall. We're going with LightGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd373da",
   "metadata": {},
   "source": [
    "#### best_lgbm = lgb.LGBMClassifier(objective='binary',class_weight= {0: 1.0, 1: 1.0}, n_estimators= 76)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144be620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
