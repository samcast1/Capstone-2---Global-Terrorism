{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c666cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0133814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_vec = sp.sparse.load_npz(r'C:\\Users\\casti\\OneDrive\\Documents\\A Springboard\\Capstone 2 - Global Terrorism\\data\\interim\\X_tr_vec.npz')\n",
    "X_te_vec = sp.sparse.load_npz(r'C:\\Users\\casti\\OneDrive\\Documents\\A Springboard\\Capstone 2 - Global Terrorism\\data\\interim\\X_te_vec.npz')\n",
    "y_tr = pd.read_csv(r'C:\\Users\\casti\\OneDrive\\Documents\\A Springboard\\Capstone 2 - Global Terrorism\\data\\interim\\y_tr.csv')\n",
    "y_te = pd.read_csv(r'C:\\Users\\casti\\OneDrive\\Documents\\A Springboard\\Capstone 2 - Global Terrorism\\data\\interim\\y_te.csv')\n",
    "y_tr = y_tr.drop(columns = ['Unnamed: 0']).values.ravel()\n",
    "y_te = y_te.drop(columns = ['Unnamed: 0']).values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2614eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=15, class_weight={0:10.0, 1:1.0})\n",
    "rf.fit(X_tr_vec, y_tr)\n",
    "y_pr = rf.predict(X_te_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3e6a6751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.42      0.39       515\n",
      "           1       0.94      0.93      0.94      5426\n",
      "\n",
      "    accuracy                           0.89      5941\n",
      "   macro avg       0.66      0.68      0.67      5941\n",
      "weighted avg       0.89      0.89      0.89      5941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_te, y_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b060b923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_te, y_pr)\n",
    "\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt = 'd', cbar=True, xticklabels=True, yticklabels=True)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c5a070e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END criterion=gini, max_depth=9, max_features=sqrt, n_estimators=28; f1: (test=0.909) precision: (test=0.946) recall: (test=0.875) total time=   6.5s\n",
      "[CV 2/3] END criterion=gini, max_depth=9, max_features=sqrt, n_estimators=28; f1: (test=0.911) precision: (test=0.935) recall: (test=0.887) total time=   7.2s\n",
      "[CV 3/3] END criterion=gini, max_depth=9, max_features=sqrt, n_estimators=28; f1: (test=0.941) precision: (test=0.930) recall: (test=0.952) total time=   5.5s\n",
      "[CV 1/3] END criterion=gini, max_depth=12, max_features=log2, n_estimators=18; f1: (test=0.952) precision: (test=0.915) recall: (test=0.992) total time=   2.7s\n",
      "[CV 2/3] END criterion=gini, max_depth=12, max_features=log2, n_estimators=18; f1: (test=0.953) precision: (test=0.918) recall: (test=0.991) total time=   0.8s\n",
      "[CV 3/3] END criterion=gini, max_depth=12, max_features=log2, n_estimators=18; f1: (test=0.951) precision: (test=0.914) recall: (test=0.989) total time=   2.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=12, max_features=sqrt, n_estimators=60; f1: (test=0.921) precision: (test=0.948) recall: (test=0.895) total time=  20.8s\n",
      "[CV 2/3] END criterion=entropy, max_depth=12, max_features=sqrt, n_estimators=60; f1: (test=0.950) precision: (test=0.933) recall: (test=0.967) total time=  19.6s\n",
      "[CV 3/3] END criterion=entropy, max_depth=12, max_features=sqrt, n_estimators=60; f1: (test=0.951) precision: (test=0.933) recall: (test=0.969) total time=  17.8s\n",
      "[CV 1/3] END criterion=entropy, max_depth=12, max_features=sqrt, n_estimators=42; f1: (test=0.937) precision: (test=0.935) recall: (test=0.939) total time=  16.8s\n",
      "[CV 2/3] END criterion=entropy, max_depth=12, max_features=sqrt, n_estimators=42; f1: (test=0.926) precision: (test=0.947) recall: (test=0.907) total time=  15.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=12, max_features=sqrt, n_estimators=42; f1: (test=0.948) precision: (test=0.931) recall: (test=0.966) total time=  11.7s\n",
      "[CV 1/3] END criterion=gini, max_depth=9, max_features=sqrt, n_estimators=33; f1: (test=0.936) precision: (test=0.929) recall: (test=0.943) total time=   8.3s\n",
      "[CV 2/3] END criterion=gini, max_depth=9, max_features=sqrt, n_estimators=33; f1: (test=0.940) precision: (test=0.942) recall: (test=0.938) total time=  14.6s\n",
      "[CV 3/3] END criterion=gini, max_depth=9, max_features=sqrt, n_estimators=33; f1: (test=0.939) precision: (test=0.937) recall: (test=0.941) total time=  13.9s\n",
      "[CV 1/3] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=39; f1: (test=0.952) precision: (test=0.923) recall: (test=0.982) total time=   8.4s\n",
      "[CV 2/3] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=39; f1: (test=0.940) precision: (test=0.940) recall: (test=0.940) total time=  11.7s\n",
      "[CV 3/3] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=39; f1: (test=0.938) precision: (test=0.933) recall: (test=0.943) total time=  15.3s\n",
      "[CV 1/3] END criterion=entropy, max_depth=6, max_features=log2, n_estimators=29; f1: (test=0.953) precision: (test=0.917) recall: (test=0.993) total time=   2.4s\n",
      "[CV 2/3] END criterion=entropy, max_depth=6, max_features=log2, n_estimators=29; f1: (test=0.955) precision: (test=0.918) recall: (test=0.995) total time=   2.5s\n",
      "[CV 3/3] END criterion=entropy, max_depth=6, max_features=log2, n_estimators=29; f1: (test=0.952) precision: (test=0.915) recall: (test=0.992) total time=   3.7s\n",
      "[CV 1/3] END criterion=gini, max_depth=14, max_features=sqrt, n_estimators=47; f1: (test=0.933) precision: (test=0.940) recall: (test=0.927) total time=  32.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=14, max_features=sqrt, n_estimators=47; f1: (test=0.944) precision: (test=0.932) recall: (test=0.956) total time=  31.2s\n",
      "[CV 3/3] END criterion=gini, max_depth=14, max_features=sqrt, n_estimators=47; f1: (test=0.945) precision: (test=0.934) recall: (test=0.956) total time=  21.6s\n",
      "[CV 1/3] END criterion=entropy, max_depth=12, max_features=sqrt, n_estimators=24; f1: (test=0.944) precision: (test=0.934) recall: (test=0.954) total time=   9.5s\n",
      "[CV 2/3] END criterion=entropy, max_depth=12, max_features=sqrt, n_estimators=24; f1: (test=0.951) precision: (test=0.922) recall: (test=0.981) total time=   4.6s\n",
      "[CV 3/3] END criterion=entropy, max_depth=12, max_features=sqrt, n_estimators=24; f1: (test=0.927) precision: (test=0.931) recall: (test=0.924) total time=   7.4s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=57; f1: (test=0.955) precision: (test=0.914) recall: (test=1.000) total time=   1.6s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=57; f1: (test=0.955) precision: (test=0.915) recall: (test=1.000) total time=   1.3s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=57; f1: (test=0.955) precision: (test=0.914) recall: (test=1.000) total time=   1.2s\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m cv \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(rf, param_distributions\u001b[38;5;241m=\u001b[39mparams, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \\\n\u001b[0;32m     11\u001b[0m                        cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m], refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m cv\u001b[38;5;241m.\u001b[39mfit(X_tr_vec, y_tr)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest estimator:\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m.\u001b[39mbest_estimator_)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest paramaters:\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m.\u001b[39mbest_paramaters_)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest scores:\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m.\u001b[39mbest_scores_)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(class_weight={0:10.0, 1:1.0})\n",
    "\n",
    "params = {'n_estimators': randint(10,100),\n",
    "          'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': randint(5,15),\n",
    "         'max_features': ['sqrt','log2'],\n",
    "         }\n",
    "\n",
    "\n",
    "cv = RandomizedSearchCV(rf, param_distributions=params, n_iter=10, \\\n",
    "                       cv = 3, verbose=3, scoring = ['precision','recall','f1'], refit=False)\n",
    "\n",
    "cv.fit(X_tr_vec, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b802a5c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_precision</th>\n",
       "      <th>split1_test_precision</th>\n",
       "      <th>split2_test_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>rank_test_precision</th>\n",
       "      <th>split0_test_recall</th>\n",
       "      <th>split1_test_recall</th>\n",
       "      <th>split2_test_recall</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>rank_test_recall</th>\n",
       "      <th>split0_test_f1</th>\n",
       "      <th>split1_test_f1</th>\n",
       "      <th>split2_test_f1</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.308179</td>\n",
       "      <td>0.217772</td>\n",
       "      <td>0.184036</td>\n",
       "      <td>0.028069</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>57</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'max_...</td>\n",
       "      <td>0.914238</td>\n",
       "      <td>0.914980</td>\n",
       "      <td>0.913930</td>\n",
       "      <td>0.914382</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>10</td>\n",
       "      <td>0.999816</td>\n",
       "      <td>0.999631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999816</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>1</td>\n",
       "      <td>0.955114</td>\n",
       "      <td>0.955434</td>\n",
       "      <td>0.955029</td>\n",
       "      <td>0.955192</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.607995</td>\n",
       "      <td>0.559407</td>\n",
       "      <td>0.380370</td>\n",
       "      <td>0.060535</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>log2</td>\n",
       "      <td>29</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6, 'max_...</td>\n",
       "      <td>0.916638</td>\n",
       "      <td>0.917573</td>\n",
       "      <td>0.914810</td>\n",
       "      <td>0.916341</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>8</td>\n",
       "      <td>0.992814</td>\n",
       "      <td>0.995024</td>\n",
       "      <td>0.991522</td>\n",
       "      <td>0.993120</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>2</td>\n",
       "      <td>0.953207</td>\n",
       "      <td>0.954730</td>\n",
       "      <td>0.951623</td>\n",
       "      <td>0.953187</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.902558</td>\n",
       "      <td>0.785276</td>\n",
       "      <td>0.083294</td>\n",
       "      <td>0.012778</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>log2</td>\n",
       "      <td>18</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'max_fe...</td>\n",
       "      <td>0.915053</td>\n",
       "      <td>0.918231</td>\n",
       "      <td>0.914495</td>\n",
       "      <td>0.915926</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>9</td>\n",
       "      <td>0.992445</td>\n",
       "      <td>0.991338</td>\n",
       "      <td>0.989495</td>\n",
       "      <td>0.991093</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>3</td>\n",
       "      <td>0.952179</td>\n",
       "      <td>0.953385</td>\n",
       "      <td>0.950518</td>\n",
       "      <td>0.952027</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.563911</td>\n",
       "      <td>2.704586</td>\n",
       "      <td>0.372242</td>\n",
       "      <td>0.170225</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>39</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 8, 'max_...</td>\n",
       "      <td>0.923130</td>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.932883</td>\n",
       "      <td>0.932104</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>6</td>\n",
       "      <td>0.982495</td>\n",
       "      <td>0.940472</td>\n",
       "      <td>0.942683</td>\n",
       "      <td>0.955217</td>\n",
       "      <td>0.019310</td>\n",
       "      <td>4</td>\n",
       "      <td>0.951888</td>\n",
       "      <td>0.940385</td>\n",
       "      <td>0.937758</td>\n",
       "      <td>0.943344</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.085477</td>\n",
       "      <td>4.731450</td>\n",
       "      <td>0.283872</td>\n",
       "      <td>0.078347</td>\n",
       "      <td>gini</td>\n",
       "      <td>14</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>47</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 14, 'max_fe...</td>\n",
       "      <td>0.939518</td>\n",
       "      <td>0.932075</td>\n",
       "      <td>0.934102</td>\n",
       "      <td>0.935232</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>5</td>\n",
       "      <td>0.927400</td>\n",
       "      <td>0.955953</td>\n",
       "      <td>0.956137</td>\n",
       "      <td>0.946497</td>\n",
       "      <td>0.013504</td>\n",
       "      <td>6</td>\n",
       "      <td>0.933420</td>\n",
       "      <td>0.943863</td>\n",
       "      <td>0.944991</td>\n",
       "      <td>0.940758</td>\n",
       "      <td>0.005209</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.175259</td>\n",
       "      <td>2.002239</td>\n",
       "      <td>0.088706</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>entropy</td>\n",
       "      <td>12</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>24</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 12, 'max...</td>\n",
       "      <td>0.933983</td>\n",
       "      <td>0.922384</td>\n",
       "      <td>0.930919</td>\n",
       "      <td>0.929095</td>\n",
       "      <td>0.004908</td>\n",
       "      <td>7</td>\n",
       "      <td>0.954118</td>\n",
       "      <td>0.981202</td>\n",
       "      <td>0.923885</td>\n",
       "      <td>0.953068</td>\n",
       "      <td>0.023411</td>\n",
       "      <td>5</td>\n",
       "      <td>0.943943</td>\n",
       "      <td>0.950884</td>\n",
       "      <td>0.927389</td>\n",
       "      <td>0.940739</td>\n",
       "      <td>0.009856</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19.273308</td>\n",
       "      <td>1.192171</td>\n",
       "      <td>0.239411</td>\n",
       "      <td>0.042988</td>\n",
       "      <td>entropy</td>\n",
       "      <td>12</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>60</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 12, 'max...</td>\n",
       "      <td>0.947913</td>\n",
       "      <td>0.933108</td>\n",
       "      <td>0.933286</td>\n",
       "      <td>0.938102</td>\n",
       "      <td>0.006937</td>\n",
       "      <td>1</td>\n",
       "      <td>0.895338</td>\n",
       "      <td>0.966642</td>\n",
       "      <td>0.969407</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.034283</td>\n",
       "      <td>7</td>\n",
       "      <td>0.920876</td>\n",
       "      <td>0.949579</td>\n",
       "      <td>0.951003</td>\n",
       "      <td>0.940486</td>\n",
       "      <td>0.013879</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.165581</td>\n",
       "      <td>2.729445</td>\n",
       "      <td>0.186545</td>\n",
       "      <td>0.069916</td>\n",
       "      <td>gini</td>\n",
       "      <td>9</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>33</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 9, 'max_fea...</td>\n",
       "      <td>0.929168</td>\n",
       "      <td>0.941873</td>\n",
       "      <td>0.936697</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>4</td>\n",
       "      <td>0.942694</td>\n",
       "      <td>0.937707</td>\n",
       "      <td>0.940840</td>\n",
       "      <td>0.940414</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>8</td>\n",
       "      <td>0.935882</td>\n",
       "      <td>0.939786</td>\n",
       "      <td>0.938764</td>\n",
       "      <td>0.938144</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.322800</td>\n",
       "      <td>1.958877</td>\n",
       "      <td>0.304014</td>\n",
       "      <td>0.246772</td>\n",
       "      <td>entropy</td>\n",
       "      <td>12</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>42</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 12, 'max...</td>\n",
       "      <td>0.935377</td>\n",
       "      <td>0.946700</td>\n",
       "      <td>0.930894</td>\n",
       "      <td>0.937657</td>\n",
       "      <td>0.006651</td>\n",
       "      <td>2</td>\n",
       "      <td>0.938824</td>\n",
       "      <td>0.906745</td>\n",
       "      <td>0.965721</td>\n",
       "      <td>0.937097</td>\n",
       "      <td>0.024108</td>\n",
       "      <td>9</td>\n",
       "      <td>0.937098</td>\n",
       "      <td>0.926292</td>\n",
       "      <td>0.947987</td>\n",
       "      <td>0.937126</td>\n",
       "      <td>0.008857</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.362544</td>\n",
       "      <td>0.702424</td>\n",
       "      <td>0.124646</td>\n",
       "      <td>0.017773</td>\n",
       "      <td>gini</td>\n",
       "      <td>9</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>28</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 9, 'max_fea...</td>\n",
       "      <td>0.946204</td>\n",
       "      <td>0.935290</td>\n",
       "      <td>0.930115</td>\n",
       "      <td>0.937203</td>\n",
       "      <td>0.006706</td>\n",
       "      <td>3</td>\n",
       "      <td>0.875069</td>\n",
       "      <td>0.887025</td>\n",
       "      <td>0.951714</td>\n",
       "      <td>0.904603</td>\n",
       "      <td>0.033668</td>\n",
       "      <td>10</td>\n",
       "      <td>0.909248</td>\n",
       "      <td>0.910518</td>\n",
       "      <td>0.940791</td>\n",
       "      <td>0.920186</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       1.308179      0.217772         0.184036        0.028069   \n",
       "1       2.607995      0.559407         0.380370        0.060535   \n",
       "2       1.902558      0.785276         0.083294        0.012778   \n",
       "3      11.563911      2.704586         0.372242        0.170225   \n",
       "4      28.085477      4.731450         0.283872        0.078347   \n",
       "5       7.175259      2.002239         0.088706        0.002945   \n",
       "6      19.273308      1.192171         0.239411        0.042988   \n",
       "7      12.165581      2.729445         0.186545        0.069916   \n",
       "8      14.322800      1.958877         0.304014        0.246772   \n",
       "9       6.362544      0.702424         0.124646        0.017773   \n",
       "\n",
       "  param_criterion param_max_depth param_max_features param_n_estimators  \\\n",
       "0         entropy               5               log2                 57   \n",
       "1         entropy               6               log2                 29   \n",
       "2            gini              12               log2                 18   \n",
       "3         entropy               8               sqrt                 39   \n",
       "4            gini              14               sqrt                 47   \n",
       "5         entropy              12               sqrt                 24   \n",
       "6         entropy              12               sqrt                 60   \n",
       "7            gini               9               sqrt                 33   \n",
       "8         entropy              12               sqrt                 42   \n",
       "9            gini               9               sqrt                 28   \n",
       "\n",
       "                                              params  split0_test_precision  \\\n",
       "0  {'criterion': 'entropy', 'max_depth': 5, 'max_...               0.914238   \n",
       "1  {'criterion': 'entropy', 'max_depth': 6, 'max_...               0.916638   \n",
       "2  {'criterion': 'gini', 'max_depth': 12, 'max_fe...               0.915053   \n",
       "3  {'criterion': 'entropy', 'max_depth': 8, 'max_...               0.923130   \n",
       "4  {'criterion': 'gini', 'max_depth': 14, 'max_fe...               0.939518   \n",
       "5  {'criterion': 'entropy', 'max_depth': 12, 'max...               0.933983   \n",
       "6  {'criterion': 'entropy', 'max_depth': 12, 'max...               0.947913   \n",
       "7  {'criterion': 'gini', 'max_depth': 9, 'max_fea...               0.929168   \n",
       "8  {'criterion': 'entropy', 'max_depth': 12, 'max...               0.935377   \n",
       "9  {'criterion': 'gini', 'max_depth': 9, 'max_fea...               0.946204   \n",
       "\n",
       "   split1_test_precision  split2_test_precision  mean_test_precision  \\\n",
       "0               0.914980               0.913930             0.914382   \n",
       "1               0.917573               0.914810             0.916341   \n",
       "2               0.918231               0.914495             0.915926   \n",
       "3               0.940299               0.932883             0.932104   \n",
       "4               0.932075               0.934102             0.935232   \n",
       "5               0.922384               0.930919             0.929095   \n",
       "6               0.933108               0.933286             0.938102   \n",
       "7               0.941873               0.936697             0.935913   \n",
       "8               0.946700               0.930894             0.937657   \n",
       "9               0.935290               0.930115             0.937203   \n",
       "\n",
       "   std_test_precision  rank_test_precision  split0_test_recall  \\\n",
       "0            0.000441                   10            0.999816   \n",
       "1            0.001147                    8            0.992814   \n",
       "2            0.001646                    9            0.992445   \n",
       "3            0.007031                    6            0.982495   \n",
       "4            0.003142                    5            0.927400   \n",
       "5            0.004908                    7            0.954118   \n",
       "6            0.006937                    1            0.895338   \n",
       "7            0.005216                    4            0.942694   \n",
       "8            0.006651                    2            0.938824   \n",
       "9            0.006706                    3            0.875069   \n",
       "\n",
       "   split1_test_recall  split2_test_recall  mean_test_recall  std_test_recall  \\\n",
       "0            0.999631            1.000000          0.999816         0.000150   \n",
       "1            0.995024            0.991522          0.993120         0.001446   \n",
       "2            0.991338            0.989495          0.991093         0.001217   \n",
       "3            0.940472            0.942683          0.955217         0.019310   \n",
       "4            0.955953            0.956137          0.946497         0.013504   \n",
       "5            0.981202            0.923885          0.953068         0.023411   \n",
       "6            0.966642            0.969407          0.943796         0.034283   \n",
       "7            0.937707            0.940840          0.940414         0.002058   \n",
       "8            0.906745            0.965721          0.937097         0.024108   \n",
       "9            0.887025            0.951714          0.904603         0.033668   \n",
       "\n",
       "   rank_test_recall  split0_test_f1  split1_test_f1  split2_test_f1  \\\n",
       "0                 1        0.955114        0.955434        0.955029   \n",
       "1                 2        0.953207        0.954730        0.951623   \n",
       "2                 3        0.952179        0.953385        0.950518   \n",
       "3                 4        0.951888        0.940385        0.937758   \n",
       "4                 6        0.933420        0.943863        0.944991   \n",
       "5                 5        0.943943        0.950884        0.927389   \n",
       "6                 7        0.920876        0.949579        0.951003   \n",
       "7                 8        0.935882        0.939786        0.938764   \n",
       "8                 9        0.937098        0.926292        0.947987   \n",
       "9                10        0.909248        0.910518        0.940791   \n",
       "\n",
       "   mean_test_f1  std_test_f1  rank_test_f1  \n",
       "0      0.955192     0.000174             1  \n",
       "1      0.953187     0.001269             2  \n",
       "2      0.952027     0.001176             3  \n",
       "3      0.943344     0.006136             4  \n",
       "4      0.940758     0.005209             5  \n",
       "5      0.940739     0.009856             6  \n",
       "6      0.940486     0.013879             7  \n",
       "7      0.938144     0.001653             8  \n",
       "8      0.937126     0.008857             9  \n",
       "9      0.920186     0.014579            10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>1.308179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.217772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.184036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.028069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_criterion</th>\n",
       "      <td>entropy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_max_depth</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_max_features</th>\n",
       "      <td>log2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_n_estimators</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'max_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.914238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.91498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.91393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.914382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.000441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.999816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.999631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.999816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.955114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.955434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.955029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.955192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.000174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       0\n",
       "mean_fit_time                                                   1.308179\n",
       "std_fit_time                                                    0.217772\n",
       "mean_score_time                                                 0.184036\n",
       "std_score_time                                                  0.028069\n",
       "param_criterion                                                  entropy\n",
       "param_max_depth                                                        5\n",
       "param_max_features                                                  log2\n",
       "param_n_estimators                                                    57\n",
       "params                 {'criterion': 'entropy', 'max_depth': 5, 'max_...\n",
       "split0_test_precision                                           0.914238\n",
       "split1_test_precision                                            0.91498\n",
       "split2_test_precision                                            0.91393\n",
       "mean_test_precision                                             0.914382\n",
       "std_test_precision                                              0.000441\n",
       "rank_test_precision                                                   10\n",
       "split0_test_recall                                              0.999816\n",
       "split1_test_recall                                              0.999631\n",
       "split2_test_recall                                                   1.0\n",
       "mean_test_recall                                                0.999816\n",
       "std_test_recall                                                  0.00015\n",
       "rank_test_recall                                                       1\n",
       "split0_test_f1                                                  0.955114\n",
       "split1_test_f1                                                  0.955434\n",
       "split2_test_f1                                                  0.955029\n",
       "mean_test_f1                                                    0.955192\n",
       "std_test_f1                                                     0.000174\n",
       "rank_test_f1                                                           1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "rf_cvdf = pd.DataFrame(cv.cv_results_).sort_values(by='rank_test_f1').reset_index(drop=True)\n",
    "rf_best = pd.DataFrame(rf_cvdf.iloc[0,:])\n",
    "display(rf_cvdf)\n",
    "display(rf_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7c0c1d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.02      0.05       515\n",
      "           1       0.92      1.00      0.96      5426\n",
      "\n",
      "    accuracy                           0.91      5941\n",
      "   macro avg       0.83      0.51      0.50      5941\n",
      "weighted avg       0.90      0.91      0.88      5941\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_rf = RandomForestClassifier(class_weight={0:10.0, 1:1.0}, criterion= 'entropy', max_depth= 5, \\\n",
    "                                 max_features= 'log2', n_estimators= 57)\n",
    "\n",
    "best_rf.fit(X_tr_vec, y_tr)\n",
    "y_pr = best_rf.predict(X_te_vec)\n",
    "\n",
    "print(classification_report(y_te, y_pr))\n",
    "sns.heatmap(confusion_matrix(y_te, y_pr), cmap='Blues', cbar=True, annot=True, xticklabels= True, yticklabels=True, fmt = 'd')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f77de",
   "metadata": {},
   "source": [
    "### Best RF model: class_weight={0:10.0, 1:1.0}, 'criterion': 'entropy', 'max_depth': 5, 'max_features': 'log2', 'n_estimators': 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e7954c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16279, number of negative: 1544\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.328603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 40627\n",
      "[LightGBM] [Info] Number of data points in the train set: 17823, number of used features: 1999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678320 -> initscore=0.746062\n",
      "[LightGBM] [Info] Start training from score 0.746062\n"
     ]
    }
   ],
   "source": [
    "lgbm = lgb.LGBMClassifier(objective='binary', class_weight={0: 5.0, 1: 1.0}, n_estimators=500)\n",
    "\n",
    "lgbm.fit(X_tr_vec, y_tr)\n",
    "\n",
    "y_pr = lgbm.predict(X_te_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "827be78d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.60      0.58       515\n",
      "           1       0.96      0.96      0.96      5426\n",
      "\n",
      "    accuracy                           0.92      5941\n",
      "   macro avg       0.76      0.78      0.77      5941\n",
      "weighted avg       0.93      0.92      0.93      5941\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(classification_report(y_te, y_pr))\n",
    "sns.heatmap(confusion_matrix(y_te, y_pr), cmap='Blues', cbar=True, annot=True, xticklabels= True, yticklabels=True, fmt = 'd')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7b05a050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.137540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[CV 1/3] END class_weight=balanced, n_estimators=66; f1: (test=0.919) precision: (test=0.976) recall: (test=0.868) total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/3] END class_weight=balanced, n_estimators=66; f1: (test=0.911) precision: (test=0.979) recall: (test=0.851) total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/3] END class_weight=balanced, n_estimators=66; f1: (test=0.924) precision: (test=0.978) recall: (test=0.874) total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.156867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678165 -> initscore=0.745353\n",
      "[LightGBM] [Info] Start training from score 0.745353\n",
      "[CV 1/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=460; f1: (test=0.956) precision: (test=0.955) recall: (test=0.957) total time=  13.2s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 2/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=460; f1: (test=0.953) precision: (test=0.958) recall: (test=0.948) total time=  15.0s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.138569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 3/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=460; f1: (test=0.958) precision: (test=0.959) recall: (test=0.958) total time=  14.7s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[CV 1/3] END class_weight=balanced, n_estimators=388; f1: (test=0.946) precision: (test=0.961) recall: (test=0.932) total time=  11.5s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/3] END class_weight=balanced, n_estimators=388; f1: (test=0.944) precision: (test=0.964) recall: (test=0.924) total time=  10.8s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/3] END class_weight=balanced, n_estimators=388; f1: (test=0.950) precision: (test=0.965) recall: (test=0.935) total time=  11.5s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678165 -> initscore=0.745353\n",
      "[LightGBM] [Info] Start training from score 0.745353\n",
      "[CV 1/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=80; f1: (test=0.941) precision: (test=0.968) recall: (test=0.917) total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 2/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=80; f1: (test=0.935) precision: (test=0.973) recall: (test=0.901) total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.268323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 3/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=80; f1: (test=0.945) precision: (test=0.971) recall: (test=0.920) total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678165 -> initscore=0.745353\n",
      "[LightGBM] [Info] Start training from score 0.745353\n",
      "[CV 1/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=381; f1: (test=0.955) precision: (test=0.958) recall: (test=0.952) total time=  13.4s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 2/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=381; f1: (test=0.952) precision: (test=0.960) recall: (test=0.945) total time=  11.6s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 3/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=381; f1: (test=0.957) precision: (test=0.959) recall: (test=0.955) total time=  11.6s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.152676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678165 -> initscore=0.745353\n",
      "[LightGBM] [Info] Start training from score 0.745353\n",
      "[CV 1/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=409; f1: (test=0.956) precision: (test=0.957) recall: (test=0.955) total time=  14.2s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116176 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 2/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=409; f1: (test=0.953) precision: (test=0.959) recall: (test=0.947) total time=  14.1s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.212670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 3/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=409; f1: (test=0.958) precision: (test=0.959) recall: (test=0.957) total time=  12.1s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678165 -> initscore=0.745353\n",
      "[LightGBM] [Info] Start training from score 0.745353\n",
      "[CV 1/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=282; f1: (test=0.952) precision: (test=0.960) recall: (test=0.945) total time=   9.0s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.863381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 2/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=282; f1: (test=0.950) precision: (test=0.962) recall: (test=0.939) total time=  10.5s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 3/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=282; f1: (test=0.955) precision: (test=0.963) recall: (test=0.948) total time=  12.6s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678165 -> initscore=0.745353\n",
      "[LightGBM] [Info] Start training from score 0.745353\n",
      "[CV 1/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=145; f1: (test=0.948) precision: (test=0.964) recall: (test=0.933) total time=   9.7s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 2/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=145; f1: (test=0.945) precision: (test=0.967) recall: (test=0.924) total time=   7.8s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.171527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.678397 -> initscore=0.746416\n",
      "[LightGBM] [Info] Start training from score 0.746416\n",
      "[CV 3/3] END class_weight={0: 5.0, 1: 1.0}, n_estimators=145; f1: (test=0.950) precision: (test=0.967) recall: (test=0.934) total time=   7.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087676 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[CV 1/3] END class_weight=balanced, n_estimators=112; f1: (test=0.929) precision: (test=0.973) recall: (test=0.889) total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.192337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/3] END class_weight=balanced, n_estimators=112; f1: (test=0.922) precision: (test=0.976) recall: (test=0.873) total time=   6.9s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/3] END class_weight=balanced, n_estimators=112; f1: (test=0.931) precision: (test=0.974) recall: (test=0.892) total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 10852, number of negative: 1030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139243 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27949\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1174\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[CV 1/3] END class_weight=balanced, n_estimators=319; f1: (test=0.945) precision: (test=0.962) recall: (test=0.929) total time=  10.0s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27569\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 2/3] END class_weight=balanced, n_estimators=319; f1: (test=0.942) precision: (test=0.966) recall: (test=0.919) total time=  11.0s\n",
      "[LightGBM] [Info] Number of positive: 10853, number of negative: 1029\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.173032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27948\n",
      "[LightGBM] [Info] Number of data points in the train set: 11882, number of used features: 1222\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[CV 3/3] END class_weight=balanced, n_estimators=319; f1: (test=0.948) precision: (test=0.967) recall: (test=0.931) total time=  13.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3, estimator=LGBMClassifier(objective=&#x27;binary&#x27;),\n",
       "                   param_distributions={&#x27;class_weight&#x27;: [&#x27;balanced&#x27;,\n",
       "                                                         {0: 1.0, 1: 1.0},\n",
       "                                                         {0: 5.0, 1: 1.0},\n",
       "                                                         {0: 1.0, 1: 1.0},\n",
       "                                                         {0: 10.0, 1: 1.0}],\n",
       "                                        &#x27;n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001858A71BB90&gt;},\n",
       "                   refit=False, scoring=[&#x27;precision&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;],\n",
       "                   verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3, estimator=LGBMClassifier(objective=&#x27;binary&#x27;),\n",
       "                   param_distributions={&#x27;class_weight&#x27;: [&#x27;balanced&#x27;,\n",
       "                                                         {0: 1.0, 1: 1.0},\n",
       "                                                         {0: 5.0, 1: 1.0},\n",
       "                                                         {0: 1.0, 1: 1.0},\n",
       "                                                         {0: 10.0, 1: 1.0}],\n",
       "                                        &#x27;n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001858A71BB90&gt;},\n",
       "                   refit=False, scoring=[&#x27;precision&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;],\n",
       "                   verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(objective=&#x27;binary&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(objective=&#x27;binary&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LGBMClassifier(objective='binary'),\n",
       "                   param_distributions={'class_weight': ['balanced',\n",
       "                                                         {0: 1.0, 1: 1.0},\n",
       "                                                         {0: 5.0, 1: 1.0},\n",
       "                                                         {0: 1.0, 1: 1.0},\n",
       "                                                         {0: 10.0, 1: 1.0}],\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001858A71BB90>},\n",
       "                   refit=False, scoring=['precision', 'recall', 'f1'],\n",
       "                   verbose=3)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm = lgb.LGBMClassifier(objective='binary')\n",
    "\n",
    "lgb_cv = RandomizedSearchCV(lgbm, {'class_weight':['balanced', {0: 1.0, 1: 1.0}, {0: 5.0, 1: 1.0}, {0: 1.0, 1: 1.0}, {0: 10.0, 1: 1.0}],\n",
    "                            'n_estimators': randint(50,500)},\\\n",
    "                            n_iter=10, cv = 3, verbose=3, \\\n",
    "                            scoring = ['precision','recall','f1'], refit=False)\n",
    "\n",
    "lgb_cv.fit(X_tr_vec, y_tr)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7ddc7191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_precision</th>\n",
       "      <th>split1_test_precision</th>\n",
       "      <th>split2_test_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>rank_test_precision</th>\n",
       "      <th>split0_test_recall</th>\n",
       "      <th>split1_test_recall</th>\n",
       "      <th>split2_test_recall</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>rank_test_recall</th>\n",
       "      <th>split0_test_f1</th>\n",
       "      <th>split1_test_f1</th>\n",
       "      <th>split2_test_f1</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.408356</td>\n",
       "      <td>0.773538</td>\n",
       "      <td>0.953348</td>\n",
       "      <td>0.031670</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>460</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.955298</td>\n",
       "      <td>0.958085</td>\n",
       "      <td>0.959033</td>\n",
       "      <td>0.957472</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>10</td>\n",
       "      <td>0.956882</td>\n",
       "      <td>0.947844</td>\n",
       "      <td>0.957796</td>\n",
       "      <td>0.954174</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956089</td>\n",
       "      <td>0.952937</td>\n",
       "      <td>0.958414</td>\n",
       "      <td>0.955813</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.720309</td>\n",
       "      <td>0.909609</td>\n",
       "      <td>0.827274</td>\n",
       "      <td>0.063750</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>409</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.956819</td>\n",
       "      <td>0.958925</td>\n",
       "      <td>0.959157</td>\n",
       "      <td>0.958300</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>9</td>\n",
       "      <td>0.955408</td>\n",
       "      <td>0.946554</td>\n",
       "      <td>0.956506</td>\n",
       "      <td>0.952822</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>2</td>\n",
       "      <td>0.956113</td>\n",
       "      <td>0.952699</td>\n",
       "      <td>0.957830</td>\n",
       "      <td>0.955547</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.513864</td>\n",
       "      <td>0.853734</td>\n",
       "      <td>0.747392</td>\n",
       "      <td>0.029642</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>381</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.957545</td>\n",
       "      <td>0.959753</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.958919</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>8</td>\n",
       "      <td>0.951723</td>\n",
       "      <td>0.944895</td>\n",
       "      <td>0.955216</td>\n",
       "      <td>0.950611</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>3</td>\n",
       "      <td>0.954625</td>\n",
       "      <td>0.952266</td>\n",
       "      <td>0.957333</td>\n",
       "      <td>0.954741</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.077706</td>\n",
       "      <td>1.299400</td>\n",
       "      <td>0.728233</td>\n",
       "      <td>0.207451</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>282</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.959588</td>\n",
       "      <td>0.961669</td>\n",
       "      <td>0.963463</td>\n",
       "      <td>0.961573</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>7</td>\n",
       "      <td>0.945089</td>\n",
       "      <td>0.938629</td>\n",
       "      <td>0.947659</td>\n",
       "      <td>0.943793</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>4</td>\n",
       "      <td>0.952284</td>\n",
       "      <td>0.950009</td>\n",
       "      <td>0.955496</td>\n",
       "      <td>0.952596</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.023707</td>\n",
       "      <td>1.039626</td>\n",
       "      <td>0.337143</td>\n",
       "      <td>0.005560</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>145</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.964374</td>\n",
       "      <td>0.966647</td>\n",
       "      <td>0.966997</td>\n",
       "      <td>0.966006</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>4</td>\n",
       "      <td>0.932744</td>\n",
       "      <td>0.924069</td>\n",
       "      <td>0.934206</td>\n",
       "      <td>0.930340</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>6</td>\n",
       "      <td>0.948295</td>\n",
       "      <td>0.944879</td>\n",
       "      <td>0.950319</td>\n",
       "      <td>0.947831</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.696121</td>\n",
       "      <td>0.353304</td>\n",
       "      <td>0.667800</td>\n",
       "      <td>0.014865</td>\n",
       "      <td>balanced</td>\n",
       "      <td>388</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 388}</td>\n",
       "      <td>0.960516</td>\n",
       "      <td>0.964416</td>\n",
       "      <td>0.965169</td>\n",
       "      <td>0.963367</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>6</td>\n",
       "      <td>0.932375</td>\n",
       "      <td>0.924069</td>\n",
       "      <td>0.934574</td>\n",
       "      <td>0.930340</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>5</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>0.943812</td>\n",
       "      <td>0.949625</td>\n",
       "      <td>0.946558</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.755256</td>\n",
       "      <td>1.178632</td>\n",
       "      <td>0.662893</td>\n",
       "      <td>0.068948</td>\n",
       "      <td>balanced</td>\n",
       "      <td>319</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 319}</td>\n",
       "      <td>0.961649</td>\n",
       "      <td>0.966105</td>\n",
       "      <td>0.966520</td>\n",
       "      <td>0.964758</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>5</td>\n",
       "      <td>0.928690</td>\n",
       "      <td>0.919278</td>\n",
       "      <td>0.931073</td>\n",
       "      <td>0.926347</td>\n",
       "      <td>0.005092</td>\n",
       "      <td>7</td>\n",
       "      <td>0.944882</td>\n",
       "      <td>0.942110</td>\n",
       "      <td>0.948465</td>\n",
       "      <td>0.945152</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.837220</td>\n",
       "      <td>0.732855</td>\n",
       "      <td>0.221988</td>\n",
       "      <td>0.014651</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>80</td>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "      <td>0.967516</td>\n",
       "      <td>0.972731</td>\n",
       "      <td>0.971006</td>\n",
       "      <td>0.970418</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>3</td>\n",
       "      <td>0.916528</td>\n",
       "      <td>0.900663</td>\n",
       "      <td>0.919646</td>\n",
       "      <td>0.912279</td>\n",
       "      <td>0.008312</td>\n",
       "      <td>8</td>\n",
       "      <td>0.941332</td>\n",
       "      <td>0.935311</td>\n",
       "      <td>0.944628</td>\n",
       "      <td>0.940424</td>\n",
       "      <td>0.003858</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.882831</td>\n",
       "      <td>0.674751</td>\n",
       "      <td>0.238767</td>\n",
       "      <td>0.036211</td>\n",
       "      <td>balanced</td>\n",
       "      <td>112</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 112}</td>\n",
       "      <td>0.973376</td>\n",
       "      <td>0.975896</td>\n",
       "      <td>0.973854</td>\n",
       "      <td>0.974375</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>2</td>\n",
       "      <td>0.889257</td>\n",
       "      <td>0.873019</td>\n",
       "      <td>0.892370</td>\n",
       "      <td>0.884882</td>\n",
       "      <td>0.008484</td>\n",
       "      <td>9</td>\n",
       "      <td>0.929417</td>\n",
       "      <td>0.921595</td>\n",
       "      <td>0.931333</td>\n",
       "      <td>0.927449</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.054953</td>\n",
       "      <td>0.531393</td>\n",
       "      <td>0.164447</td>\n",
       "      <td>0.035959</td>\n",
       "      <td>balanced</td>\n",
       "      <td>66</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 66}</td>\n",
       "      <td>0.976176</td>\n",
       "      <td>0.978814</td>\n",
       "      <td>0.978351</td>\n",
       "      <td>0.977780</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>1</td>\n",
       "      <td>0.868251</td>\n",
       "      <td>0.851456</td>\n",
       "      <td>0.874493</td>\n",
       "      <td>0.864733</td>\n",
       "      <td>0.009728</td>\n",
       "      <td>10</td>\n",
       "      <td>0.919056</td>\n",
       "      <td>0.910704</td>\n",
       "      <td>0.923511</td>\n",
       "      <td>0.917757</td>\n",
       "      <td>0.005309</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      13.408356      0.773538         0.953348        0.031670   \n",
       "1      12.720309      0.909609         0.827274        0.063750   \n",
       "2      11.513864      0.853734         0.747392        0.029642   \n",
       "3      10.077706      1.299400         0.728233        0.207451   \n",
       "4       8.023707      1.039626         0.337143        0.005560   \n",
       "5      10.696121      0.353304         0.667800        0.014865   \n",
       "6      10.755256      1.178632         0.662893        0.068948   \n",
       "7       5.837220      0.732855         0.221988        0.014651   \n",
       "8       5.882831      0.674751         0.238767        0.036211   \n",
       "9       5.054953      0.531393         0.164447        0.035959   \n",
       "\n",
       "  param_class_weight param_n_estimators  \\\n",
       "0   {0: 5.0, 1: 1.0}                460   \n",
       "1   {0: 5.0, 1: 1.0}                409   \n",
       "2   {0: 5.0, 1: 1.0}                381   \n",
       "3   {0: 5.0, 1: 1.0}                282   \n",
       "4   {0: 5.0, 1: 1.0}                145   \n",
       "5           balanced                388   \n",
       "6           balanced                319   \n",
       "7   {0: 5.0, 1: 1.0}                 80   \n",
       "8           balanced                112   \n",
       "9           balanced                 66   \n",
       "\n",
       "                                              params  split0_test_precision  \\\n",
       "0  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.955298   \n",
       "1  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.956819   \n",
       "2  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.957545   \n",
       "3  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.959588   \n",
       "4  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.964374   \n",
       "5  {'class_weight': 'balanced', 'n_estimators': 388}               0.960516   \n",
       "6  {'class_weight': 'balanced', 'n_estimators': 319}               0.961649   \n",
       "7  {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...               0.967516   \n",
       "8  {'class_weight': 'balanced', 'n_estimators': 112}               0.973376   \n",
       "9   {'class_weight': 'balanced', 'n_estimators': 66}               0.976176   \n",
       "\n",
       "   split1_test_precision  split2_test_precision  mean_test_precision  \\\n",
       "0               0.958085               0.959033             0.957472   \n",
       "1               0.958925               0.959157             0.958300   \n",
       "2               0.959753               0.959459             0.958919   \n",
       "3               0.961669               0.963463             0.961573   \n",
       "4               0.966647               0.966997             0.966006   \n",
       "5               0.964416               0.965169             0.963367   \n",
       "6               0.966105               0.966520             0.964758   \n",
       "7               0.972731               0.971006             0.970418   \n",
       "8               0.975896               0.973854             0.974375   \n",
       "9               0.978814               0.978351             0.977780   \n",
       "\n",
       "   std_test_precision  rank_test_precision  split0_test_recall  \\\n",
       "0            0.001585                   10            0.956882   \n",
       "1            0.001052                    9            0.955408   \n",
       "2            0.000979                    8            0.951723   \n",
       "3            0.001583                    7            0.945089   \n",
       "4            0.001163                    4            0.932744   \n",
       "5            0.002039                    6            0.932375   \n",
       "6            0.002205                    5            0.928690   \n",
       "7            0.002169                    3            0.916528   \n",
       "8            0.001093                    2            0.889257   \n",
       "9            0.001150                    1            0.868251   \n",
       "\n",
       "   split1_test_recall  split2_test_recall  mean_test_recall  std_test_recall  \\\n",
       "0            0.947844            0.957796          0.954174         0.004492   \n",
       "1            0.946554            0.956506          0.952822         0.004455   \n",
       "2            0.944895            0.955216          0.950611         0.004286   \n",
       "3            0.938629            0.947659          0.943793         0.003799   \n",
       "4            0.924069            0.934206          0.930340         0.004474   \n",
       "5            0.924069            0.934574          0.930340         0.004524   \n",
       "6            0.919278            0.931073          0.926347         0.005092   \n",
       "7            0.900663            0.919646          0.912279         0.008312   \n",
       "8            0.873019            0.892370          0.884882         0.008484   \n",
       "9            0.851456            0.874493          0.864733         0.009728   \n",
       "\n",
       "   rank_test_recall  split0_test_f1  split1_test_f1  split2_test_f1  \\\n",
       "0                 1        0.956089        0.952937        0.958414   \n",
       "1                 2        0.956113        0.952699        0.957830   \n",
       "2                 3        0.954625        0.952266        0.957333   \n",
       "3                 4        0.952284        0.950009        0.955496   \n",
       "4                 6        0.948295        0.944879        0.950319   \n",
       "5                 5        0.946237        0.943812        0.949625   \n",
       "6                 7        0.944882        0.942110        0.948465   \n",
       "7                 8        0.941332        0.935311        0.944628   \n",
       "8                 9        0.929417        0.921595        0.931333   \n",
       "9                10        0.919056        0.910704        0.923511   \n",
       "\n",
       "   mean_test_f1  std_test_f1  rank_test_f1  \n",
       "0      0.955813     0.002245             1  \n",
       "1      0.955547     0.002132             2  \n",
       "2      0.954741     0.002070             3  \n",
       "3      0.952596     0.002251             4  \n",
       "4      0.947831     0.002245             5  \n",
       "5      0.946558     0.002384             6  \n",
       "6      0.945152     0.002602             7  \n",
       "7      0.940424     0.003858             8  \n",
       "8      0.927449     0.004212             9  \n",
       "9      0.917757     0.005309            10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>13.408356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.773538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.953348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.03167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_class_weight</th>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_n_estimators</th>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.955298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.958085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.959033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.957472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.001585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.956882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.947844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.957796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.954174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.004492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.956089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.952937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.958414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.955813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.002245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       0\n",
       "mean_fit_time                                                  13.408356\n",
       "std_fit_time                                                    0.773538\n",
       "mean_score_time                                                 0.953348\n",
       "std_score_time                                                   0.03167\n",
       "param_class_weight                                      {0: 5.0, 1: 1.0}\n",
       "param_n_estimators                                                   460\n",
       "params                 {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimato...\n",
       "split0_test_precision                                           0.955298\n",
       "split1_test_precision                                           0.958085\n",
       "split2_test_precision                                           0.959033\n",
       "mean_test_precision                                             0.957472\n",
       "std_test_precision                                              0.001585\n",
       "rank_test_precision                                                   10\n",
       "split0_test_recall                                              0.956882\n",
       "split1_test_recall                                              0.947844\n",
       "split2_test_recall                                              0.957796\n",
       "mean_test_recall                                                0.954174\n",
       "std_test_recall                                                 0.004492\n",
       "rank_test_recall                                                       1\n",
       "split0_test_f1                                                  0.956089\n",
       "split1_test_f1                                                  0.952937\n",
       "split2_test_f1                                                  0.958414\n",
       "mean_test_f1                                                    0.955813\n",
       "std_test_f1                                                     0.002245\n",
       "rank_test_f1                                                           1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lgbm_cvdf = pd.DataFrame(lgb_cv.cv_results_).sort_values(by='rank_test_f1').reset_index(drop=True)\n",
    "lgbm_best = pd.DataFrame(lgbm_cvdf.iloc[0,:])\n",
    "display(lgbm_cvdf)\n",
    "display(lgbm_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2af18d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           0\n",
      "params               {'class_weight': {0: 5.0, 1: 1.0}, 'n_estimators': 460}\n",
      "mean_test_precision                                                 0.957472\n",
      "mean_test_recall                                                    0.954174\n",
      "mean_test_f1                                                        0.955813\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(lgbm_best.loc[['params', 'mean_test_precision','mean_test_recall','mean_test_f1'],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9a7a2b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(class_weight='balanced', max_iter=5000)\n",
    "\n",
    "svc.fit(X_tr_vec, y_tr)\n",
    "\n",
    "y_pr = svc.predict(X_te_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "71b862b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.56      0.50       515\n",
      "           1       0.96      0.94      0.95      5426\n",
      "\n",
      "    accuracy                           0.90      5941\n",
      "   macro avg       0.70      0.75      0.72      5941\n",
      "weighted avg       0.91      0.90      0.91      5941\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAGwCAYAAACZ7H64AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA2ZUlEQVR4nO3de1hVZd7/8c+WwxZRd6ICYlqaZJhmhjOI5SkPaRkxTaMNDWOTp7JUUtNx7GAnMCvxgJpZqZmO9su0pjHSTpajeBopNc1MPKWIFqIgbgj37w+n9bQFXVB7sZDer+da1xVr3fve9+a6mOfj97vutR0ej8cjAAAAG9WwewEAAAAEEgAAYDsCCQAAsB2BBAAA2I5AAgAAbEcgAQAAtiOQAAAA2xFIAACA7fztXoAVThfxrDegLDVqOOxeAlDl1KyE/08Y1O4hn8xTuDXNJ/NURVRIAACA7QgkAABYzVHDN0cFTJw4UQ6Hw+sIDw83rns8Hk2cOFEREREKCgpS165dtWPHDq853G63hg8frgYNGig4OFhxcXE6dOiQ15jc3FwlJibK5XLJ5XIpMTFRJ06cqPCviEACAIDVHA7fHBV07bXX6siRI8axbds249rkyZM1ZcoUpaWladOmTQoPD1fPnj116tQpY0xSUpKWL1+uJUuWaO3atcrPz1ffvn1VUlJijElISFBmZqbS09OVnp6uzMxMJSYmVnit1fIeEgAAqpQKVjd8xd/f36sq8hOPx6OpU6dqwoQJuvPOOyVJCxYsUFhYmBYvXqyhQ4cqLy9Pr776qhYuXKgePXpIkt544w01adJEH374oW655Rbt3LlT6enpysjIUExMjCRp7ty5io2N1ddff62WLVuWe61USAAAuES43W6dPHnS63C73Rcc/8033ygiIkLNmjXT3Xffrb1790qSsrKylJ2drV69ehljnU6nunTponXr1kmStmzZouLiYq8xERERat26tTFm/fr1crlcRhiRpA4dOsjlchljyotAAgCA1XzUsklJSTHu1fjpSElJKfMtY2Ji9Prrr+uDDz7Q3LlzlZ2drY4dO+r7779Xdna2JCksLMzrNWFhYca17OxsBQYGql69ehcdExoaWuq9Q0NDjTHlRcsGAACr+ahlM378eI0aNcrrnNPpLHNsnz59jP9u06aNYmNjddVVV2nBggXq0KHDuWWdd1+Kx+Mpde58548pa3x55jkfFRIAAC4RTqdTdevW9TouFEjOFxwcrDZt2uibb74x7is5v4qRk5NjVE3Cw8NVVFSk3Nzci445evRoqfc6duxYqeqLGQIJAABWs2mXzc+53W7t3LlTjRo1UrNmzRQeHq7Vq1cb14uKirRmzRp17NhRkhQdHa2AgACvMUeOHNH27duNMbGxscrLy9PGjRuNMRs2bFBeXp4xprxo2QAAYDUbdtmMGTNGt99+u5o2baqcnBw988wzOnnypAYMGCCHw6GkpCQlJycrMjJSkZGRSk5OVq1atZSQkCBJcrlcGjhwoEaPHq369esrJCREY8aMUZs2bYxdN1FRUerdu7cGDx6sOXPmSJKGDBmivn37VmiHjUQgAQCgWjp06JD+/Oc/6/jx42rYsKE6dOigjIwMXXHFFZKksWPHqrCwUMOGDVNubq5iYmK0atUq1alTx5gjNTVV/v7+6tevnwoLC9W9e3fNnz9ffn5+xphFixZpxIgRxm6cuLg4paVV/BH3Do/HU+2++IXvsgHKxnfZAKVVynfZxP7dJ/MUrp/kk3mqIiokAABYzaYHo11K+A0BAADbUSEBAMBqv3KHzG8BgQQAAKvRsjFFIAEAwGpUSEwR2QAAgO2okAAAYDVaNqYIJAAAWI1AYorfEAAAsB0VEgAArMZTkk0RSAAAsBotG1P8hgAAgO2okAAAYDWeQ2KKQAIAgNVo2ZjiNwQAAGxHhQQAAKvRsjFFIAEAwGq0bEwRSAAAsBoVElNENgAAYDsqJAAAWI2WjSkCCQAAVqNlY4rIBgAAbEeFBAAAq9GyMUUgAQDAarRsTBHZAACA7aiQAABgNVo2pggkAABYjUBiit8QAACwHRUSAACsxk2tpggkAABYjZaNKQIJAABWo0JiisgGAABsR4UEAACr0bIxRSABAMBqtGxMEdkAAIDtqJAAAGAxBxUSUwQSAAAsRiAxR8sGAADYjgoJAABWo0BiikACAIDFaNmYo2UDAABsR4UEAACLUSExRyABAMBiBBJzBBIAACxGIDHHPSQAAMB2VEgAALAaBRJTBBIAACxGy8YcLRsAAGA7KiQAAFiMCok5AgkAABYjkJijZQMAAGxHhQQAAItRITFHIAEAwGrkEVO0bAAAgO2okAAAYDFaNuYIJAAAWIxAYo5AAgCAxQgk5riHBAAA2I4KCQAAVqNAYopAAgCAxWjZmKNlAwAAbEeFBAAAi1EhMUcgAQDAYgQSc7RsAACA7aiQAABgMSok5qiQAABgNYePjl8hJSVFDodDSUlJxjmPx6OJEycqIiJCQUFB6tq1q3bs2OH1OrfbreHDh6tBgwYKDg5WXFycDh065DUmNzdXiYmJcrlccrlcSkxM1IkTJyq0PgIJAADV3KZNm/Tyyy/ruuuu8zo/efJkTZkyRWlpadq0aZPCw8PVs2dPnTp1yhiTlJSk5cuXa8mSJVq7dq3y8/PVt29flZSUGGMSEhKUmZmp9PR0paenKzMzU4mJiRVaI4EEAACLORwOnxy/RH5+vu655x7NnTtX9erVM857PB5NnTpVEyZM0J133qnWrVtrwYIFOn36tBYvXixJysvL06uvvqoXX3xRPXr0ULt27fTGG29o27Zt+vDDDyVJO3fuVHp6ul555RXFxsYqNjZWc+fO1Xvvvaevv/663OskkAAAYDFfBRK3262TJ096HW63+6Lv/eCDD+q2225Tjx49vM5nZWUpOztbvXr1Ms45nU516dJF69atkyRt2bJFxcXFXmMiIiLUunVrY8z69evlcrkUExNjjOnQoYNcLpcxpjwIJAAAWMxXgSQlJcW4T+OnIyUl5YLvu2TJEv33v/8tc0x2drYkKSwszOt8WFiYcS07O1uBgYFelZWyxoSGhpaaPzQ01BhTHuyyAQDgEjF+/HiNGjXK65zT6Sxz7MGDBzVy5EitWrVKNWvWvOCc57eCPB6PaXvo/DFljS/PPD9HhQQAAKv5aJeN0+lU3bp1vY4LBZItW7YoJydH0dHR8vf3l7+/v9asWaPp06fL39/fqIycX8XIyckxroWHh6uoqEi5ubkXHXP06NFS73/s2LFS1ZeLIZAAAGAxO25q7d69u7Zt26bMzEzjaN++ve655x5lZmaqefPmCg8P1+rVq43XFBUVac2aNerYsaMkKTo6WgEBAV5jjhw5ou3btxtjYmNjlZeXp40bNxpjNmzYoLy8PGNMedCyAQCgGqpTp45at27tdS44OFj169c3ziclJSk5OVmRkZGKjIxUcnKyatWqpYSEBEmSy+XSwIEDNXr0aNWvX18hISEaM2aM2rRpY9wkGxUVpd69e2vw4MGaM2eOJGnIkCHq27evWrZsWe71EkhQIa++Mkcff7ha+7L2ylmzptq2baeRD4/Wlc2aG2NOny7Q9NQX9cnHHykv74QiIhrr7nsS1a//n40xBw8eUOoLk7V16xYVFxWp442dNG78o6rfoIEdHwv41V6dO0cfrV6lrP/9bVx/fTsljRpj/G0UFxcrbfpUrf38Mx06dFB1atdWTGxHjXx4tEJDvcvaX2Ru1Yxpqdq27UsF+Pur5TVRmvnS3IveB4Cqrao+qXXs2LEqLCzUsGHDlJubq5iYGK1atUp16tQxxqSmpsrf31/9+vVTYWGhunfvrvnz58vPz88Ys2jRIo0YMcLYjRMXF6e0tLQKrcXh8Xg8vvlYVcfpomr3kaqMB+8fpFt636prW7fRjyUlmjk9Vd98843eXvGegmrVkiQ9NfExbd64QY8/+bQiIhpr/br/KOXZp/T8lOnqdnN3FZ4+rX5/vENXt7xG9w97SJI0K226jh3L0euLlqpGDTqJVqlRo2r+j2J18MCQgerd5zZd26aNSn4s0Yzpqdqze7fefvffqlWrlk6dOqUxD4/QnXf9SS1bXqOTJ09q8qRklZT8qH+++bYxzxeZWzVs6CDdN2iounTrpoCAAO3etUtdut2swMBAGz9h9VWzEv5pfuXI93wyz75pfX0yT1VEIMGv8sMPP6h7l456Zd5CRbf/nSTprj/crl639NGQ+4cZ4xL63akbO3XRg8NHav26tXrogSFa85+Nql27tiTpZF6eutwUo9kvv6YOseXvOaJiCCSV54cfflC3TrF6bcEbxt/G+bZv+1L33P0npa/+RI0iIiRJf/lzP3WI7aiHRiRV4mp/2wgkVQP/FMWvkp9/7vHCLpfLOHd9uxu05tOPlXP0qDwejzZtzND+/fvU8cabJJ27acrhcHj9ay/Q6VSNGjWUuXVL5X4AwCL5/3v0dt2f/W2UGpOfL4fDoTp160qSvv/+e2378guF1K+vv95zt7p17qj7BvxF/92yuVLWDOvY+aTWS4WtgeTQoUOaMGGCunXrpqioKLVq1UrdunXThAkTdPDgQTuXhnLweDx68flJandDtFpEXm2cHzd+gppfdZVu6dFFv7+hjR68f7DGP/qE2t0QLUlqc931CgoK0rTUF1RYWKjC06c19cXJOnv2rI4fO2bXxwF8xuPx6IXJKWp3Q7Qif/a38XNut1vTUl9Qn9v6GpXC7w6d+9+9l2am6c67/qRZc15RVFQrDRl4r/bv31dZy4cVqsCX61V1tt3UunbtWvXp00dNmjRRr1691KtXL3k8HuXk5GjFihWaMWOG3n//fd14440Xncftdpd6bG6JI/CC+7LhO5OefVrf7P5a8xYs9jr/z0ULte3LLzR1xiw1atRY/92ySSnPPKkGDRqqQ2xHhYSEaPKLU5X89JP656KFqlGjhnr3uU1RUa1U42c3SQGXqpRnntI3u3dr/sLFZV4vLi7WuDEP6+xZjyY8NtE4f/bsWUnSXf36K/4Pf5QkRUW10oYN67Xi7WUa+fBoy9cO2MW2QPLwww9r0KBBSk1NveD1pKQkbdq06aLzpKSk6Mknn/Q6949HH/f6I4fvTUp+Wms+/Vivzn9DYeHhxvkzZ85oxrSpmjJthjp17ipJurplS3399S4tXPB/94fEdrxJ/3p/tXJzc+Xv56c6deuqR9eb1Ljx5XZ8HMBnUp59Wp9++rFeW+D9t/GT4uJiPTI6Sd8dOqS58xYY1RFJatCwoSSp+VVXeb2mWfOrlH3ksLULh6Wqe7vFF2wLJNu3b9cbb7xxwetDhw7VSy+9ZDpPWY/RLXFwJ7pVPB6Pnkt+Wh9//KHmvva6Gl/uHSB+/PFH/fhjsRwO726gX40axr/+fu6n70fYuCFDP/zwvbp07Wbd4gELeTwepTz7tD7+aLVenb9Ql1/epNSYn8LIgf379cq813XZZd7fD9K48eVqGBqqfVlZXuf379unmzp1tnT9sBaBxJxtgaRRo0Zat27dBR+asn79ejVq1Mh0HqfTWao9wy4b66Q8+5TeX/meUqfNVHBwsI4fP3fPR+3adVSzZk3Vrl1b0e1/p6lTnlfNmk41atRYWzZv1Hv/ekejHvm7Mc87y5epWfOrVC8kRF9mZur5557VPYkDvJ5nAlxKkp9+Uu+vfE9TZ8xScK1g436o2nXO/W38+OOPGvPwCO3c+ZVmzJyjsyUlxhiXy6WAwEA5HA7d+7eBmj1zhlq2vEYtr4nSu+8s176svXoxdbqdHw+/EnnEnG3bfmfNmqWHH35YgwcPVs+ePRUWFiaHw6Hs7GytXr1ar7zyiqZOnar777+/wnMTSKzTrs01ZZ5/8ulkxcXfKUk6fvyYZkydovXr/6OTeXlq1ChCd97VT3/5673GvxKmpb6of72zXHl5eYpoHKG7/nS313VYg22/1ml7bdn/uHrqmRTd8Yc79d13h3Rrr+5ljnll3uv63e//76vbX537spYuWaS8vDy1bHmNkkaN0Q3R7S1ZNypn22+LMe/7ZJ49L/TxyTxVka3PIVm6dKlSU1O1ZcsWlZSUSJL8/PwUHR2tUaNGqV+/fr9oXgIJUDYCCVBaZQSSyEfSfTLPN8/39sk8VVGVeDBacXGxjh8/Lklq0KCBAgICftV8BBKgbAQSoLTKCCRXj/VNINk9ufoGkirxXTYBAQHlul8EAABUT1UikAAAUJ1xf5w5AgkAABYjj5jju2wAAIDtqJAAAGAxbig3RyABAMBitGzM0bIBAAC2o0ICAIDF2GVjjkACAIDFyCPmCCQAAFiMCok57iEBAAC2o0ICAIDFqJCYI5AAAGAx8og5WjYAAMB2VEgAALAYLRtzBBIAACxGHjFHywYAANiOCgkAABajZWOOQAIAgMXII+Zo2QAAANtRIQEAwGK0bMwRSAAAsBh5xByBBAAAi1EhMcc9JAAAwHZUSAAAsBgFEnMEEgAALEbLxhwtGwAAYDsqJAAAWIwCiTkCCQAAFqNlY46WDQAAsB0VEgAALEaBxByBBAAAi9GyMUfLBgAA2I4KCQAAFqNCYo5AAgCAxcgj5ggkAABYjAqJOe4hAQAAtqNCAgCAxSiQmCOQAABgMVo25mjZAAAA21EhAQDAYhRIzBFIAACwWA0SiSlaNgAAwHZUSAAAsBgFEnMEEgAALMYuG3MEEgAALFaDPGKKe0gAAIDtqJAAAGAxWjbmCCQAAFiMPGKOlg0AALAdFRIAACzmECUSMwQSAAAsxi4bc7RsAACA7aiQAABgMXbZmCOQAABgMfKIOVo2AADAdgQSAAAsVsPh8MlREbNnz9Z1112nunXrqm7duoqNjdX7779vXPd4PJo4caIiIiIUFBSkrl27aseOHV5zuN1uDR8+XA0aNFBwcLDi4uJ06NAhrzG5ublKTEyUy+WSy+VSYmKiTpw4UfHfUYVfAQAAKsTh8M1REZdffrkmTZqkzZs3a/Pmzbr55pt1xx13GKFj8uTJmjJlitLS0rRp0yaFh4erZ8+eOnXqlDFHUlKSli9friVLlmjt2rXKz89X3759VVJSYoxJSEhQZmam0tPTlZ6erszMTCUmJlb8d+TxeDwVflUVd7qo2n0kwCdqsPcQKKVmJdxNede8//pknkUJ18rtdnudczqdcjqd5Xp9SEiInn/+ed13332KiIhQUlKSxo0bJ+lcNSQsLEzPPfechg4dqry8PDVs2FALFy5U//79JUmHDx9WkyZNtHLlSt1yyy3auXOnWrVqpYyMDMXExEiSMjIyFBsbq127dqlly5bl/mxUSAAAuESkpKQYrZGfjpSUFNPXlZSUaMmSJSooKFBsbKyysrKUnZ2tXr16GWOcTqe6dOmidevWSZK2bNmi4uJirzERERFq3bq1MWb9+vVyuVxGGJGkDh06yOVyGWPKi102AABYzFe7bMaPH69Ro0Z5nbtYdWTbtm2KjY3VmTNnVLt2bS1fvlytWrUywkJYWJjX+LCwMO3fv1+SlJ2drcDAQNWrV6/UmOzsbGNMaGhoqfcNDQ01xpQXgQQAAItV9IbUC6lIe0aSWrZsqczMTJ04cULLli3TgAEDtGbNGuP6+c9H8Xg8ps9MOX9MWePLM8/5aNkAAFBNBQYGqkWLFmrfvr1SUlLUtm1bTZs2TeHh4ZJUqoqRk5NjVE3Cw8NVVFSk3Nzci445evRoqfc9duxYqeqLGQIJAAAWc/jo+LU8Ho/cbreaNWum8PBwrV692rhWVFSkNWvWqGPHjpKk6OhoBQQEeI05cuSItm/fboyJjY1VXl6eNm7caIzZsGGD8vLyjDHlRcsGAACL2fHo+H/84x/q06ePmjRpolOnTmnJkiX69NNPlZ6eLofDoaSkJCUnJysyMlKRkZFKTk5WrVq1lJCQIElyuVwaOHCgRo8erfr16yskJERjxoxRmzZt1KNHD0lSVFSUevfurcGDB2vOnDmSpCFDhqhv374V2mEjEUgAAKiWjh49qsTERB05ckQul0vXXXed0tPT1bNnT0nS2LFjVVhYqGHDhik3N1cxMTFatWqV6tSpY8yRmpoqf39/9evXT4WFherevbvmz58vPz8/Y8yiRYs0YsQIYzdOXFyc0tLSKrxenkMC/IbwHBKgtMp4Dsk9CzN9Ms+ixOt9Mk9VRIUEAACL8W2/5soVSN59991yTxgXF/eLFwMAAH6byhVI4uPjyzWZw+Hwer49AADw3YPRqrNyBZKzZ89avQ4AAKotWjbmuIcEAACLcT+5uV8USAoKCrRmzRodOHBARUVFXtdGjBjhk4UBAIDfjgoHkq1bt+rWW2/V6dOnVVBQoJCQEB0/fly1atVSaGgogQQAgPPQsjFX4UfHP/zww7r99tv1ww8/KCgoSBkZGdq/f7+io6P1wgsvWLFGAAAuaVXl0fFVWYUDSWZmpkaPHi0/Pz/5+fnJ7XarSZMmmjx5sv7xj39YsUYAAFDNVTiQBAQEGKWnsLAwHThwQNK5Z97/9N8AAOD/1HA4fHJUZxW+h6Rdu3bavHmzrr76anXr1k2PP/64jh8/roULF6pNmzZWrBEAgEtaNc8SPlHhCklycrIaNWokSXr66adVv359PfDAA8rJydHLL7/s8wUCAIDqr8IVkvbt2xv/3bBhQ61cudKnCwIAoLphl405HowGAIDFyCPmKhxImjVrdtGkt3fv3l+1IAAA8NtT4UCSlJTk9XNxcbG2bt2q9PR0PfLII75aFwAA1UZ13yHjCxUOJCNHjizz/MyZM7V58+ZfvSAAAKob8oi5Cu+yuZA+ffpo2bJlvpoOAIBqw+Fw+OSoznwWSN566y2FhIT4ajoAAPAb8osejPbzlObxeJSdna1jx45p1qxZPl3cL1Xi8di9BKBKqv+74XYvAahyCremWf4ePvvXfzVW4UByxx13eAWSGjVqqGHDhuratauuueYany4OAIDqoLq3W3yhwoFk4sSJFiwDAAD8llW4iuTn56ecnJxS57///nv5+fn5ZFEAAFQnNRy+OaqzCldIPBe4P8PtdiswMPBXLwgAgOqmuocJXyh3IJk+fbqkc32wV155RbVr1zaulZSU6LPPPuMeEgAA8IuUO5CkpqZKOlcheemll7zaM4GBgbryyiv10ksv+X6FAABc4rip1Vy5A0lWVpYkqVu3bnr77bdVr149yxYFAEB1QsvGXIXvIfnkk0+sWAcAAPgNq/Aum7vuukuTJk0qdf7555/Xn/70J58sCgCA6sTh8M1RnVU4kKxZs0a33XZbqfO9e/fWZ5995pNFAQBQndRwOHxyVGcVbtnk5+eXub03ICBAJ0+e9MmiAACoTnh0vLkK/45at26tpUuXljq/ZMkStWrVyieLAgAAvy0VrpA89thj+uMf/6hvv/1WN998syTpo48+0uLFi/XWW2/5fIEAAFzqqnm3xScqHEji4uK0YsUKJScn66233lJQUJDatm2rjz/+WHXr1rVijQAAXNKq+/0fvlDhQCJJt912m3Fj64kTJ7Ro0SIlJSXpiy++UElJiU8XCAAAqr9ffJ/Nxx9/rL/85S+KiIhQWlqabr31Vm3evNmXawMAoFpg26+5ClVIDh06pPnz5+u1115TQUGB+vXrp+LiYi1btowbWgEAuACe1Gqu3BWSW2+9Va1atdJXX32lGTNm6PDhw5oxY4aVawMAAL8R5a6QrFq1SiNGjNADDzygyMhIK9cEAEC1wk2t5spdIfn888916tQptW/fXjExMUpLS9OxY8esXBsAANUC95CYK3cgiY2N1dy5c3XkyBENHTpUS5YsUePGjXX27FmtXr1ap06dsnKdAACgGqvwLptatWrpvvvu09q1a7Vt2zaNHj1akyZNUmhoqOLi4qxYIwAAl7QaDt8c1dmverx+y5YtNXnyZB06dEj//Oc/fbUmAACqFYeP/q86+0UPRjufn5+f4uPjFR8f74vpAACoVqp7dcMX+AJCAABgO59USAAAwIVRITFHIAEAwGKO6r5n1wdo2QAAANtRIQEAwGK0bMwRSAAAsBgdG3O0bAAAgO2okAAAYDG+XM8cgQQAAItxD4k5WjYAAMB2VEgAALAYHRtzBBIAACxWo5p/MZ4vEEgAALAYFRJz3EMCAABsR4UEAACLscvGHIEEAACL8RwSc7RsAACA7aiQAABgMQok5ggkAABYjJaNOVo2AADAdlRIAACwGAUSc1RIAACwWA0fHRWRkpKi3/3ud6pTp45CQ0MVHx+vr7/+2muMx+PRxIkTFRERoaCgIHXt2lU7duzwGuN2uzV8+HA1aNBAwcHBiouL06FDh7zG5ObmKjExUS6XSy6XS4mJiTpx4kSF1ksgAQCgGlqzZo0efPBBZWRkaPXq1frxxx/Vq1cvFRQUGGMmT56sKVOmKC0tTZs2bVJ4eLh69uypU6dOGWOSkpK0fPlyLVmyRGvXrlV+fr769u2rkpISY0xCQoIyMzOVnp6u9PR0ZWZmKjExsULrdXg8Hs+v/9hVyyn3WbuXAFRJoR1G2L0EoMop3Jpm+Xss2HzQJ/MMaN/kF7/22LFjCg0N1Zo1a9S5c2d5PB5FREQoKSlJ48aNk3SuGhIWFqbnnntOQ4cOVV5enho2bKiFCxeqf//+kqTDhw+rSZMmWrlypW655Rbt3LlTrVq1UkZGhmJiYiRJGRkZio2N1a5du9SyZctyrY8KCQAAFnP46HC73Tp58qTX4Xa7y7WGvLw8SVJISIgkKSsrS9nZ2erVq5cxxul0qkuXLlq3bp0kacuWLSouLvYaExERodatWxtj1q9fL5fLZYQRSerQoYNcLpcxpjwIJAAAWKyGw+GTIyUlxbhP46cjJSXF9P09Ho9GjRqlm266Sa1bt5YkZWdnS5LCwsK8xoaFhRnXsrOzFRgYqHr16l10TGhoaKn3DA0NNcaUB7tsAAC4RIwfP16jRo3yOud0Ok1f99BDD+nLL7/U2rVrS11znLcFyOPxlDp3vvPHlDW+PPP8HBUSAAAs5quWjdPpVN26db0Os0AyfPhwvfvuu/rkk090+eWXG+fDw8MlqVQVIycnx6iahIeHq6ioSLm5uRcdc/To0VLve+zYsVLVl4shkAAAYDGHwzdHRXg8Hj300EN6++239fHHH6tZs2Ze15s1a6bw8HCtXr3aOFdUVKQ1a9aoY8eOkqTo6GgFBAR4jTly5Ii2b99ujImNjVVeXp42btxojNmwYYPy8vKMMeVBywYAgGrowQcf1OLFi/XOO++oTp06RiXE5XIpKChIDodDSUlJSk5OVmRkpCIjI5WcnKxatWopISHBGDtw4ECNHj1a9evXV0hIiMaMGaM2bdqoR48ekqSoqCj17t1bgwcP1pw5cyRJQ4YMUd++fcu9w0YikAAAYLmK3EvhK7Nnz5Ykde3a1ev8vHnzdO+990qSxo4dq8LCQg0bNky5ubmKiYnRqlWrVKdOHWN8amqq/P391a9fPxUWFqp79+6aP3++/Pz8jDGLFi3SiBEjjN04cXFxSkur2HZqnkMC/IbwHBKgtMp4DsnSrd/5ZJ7+7Rr7ZJ6qiHtIAACA7WjZAABgMTtaNpcaAgkAABYjjpijZQMAAGxHhQQAAIvRsjFHIAEAwGK0I8wRSAAAsBgVEnOENgAAYDsqJAAAWIz6iDkCCQAAFqNjY46WDQAAsB0VEgAALFaDpo0pAgkAABajZWOOlg0AALAdFRIAACzmoGVjikACAIDFaNmYo2UDAABsR4UEAACLscvGHIEEAACL0bIxRyABAMBiBBJz3EMCAABsR4UEAACLse3XHIEEAACL1SCPmKJlAwAAbEeFBAAAi9GyMUcgAQDAYuyyMUfLBgAA2I4KCQAAFqNlY45AAgCAxdhlY46WDQAAsB0VElTIW0v/qbfeXKIjh7+TJDW/qoUGDR2mGzt1liRNfHS83nt3hddrWre5TvMXLTV+fvutN5W+8j19vfMrFRQU6JO1G1Snbt1K+wzArzVh6K169P5bvc5lHz+pZj3/4TVm4B9v1GV1grRp+34lpSzVzr3ZkqSmjUL09cqnypz7nkde1dsfbpUk7fr3k7oior7X9RfmrdJj09/15cdBJaBlY45AggoJDQvXQ0mj1KRJU0nSe+++o9EjH9KiN5fpqhaRkqSON3bS408/a7wmICDAa44zhYXqeGMndbyxk9KmTam8xQM+tGPPYd12/wzj55KzHuO/R9/bQyP+0k1DnnhD3+zP0d8H99a/Xxqu6+KfUv5ptw4dzdWVPcZ7zXffH2/UqAE99cF/dnidf3LWe5r39n+Mn/NPuy36RLASu2zMEUhQIZ27dvP6+cERSVr25hJt+/ILI5AEBAaqQYOGF5wjIXGAJGnzpo3WLRSw2I8lZ3X0+1NlXnswoZsmv/qB3vn4C0nSoMcWav9Hyerfp71eXfYfnT3rKfXauG5t9daqLSooLPI6n19w5oLvg0sHecQc95DgFyspKdEH7/9bhYWndV3b643zWzZvVM8uN+rO23vrmYmP6Yfvv7dvkYBFWjRtqL2rntXO9ybq9Ul/05WNz7VWrmxcX40auvTh+l3G2KLiH/X5lj3q0LZ5mXO1i2qi669pogUr1pe6Nurenjr0yXPKWPJ3jR14iwL8/az5QIDNLvkKidvtltvtXcIsUoCcTqdNK6r+9uzerb8l/llFRW4F1aql56fOUPOrWkiSOt7UST163aLwRhE6/N13emnmdN0/6F69sXSZAgMDbV454Bubtu/ToMcW6pv9OQqtX0d/H9Rbn8wfrei7nlV4g3P3Q+X84F3VyPn+lJo2CilzvgHxsdq594gyvsjyOj9z8afauuugTpw8rfatr9BTw+N0ZeP6GvbUYms+GCxTg56NqSpdITl48KDuu+++i45JSUmRy+XyOl6cPKmSVvjbdEWzK7X4/72teW8s0V397tbER8dr77d7JEm9et+qmzp3VYvIq9W5azdNnzVHB/bv19rPPrV30YAPrfrPV1rxUaZ27DmsTzZ8rT8Mny1J+svtMcYYj8fj9RqHo/Q5SarpDFD/Pu3LrI7MWPSJ1m7Zo+3fHNb85es14tml+tsfOirEFezjTwSrOXx0VGdVOpD88MMPWrBgwUXHjB8/Xnl5eV7H6LF/r6QV/jYFBASqSdMr1Ora1npo5ChdfXVL/XPRwjLHNmgYqkYRjXTgwP5KXiVQeU6fKdKOPYd1VdOGyj5+UpIUVt9751jDkDqlqiaS9Ice16tWzUAtes/8nqqNX56roFzVpIEPVg1ULba2bN599+Jb1/bu3Ws6h9PpLNWeOeU++6vWhYrxeKTioqIyr504kauj2dkXvckVuNQFBvjrmmZh+s/WPdr33fc6cixP3Ttcoy++PiRJCvD3U6foFnp02julXntvfEf9e802Hc/NN32fttc0kSQj9OASUt3LGz5gayCJj4+Xw+Eos4z5Ewd9typl5rRUdbypk8LCG+l0QYE+SF+pLZs3avrsl3X6dIFenjVTN/fsqQYNQnX48HeaNT1Vl11WT9269zTmOH78mL4/flyH/lc12fPNbtUKDlZ4o0ZyuS6z6ZMB5Zfy8B/078+26eCRXIWG1Na4Qb1VJ7imFv1rgyRp5uJP9MjAXtpzIEd7DhzT2IG3qPBMsZa+v9lrnuZNGuimG65S/P9aPj8Xc10z/b7NlVqzabfy8s+o/bVNNXnMH/WvT7/UwezcSvmc8B2eQ2LO1kDSqFEjzZw5U/Hx8WVez8zMVHR0dOUuChf1/Q/H9fiEcTp+7Jhq166jyKuv1vTZL6tD7I06c+aM9uzZrX//6x2dOnVKDRo2UPvfxSj5+SkKDv6/nveyN5dq7kszjZ8H/y1RkvTE08m6/Y4/VPpnAiqqcdhlej3lb6p/WbCO5+Zr47Z96jLgRR04ci4ovDj/Q9V0Bmrq+P6qV7eWNm3fp74PpJV6hsiAO2J1OCfPa0fOT9xFxbqr1w36x9A+cgb468CRH/Ta2+s0ZcHqSvmMQGVzeC5WnrBYXFycrr/+ej31VNlPLPziiy/Url07nT1bsRYMLRugbKEdRti9BKDKKdyaZvl7bNyb55N5ft/c5ZN5qiJbKySPPPKICgoKLni9RYsW+uSTTypxRQAA+B4NG3O2BpJOnTpd9HpwcLC6dOlSSasBAAB2ueQfjAYAQJVHicQUgQQAAIuxy8YcgQQAAIvxBAtzVfpJrQAA4LeBCgkAABajQGKOQAIAgNVIJKZo2QAAANtRIQEAwGLssjFHIAEAwGLssjFHywYAANiOCgkAABajQGKOQAIAgNVIJKZo2QAAANtRIQEAwGLssjFHIAEAwGLssjFHIAEAwGLkEXPcQwIAAGxHhQQAAKtRIjFFIAEAwGLc1GqOlg0AALAdFRIAACzGLhtzBBIAACxGHjFHywYAgGrqs88+0+23366IiAg5HA6tWLHC67rH49HEiRMVERGhoKAgde3aVTt27PAa43a7NXz4cDVo0EDBwcGKi4vToUOHvMbk5uYqMTFRLpdLLpdLiYmJOnHiRIXWSiABAMBqDh8dFVRQUKC2bdsqLS2tzOuTJ0/WlClTlJaWpk2bNik8PFw9e/bUqVOnjDFJSUlavny5lixZorVr1yo/P199+/ZVSUmJMSYhIUGZmZlKT09Xenq6MjMzlZiYWKG1Ojwej6fiH7FqO+U+a/cSgCoptMMIu5cAVDmFW8v+f9a+tOvIaZ/Mc02jWr/4tQ6HQ8uXL1d8fLykc9WRiIgIJSUlady4cZLOVUPCwsL03HPPaejQocrLy1PDhg21cOFC9e/fX5J0+PBhNWnSRCtXrtQtt9yinTt3qlWrVsrIyFBMTIwkKSMjQ7Gxsdq1a5datmxZrvVRIQEA4BLhdrt18uRJr8Ptdv+iubKyspSdna1evXoZ55xOp7p06aJ169ZJkrZs2aLi4mKvMREREWrdurUxZv369XK5XEYYkaQOHTrI5XIZY8qDQAIAgMUcDt8cKSkpxn0aPx0pKSm/aE3Z2dmSpLCwMK/zYWFhxrXs7GwFBgaqXr16Fx0TGhpaav7Q0FBjTHmwywYAAIv5apfN+PHjNWrUKK9zTqfzV83pOG9PssfjKXXufOePKWt8eeb5OSokAABYzUc3tTqdTtWtW9fr+KWBJDw8XJJKVTFycnKMqkl4eLiKioqUm5t70TFHjx4tNf+xY8dKVV8uhkACAMBvULNmzRQeHq7Vq1cb54qKirRmzRp17NhRkhQdHa2AgACvMUeOHNH27duNMbGxscrLy9PGjRuNMRs2bFBeXp4xpjxo2QAAYDG7vssmPz9fe/bsMX7OyspSZmamQkJC1LRpUyUlJSk5OVmRkZGKjIxUcnKyatWqpYSEBEmSy+XSwIEDNXr0aNWvX18hISEaM2aM2rRpox49ekiSoqKi1Lt3bw0ePFhz5syRJA0ZMkR9+/Yt9w4biUACAIDl7Hp0/ObNm9WtWzfj55/uPxkwYIDmz5+vsWPHqrCwUMOGDVNubq5iYmK0atUq1alTx3hNamqq/P391a9fPxUWFqp79+6aP3++/Pz8jDGLFi3SiBEjjN04cXFxF3z2yYXwHBLgN4TnkAClVcZzSPbkFPpknhahQT6ZpyqiQgIAgMX4LhtzBBIAAKxGIjHFLhsAAGA7KiQAAFjMrl02lxICCQAAFrNrl82lhJYNAACwHRUSAAAsRoHEHIEEAACrkUhMEUgAALAYN7Wa4x4SAABgOyokAABYjF025ggkAABYjDxijpYNAACwHRUSAAAsRsvGHIEEAADLkUjM0LIBAAC2o0ICAIDFaNmYI5AAAGAx8og5WjYAAMB2VEgAALAYLRtzBBIAACzGd9mYI5AAAGA18ogp7iEBAAC2o0ICAIDFKJCYI5AAAGAxbmo1R8sGAADYjgoJAAAWY5eNOQIJAABWI4+YomUDAABsR4UEAACLUSAxRyABAMBi7LIxR8sGAADYjgoJAAAWY5eNOQIJAAAWo2VjjpYNAACwHYEEAADYjpYNAAAWo2VjjkACAIDFuKnVHC0bAABgOyokAABYjJaNOQIJAAAWI4+Yo2UDAABsR4UEAACrUSIxRSABAMBi7LIxR8sGAADYjgoJAAAWY5eNOQIJAAAWI4+YI5AAAGA1Eokp7iEBAAC2o0ICAIDF2GVjjkACAIDFuKnVHC0bAABgO4fH4/HYvQhUT263WykpKRo/frycTqfdywGqDP42gNIIJLDMyZMn5XK5lJeXp7p169q9HKDK4G8DKI2WDQAAsB2BBAAA2I5AAgAAbEcggWWcTqeeeOIJbtoDzsPfBlAaN7UCAADbUSEBAAC2I5AAAADbEUgAAIDtCCQAAMB2BBJYZtasWWrWrJlq1qyp6Ohoff7553YvCbDVZ599pttvv10RERFyOBxasWKF3UsCqgwCCSyxdOlSJSUlacKECdq6das6deqkPn366MCBA3YvDbBNQUGB2rZtq7S0NLuXAlQ5bPuFJWJiYnTDDTdo9uzZxrmoqCjFx8crJSXFxpUBVYPD4dDy5csVHx9v91KAKoEKCXyuqKhIW7ZsUa9evbzO9+rVS+vWrbNpVQCAqoxAAp87fvy4SkpKFBYW5nU+LCxM2dnZNq0KAFCVEUhgGYfD4fWzx+MpdQ4AAIlAAgs0aNBAfn5+paohOTk5paomAABIBBJYIDAwUNHR0Vq9erXX+dWrV6tjx442rQoAUJX5270AVE+jRo1SYmKi2rdvr9jYWL388ss6cOCA7r//fruXBtgmPz9fe/bsMX7OyspSZmamQkJC1LRpUxtXBtiPbb+wzKxZszR58mQdOXJErVu3Vmpqqjp37mz3sgDbfPrpp+rWrVup8wMGDND8+fMrf0FAFUIgAQAAtuMeEgAAYDsCCQAAsB2BBAAA2I5AAgAAbEcgAQAAtiOQAAAA2xFIAACA7QgkAADAdgQSoBqaOHGirr/+euPne++9V/Hx8ZW+jn379snhcCgzM7PS3xvApYVAAlSie++9Vw6HQw6HQwEBAWrevLnGjBmjgoICS9932rRp5X40OSECgB34cj2gkvXu3Vvz5s1TcXGxPv/8cw0aNEgFBQWaPXu217ji4mIFBAT45D1dLpdP5gEAq1AhASqZ0+lUeHi4mjRpooSEBN1zzz1asWKF0WZ57bXX1Lx5czmdTnk8HuXl5WnIkCEKDQ1V3bp1dfPNN+uLL77wmnPSpEkKCwtTnTp1NHDgQJ05c8br+vktm7Nnz+q5555TixYt5HQ61bRpUz377LOSpGbNmkmS2rVrJ4fDoa5duxqvmzdvnqKiolSzZk1dc801mjVrltf7bNy4Ue3atVPNmjXVvn17bd261Ye/OQDVGRUSwGZBQUEqLi6WJO3Zs0dvvvmmli1bJj8/P0nSbbfdppCQEK1cuVIul0tz5sxR9+7dtXv3boWEhOjNN9/UE088oZkzZ6pTp05auHChpk+frubNm1/wPcePH6+5c+cqNTVVN910k44cOaJdu3ZJOhcqfv/73+vDDz/Utddeq8DAQEnS3Llz9cQTTygtLU3t2rXT1q1bNXjwYAUHB2vAgAEqKChQ3759dfPNN+uNN95QVlaWRo4cafFvD0C14QFQaQYMGOC54447jJ83bNjgqV+/vqdfv36eJ554whMQEODJyckxrn/00UeeunXres6cOeM1z1VXXeWZM2eOx+PxeGJjYz3333+/1/WYmBhP27Zty3zfkydPepxOp2fu3LllrjErK8sjybN161av802aNPEsXrzY69zTTz/tiY2N9Xg8Hs+cOXM8ISEhnoKCAuP67Nmzy5wLAM5HywaoZO+9955q166tmjVrKjY2Vp07d9aMGTMkSVdccYUaNmxojN2yZYvy8/NVv3591a5d2ziysrL07bffSpJ27typ2NhYr/c4/+ef27lzp9xut7p3717uNR87dkwHDx7UwIEDvdbxzDPPeK2jbdu2qlWrVrnWAQA/R8sGqGTdunXT7NmzFRAQoIiICK8bV4ODg73Gnj17Vo0aNdKnn35aap7LLrvsF71/UFBQhV9z9uxZSefaNjExMV7XfmoteTyeX7QeAJAIJEClCw4OVosWLco19oYbblB2drb8/f115ZVXljkmKipKGRkZ+utf/2qcy8jIuOCckZGRCgoK0kcffaRBgwaVuv7TPSMlJSXGubCwMDVu3Fh79+7VPffcU+a8rVq10sKFC1VYWGiEnoutAwB+jpYNUIX16NFDsbGxio+P1wcffKB9+/Zp3bp1evTRR7V582ZJ0siRI/Xaa6/ptdde0+7du/XEE09ox44dF5yzZs2aGjdunMaOHavXX39d3377rTIyMvTqq69KkkJDQxUUFKT09HQdPXpUeXl5ks49bC0lJUXTpk3T7t27tW3bNs2bN09TpkyRJCUkJKhGjRoaOHCgvvrqK61cuVIvvPCCxb8hANUFgQSowhwOh1auXKnOnTvrvvvu09VXX627775b+/btU1hYmCSpf//+evzxxzVu3DhFR0dr//79euCBBy4672OPPabRo0fr8ccfV1RUlPr376+cnBxJkr+/v6ZPn645c+YoIiJCd9xxhyRp0KBBeuWVVzR//ny1adNGXbp00fz5841twrVr19a//vUvffXVV2rXrp0mTJig5557zsLfDoDqxOGh8QsAAGxGhQQAANiOQAIAAGxHIAEAALYjkAAAANsRSAAAgO0IJAAAwHYEEgAAYDsCCQAAsB2BBAAA2I5AAgAAbEcgAQAAtvv/4Hg2rSKf2iYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(classification_report(y_te, y_pr))\n",
    "sns.heatmap(confusion_matrix(y_te, y_pr), cmap='Blues', cbar=True, annot=True, xticklabels= True, yticklabels=True, fmt = 'd')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e3a7a733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 45 candidates, totalling 135 fits\n",
      "[CV 1/3] END C=0.1, class_weight=balanced, max_iter=1000; f1: (test=0.916) precision: (test=0.973) recall: (test=0.866) total time=   0.4s\n",
      "[CV 2/3] END C=0.1, class_weight=balanced, max_iter=1000; f1: (test=0.901) precision: (test=0.977) recall: (test=0.837) total time=   0.9s\n",
      "[CV 3/3] END C=0.1, class_weight=balanced, max_iter=1000; f1: (test=0.921) precision: (test=0.975) recall: (test=0.873) total time=   0.6s\n",
      "[CV 1/3] END C=0.1, class_weight=balanced, max_iter=2500; f1: (test=0.916) precision: (test=0.973) recall: (test=0.866) total time=   0.4s\n",
      "[CV 2/3] END C=0.1, class_weight=balanced, max_iter=2500; f1: (test=0.901) precision: (test=0.977) recall: (test=0.837) total time=   0.5s\n",
      "[CV 3/3] END C=0.1, class_weight=balanced, max_iter=2500; f1: (test=0.921) precision: (test=0.975) recall: (test=0.873) total time=   0.5s\n",
      "[CV 1/3] END C=0.1, class_weight=balanced, max_iter=5000; f1: (test=0.916) precision: (test=0.973) recall: (test=0.866) total time=   0.7s\n",
      "[CV 2/3] END C=0.1, class_weight=balanced, max_iter=5000; f1: (test=0.901) precision: (test=0.977) recall: (test=0.837) total time=   0.7s\n",
      "[CV 3/3] END C=0.1, class_weight=balanced, max_iter=5000; f1: (test=0.921) precision: (test=0.975) recall: (test=0.873) total time=   0.8s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.0s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.964) precision: (test=0.932) recall: (test=0.998) total time=   0.0s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.964) precision: (test=0.933) recall: (test=0.998) total time=   0.0s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.0s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.964) precision: (test=0.932) recall: (test=0.998) total time=   0.0s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.964) precision: (test=0.933) recall: (test=0.998) total time=   0.1s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.1s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.964) precision: (test=0.932) recall: (test=0.998) total time=   0.1s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.964) precision: (test=0.933) recall: (test=0.998) total time=   0.0s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.955) precision: (test=0.953) recall: (test=0.956) total time=   0.4s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.955) precision: (test=0.958) recall: (test=0.952) total time=   0.5s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.956) precision: (test=0.957) recall: (test=0.956) total time=   0.5s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.955) precision: (test=0.953) recall: (test=0.956) total time=   0.4s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.955) precision: (test=0.958) recall: (test=0.952) total time=   0.6s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.956) precision: (test=0.957) recall: (test=0.956) total time=   0.4s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.955) precision: (test=0.953) recall: (test=0.956) total time=   0.3s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.955) precision: (test=0.958) recall: (test=0.952) total time=   0.4s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.956) precision: (test=0.957) recall: (test=0.956) total time=   0.5s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.1s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.964) precision: (test=0.932) recall: (test=0.998) total time=   0.1s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.964) precision: (test=0.933) recall: (test=0.998) total time=   0.1s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.0s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.964) precision: (test=0.932) recall: (test=0.998) total time=   0.1s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.964) precision: (test=0.933) recall: (test=0.998) total time=   0.2s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.965) precision: (test=0.935) recall: (test=0.997) total time=   0.1s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.964) precision: (test=0.932) recall: (test=0.998) total time=   0.1s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.964) precision: (test=0.933) recall: (test=0.998) total time=   0.1s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.939) precision: (test=0.963) recall: (test=0.917) total time=   1.0s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.927) precision: (test=0.968) recall: (test=0.889) total time=   1.1s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.940) precision: (test=0.967) recall: (test=0.915) total time=   1.2s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.939) precision: (test=0.963) recall: (test=0.917) total time=   0.8s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.927) precision: (test=0.968) recall: (test=0.889) total time=   1.3s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.940) precision: (test=0.967) recall: (test=0.915) total time=   1.1s\n",
      "[CV 1/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.939) precision: (test=0.963) recall: (test=0.917) total time=   0.7s\n",
      "[CV 2/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.927) precision: (test=0.968) recall: (test=0.889) total time=   1.1s\n",
      "[CV 3/3] END C=0.1, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.940) precision: (test=0.967) recall: (test=0.915) total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=1, class_weight=balanced, max_iter=1000; f1: (test=0.952) precision: (test=0.949) recall: (test=0.955) total time=   2.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, class_weight=balanced, max_iter=1000; f1: (test=0.952) precision: (test=0.951) recall: (test=0.953) total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=1, class_weight=balanced, max_iter=1000; f1: (test=0.951) precision: (test=0.952) recall: (test=0.950) total time=   2.4s\n",
      "[CV 1/3] END C=1, class_weight=balanced, max_iter=2500; f1: (test=0.952) precision: (test=0.949) recall: (test=0.955) total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, class_weight=balanced, max_iter=2500; f1: (test=0.952) precision: (test=0.951) recall: (test=0.953) total time=   5.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=1, class_weight=balanced, max_iter=2500; f1: (test=0.951) precision: (test=0.952) recall: (test=0.950) total time=   5.1s\n",
      "[CV 1/3] END C=1, class_weight=balanced, max_iter=5000; f1: (test=0.952) precision: (test=0.949) recall: (test=0.955) total time=   1.6s\n",
      "[CV 2/3] END C=1, class_weight=balanced, max_iter=5000; f1: (test=0.952) precision: (test=0.951) recall: (test=0.953) total time=   5.4s\n",
      "[CV 3/3] END C=1, class_weight=balanced, max_iter=5000; f1: (test=0.951) precision: (test=0.952) recall: (test=0.950) total time=   7.4s\n",
      "[CV 1/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   1.5s\n",
      "[CV 2/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.966) precision: (test=0.942) recall: (test=0.992) total time=   2.4s\n",
      "[CV 3/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   1.9s\n",
      "[CV 1/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   2.3s\n",
      "[CV 2/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.966) precision: (test=0.942) recall: (test=0.992) total time=   1.6s\n",
      "[CV 3/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   1.4s\n",
      "[CV 1/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   0.6s\n",
      "[CV 2/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.966) precision: (test=0.942) recall: (test=0.992) total time=   1.1s\n",
      "[CV 3/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.957) precision: (test=0.946) recall: (test=0.968) total time=   1.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.958) precision: (test=0.948) recall: (test=0.967) total time=   1.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.956) precision: (test=0.948) recall: (test=0.965) total time=   2.0s\n",
      "[CV 1/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.957) precision: (test=0.946) recall: (test=0.968) total time=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.958) precision: (test=0.948) recall: (test=0.968) total time=   4.4s\n",
      "[CV 3/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.956) precision: (test=0.948) recall: (test=0.965) total time=   3.7s\n",
      "[CV 1/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.957) precision: (test=0.946) recall: (test=0.968) total time=   2.4s\n",
      "[CV 2/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.958) precision: (test=0.948) recall: (test=0.968) total time=   8.1s\n",
      "[CV 3/3] END C=1, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.956) precision: (test=0.948) recall: (test=0.965) total time=   4.7s\n",
      "[CV 1/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   0.9s\n",
      "[CV 2/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.966) precision: (test=0.942) recall: (test=0.992) total time=   1.0s\n",
      "[CV 3/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   1.0s\n",
      "[CV 1/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   0.7s\n",
      "[CV 2/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.966) precision: (test=0.942) recall: (test=0.992) total time=   1.0s\n",
      "[CV 3/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   0.8s\n",
      "[CV 1/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   0.6s\n",
      "[CV 2/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.966) precision: (test=0.942) recall: (test=0.992) total time=   1.2s\n",
      "[CV 3/3] END C=1, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.965) precision: (test=0.941) recall: (test=0.991) total time=   1.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.953) precision: (test=0.948) recall: (test=0.958) total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.952) precision: (test=0.948) recall: (test=0.957) total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.951) precision: (test=0.949) recall: (test=0.953) total time=   2.6s\n",
      "[CV 1/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.953) precision: (test=0.948) recall: (test=0.958) total time=   6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.952) precision: (test=0.948) recall: (test=0.956) total time=   9.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.951) precision: (test=0.949) recall: (test=0.953) total time=   4.4s\n",
      "[CV 1/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.953) precision: (test=0.948) recall: (test=0.958) total time=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.952) precision: (test=0.948) recall: (test=0.956) total time=  11.6s\n",
      "[CV 3/3] END C=1, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.951) precision: (test=0.949) recall: (test=0.953) total time=  10.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight=balanced, max_iter=1000; f1: (test=0.949) precision: (test=0.945) recall: (test=0.952) total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight=balanced, max_iter=1000; f1: (test=0.952) precision: (test=0.947) recall: (test=0.957) total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight=balanced, max_iter=1000; f1: (test=0.952) precision: (test=0.950) recall: (test=0.955) total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight=balanced, max_iter=2500; f1: (test=0.948) precision: (test=0.946) recall: (test=0.950) total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight=balanced, max_iter=2500; f1: (test=0.951) precision: (test=0.946) recall: (test=0.957) total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight=balanced, max_iter=2500; f1: (test=0.952) precision: (test=0.949) recall: (test=0.955) total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight=balanced, max_iter=5000; f1: (test=0.948) precision: (test=0.946) recall: (test=0.950) total time=  13.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight=balanced, max_iter=5000; f1: (test=0.951) precision: (test=0.946) recall: (test=0.956) total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight=balanced, max_iter=5000; f1: (test=0.952) precision: (test=0.949) recall: (test=0.955) total time=   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.962) precision: (test=0.942) recall: (test=0.983) total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.962) precision: (test=0.944) recall: (test=0.981) total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.962) precision: (test=0.945) recall: (test=0.979) total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.962) precision: (test=0.942) recall: (test=0.983) total time=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.962) precision: (test=0.944) recall: (test=0.981) total time=   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.962) precision: (test=0.945) recall: (test=0.979) total time=   4.7s\n",
      "[CV 1/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.962) precision: (test=0.942) recall: (test=0.983) total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.962) precision: (test=0.944) recall: (test=0.981) total time=   9.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.962) precision: (test=0.945) recall: (test=0.979) total time=   9.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.950) precision: (test=0.944) recall: (test=0.956) total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.955) precision: (test=0.947) recall: (test=0.963) total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=1000; f1: (test=0.954) precision: (test=0.948) recall: (test=0.960) total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.949) precision: (test=0.944) recall: (test=0.954) total time=   8.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.955) precision: (test=0.947) recall: (test=0.963) total time=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=2500; f1: (test=0.954) precision: (test=0.948) recall: (test=0.960) total time=   8.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.949) precision: (test=0.944) recall: (test=0.954) total time=  10.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.954) precision: (test=0.947) recall: (test=0.962) total time=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 5.0, 1: 1.0}, max_iter=5000; f1: (test=0.954) precision: (test=0.948) recall: (test=0.960) total time=   7.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.962) precision: (test=0.942) recall: (test=0.983) total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.962) precision: (test=0.944) recall: (test=0.981) total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=1000; f1: (test=0.962) precision: (test=0.945) recall: (test=0.979) total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.962) precision: (test=0.942) recall: (test=0.983) total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.962) precision: (test=0.944) recall: (test=0.981) total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=2500; f1: (test=0.962) precision: (test=0.945) recall: (test=0.979) total time=   2.7s\n",
      "[CV 1/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.962) precision: (test=0.942) recall: (test=0.983) total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.962) precision: (test=0.944) recall: (test=0.981) total time=   5.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 1.0, 1: 1.0}, max_iter=5000; f1: (test=0.962) precision: (test=0.945) recall: (test=0.979) total time=   5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.947) precision: (test=0.945) recall: (test=0.949) total time=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.952) precision: (test=0.947) recall: (test=0.958) total time=   1.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=1000; f1: (test=0.951) precision: (test=0.951) recall: (test=0.952) total time=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.945) precision: (test=0.945) recall: (test=0.944) total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.951) precision: (test=0.945) recall: (test=0.956) total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=2500; f1: (test=0.951) precision: (test=0.949) recall: (test=0.952) total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.944) precision: (test=0.945) recall: (test=0.943) total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.951) precision: (test=0.945) recall: (test=0.956) total time=   6.0s\n",
      "[CV 3/3] END C=10.0, class_weight={0: 10.0, 1: 1.0}, max_iter=5000; f1: (test=0.951) precision: (test=0.949) recall: (test=0.952) total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casti\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=LinearSVC(dual=&#x27;auto&#x27;),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10.0],\n",
       "                         &#x27;class_weight&#x27;: [&#x27;balanced&#x27;, {0: 1.0, 1: 1.0},\n",
       "                                          {0: 5.0, 1: 1.0}, {0: 1.0, 1: 1.0},\n",
       "                                          {0: 10.0, 1: 1.0}],\n",
       "                         &#x27;max_iter&#x27;: [1000, 2500, 5000]},\n",
       "             refit=False, scoring=[&#x27;precision&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;], verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=LinearSVC(dual=&#x27;auto&#x27;),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10.0],\n",
       "                         &#x27;class_weight&#x27;: [&#x27;balanced&#x27;, {0: 1.0, 1: 1.0},\n",
       "                                          {0: 5.0, 1: 1.0}, {0: 1.0, 1: 1.0},\n",
       "                                          {0: 10.0, 1: 1.0}],\n",
       "                         &#x27;max_iter&#x27;: [1000, 2500, 5000]},\n",
       "             refit=False, scoring=[&#x27;precision&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;], verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(dual=&#x27;auto&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(dual=&#x27;auto&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LinearSVC(dual='auto'),\n",
       "             param_grid={'C': [0.1, 1, 10.0],\n",
       "                         'class_weight': ['balanced', {0: 1.0, 1: 1.0},\n",
       "                                          {0: 5.0, 1: 1.0}, {0: 1.0, 1: 1.0},\n",
       "                                          {0: 10.0, 1: 1.0}],\n",
       "                         'max_iter': [1000, 2500, 5000]},\n",
       "             refit=False, scoring=['precision', 'recall', 'f1'], verbose=3)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = LinearSVC(dual='auto')\n",
    "\n",
    "svc_cv = GridSearchCV(svc, {'class_weight':['balanced', {0: 1.0, 1: 1.0}, {0: 5.0, 1: 1.0}, {0: 1.0, 1: 1.0}, {0: 10.0, 1: 1.0}],\n",
    "                           'max_iter': [1000,2500,5000],\n",
    "                           'C':[0.1, 1, 10.0]},\n",
    "                           cv = 3, verbose=3, \\\n",
    "                            scoring = ['precision','recall','f1'], refit=False)\n",
    "\n",
    "svc_cv.fit(X_tr_vec, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e767173b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_precision</th>\n",
       "      <th>split1_test_precision</th>\n",
       "      <th>split2_test_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>rank_test_precision</th>\n",
       "      <th>split0_test_recall</th>\n",
       "      <th>split1_test_recall</th>\n",
       "      <th>split2_test_recall</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>rank_test_recall</th>\n",
       "      <th>split0_test_f1</th>\n",
       "      <th>split1_test_f1</th>\n",
       "      <th>split2_test_f1</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.017925</td>\n",
       "      <td>0.359134</td>\n",
       "      <td>0.016367</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.941043</td>\n",
       "      <td>0.941557</td>\n",
       "      <td>0.940507</td>\n",
       "      <td>0.941036</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>34</td>\n",
       "      <td>0.991155</td>\n",
       "      <td>0.991707</td>\n",
       "      <td>0.990601</td>\n",
       "      <td>0.991154</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>7</td>\n",
       "      <td>0.965449</td>\n",
       "      <td>0.965982</td>\n",
       "      <td>0.964904</td>\n",
       "      <td>0.965445</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.045039</td>\n",
       "      <td>0.058741</td>\n",
       "      <td>0.012817</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.941043</td>\n",
       "      <td>0.941557</td>\n",
       "      <td>0.940507</td>\n",
       "      <td>0.941036</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>34</td>\n",
       "      <td>0.991155</td>\n",
       "      <td>0.991707</td>\n",
       "      <td>0.990601</td>\n",
       "      <td>0.991154</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>7</td>\n",
       "      <td>0.965449</td>\n",
       "      <td>0.965982</td>\n",
       "      <td>0.964904</td>\n",
       "      <td>0.965445</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.906880</td>\n",
       "      <td>0.150823</td>\n",
       "      <td>0.015623</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.941043</td>\n",
       "      <td>0.941557</td>\n",
       "      <td>0.940507</td>\n",
       "      <td>0.941036</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>34</td>\n",
       "      <td>0.991155</td>\n",
       "      <td>0.991707</td>\n",
       "      <td>0.990601</td>\n",
       "      <td>0.991154</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>7</td>\n",
       "      <td>0.965449</td>\n",
       "      <td>0.965982</td>\n",
       "      <td>0.964904</td>\n",
       "      <td>0.965445</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.877124</td>\n",
       "      <td>0.215718</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.941043</td>\n",
       "      <td>0.941557</td>\n",
       "      <td>0.940507</td>\n",
       "      <td>0.941036</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>34</td>\n",
       "      <td>0.991155</td>\n",
       "      <td>0.991707</td>\n",
       "      <td>0.990601</td>\n",
       "      <td>0.991154</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>7</td>\n",
       "      <td>0.965449</td>\n",
       "      <td>0.965982</td>\n",
       "      <td>0.964904</td>\n",
       "      <td>0.965445</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.185640</td>\n",
       "      <td>0.398609</td>\n",
       "      <td>0.014978</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.941043</td>\n",
       "      <td>0.941557</td>\n",
       "      <td>0.940507</td>\n",
       "      <td>0.941036</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>34</td>\n",
       "      <td>0.991155</td>\n",
       "      <td>0.991707</td>\n",
       "      <td>0.990601</td>\n",
       "      <td>0.991154</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>7</td>\n",
       "      <td>0.965449</td>\n",
       "      <td>0.965982</td>\n",
       "      <td>0.964904</td>\n",
       "      <td>0.965445</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.876595</td>\n",
       "      <td>0.375678</td>\n",
       "      <td>0.016896</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.941043</td>\n",
       "      <td>0.941557</td>\n",
       "      <td>0.940507</td>\n",
       "      <td>0.941036</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>34</td>\n",
       "      <td>0.991155</td>\n",
       "      <td>0.991707</td>\n",
       "      <td>0.990601</td>\n",
       "      <td>0.991154</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>7</td>\n",
       "      <td>0.965449</td>\n",
       "      <td>0.965982</td>\n",
       "      <td>0.964904</td>\n",
       "      <td>0.965445</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.141493</td>\n",
       "      <td>0.014667</td>\n",
       "      <td>0.010427</td>\n",
       "      <td>0.007373</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.935211</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>0.933287</td>\n",
       "      <td>0.933619</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>40</td>\n",
       "      <td>0.997420</td>\n",
       "      <td>0.998341</td>\n",
       "      <td>0.997788</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965314</td>\n",
       "      <td>0.964222</td>\n",
       "      <td>0.964461</td>\n",
       "      <td>0.964666</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.121098</td>\n",
       "      <td>0.008291</td>\n",
       "      <td>0.015624</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.935211</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>0.933287</td>\n",
       "      <td>0.933619</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>40</td>\n",
       "      <td>0.997420</td>\n",
       "      <td>0.998341</td>\n",
       "      <td>0.997788</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965314</td>\n",
       "      <td>0.964222</td>\n",
       "      <td>0.964461</td>\n",
       "      <td>0.964666</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.174960</td>\n",
       "      <td>0.036263</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>0.014432</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.935211</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>0.933287</td>\n",
       "      <td>0.933619</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>40</td>\n",
       "      <td>0.997420</td>\n",
       "      <td>0.998341</td>\n",
       "      <td>0.997788</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965314</td>\n",
       "      <td>0.964222</td>\n",
       "      <td>0.964461</td>\n",
       "      <td>0.964666</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.167871</td>\n",
       "      <td>0.022609</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.935211</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>0.933287</td>\n",
       "      <td>0.933619</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>40</td>\n",
       "      <td>0.997420</td>\n",
       "      <td>0.998341</td>\n",
       "      <td>0.997788</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965314</td>\n",
       "      <td>0.964222</td>\n",
       "      <td>0.964461</td>\n",
       "      <td>0.964666</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.169322</td>\n",
       "      <td>0.023033</td>\n",
       "      <td>0.008848</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.935211</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>0.933287</td>\n",
       "      <td>0.933619</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>40</td>\n",
       "      <td>0.997420</td>\n",
       "      <td>0.998341</td>\n",
       "      <td>0.997788</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965314</td>\n",
       "      <td>0.964222</td>\n",
       "      <td>0.964461</td>\n",
       "      <td>0.964666</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.181996</td>\n",
       "      <td>0.018836</td>\n",
       "      <td>0.013139</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.935211</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>0.933287</td>\n",
       "      <td>0.933619</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>40</td>\n",
       "      <td>0.997420</td>\n",
       "      <td>0.998341</td>\n",
       "      <td>0.997788</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965314</td>\n",
       "      <td>0.964222</td>\n",
       "      <td>0.964461</td>\n",
       "      <td>0.964666</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.527870</td>\n",
       "      <td>0.029245</td>\n",
       "      <td>0.018417</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.943814</td>\n",
       "      <td>0.944543</td>\n",
       "      <td>0.943309</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>28</td>\n",
       "      <td>0.982863</td>\n",
       "      <td>0.981386</td>\n",
       "      <td>0.979359</td>\n",
       "      <td>0.981203</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>13</td>\n",
       "      <td>0.961774</td>\n",
       "      <td>0.962233</td>\n",
       "      <td>0.961636</td>\n",
       "      <td>0.961881</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.027452</td>\n",
       "      <td>0.577999</td>\n",
       "      <td>0.020859</td>\n",
       "      <td>0.015172</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.943814</td>\n",
       "      <td>0.944533</td>\n",
       "      <td>0.943306</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>29</td>\n",
       "      <td>0.982863</td>\n",
       "      <td>0.981386</td>\n",
       "      <td>0.979174</td>\n",
       "      <td>0.981141</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>14</td>\n",
       "      <td>0.961774</td>\n",
       "      <td>0.962233</td>\n",
       "      <td>0.961542</td>\n",
       "      <td>0.961850</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.871116</td>\n",
       "      <td>1.296488</td>\n",
       "      <td>0.014544</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.943814</td>\n",
       "      <td>0.944533</td>\n",
       "      <td>0.943306</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>29</td>\n",
       "      <td>0.982863</td>\n",
       "      <td>0.981386</td>\n",
       "      <td>0.979174</td>\n",
       "      <td>0.981141</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>14</td>\n",
       "      <td>0.961774</td>\n",
       "      <td>0.962233</td>\n",
       "      <td>0.961542</td>\n",
       "      <td>0.961850</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.169486</td>\n",
       "      <td>0.265915</td>\n",
       "      <td>0.023109</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.943647</td>\n",
       "      <td>0.944533</td>\n",
       "      <td>0.943250</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>31</td>\n",
       "      <td>0.982863</td>\n",
       "      <td>0.981386</td>\n",
       "      <td>0.979174</td>\n",
       "      <td>0.981141</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>14</td>\n",
       "      <td>0.961774</td>\n",
       "      <td>0.962147</td>\n",
       "      <td>0.961542</td>\n",
       "      <td>0.961821</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.863868</td>\n",
       "      <td>0.848621</td>\n",
       "      <td>0.015561</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.943647</td>\n",
       "      <td>0.944533</td>\n",
       "      <td>0.943250</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>31</td>\n",
       "      <td>0.982863</td>\n",
       "      <td>0.981386</td>\n",
       "      <td>0.979174</td>\n",
       "      <td>0.981141</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>14</td>\n",
       "      <td>0.961774</td>\n",
       "      <td>0.962147</td>\n",
       "      <td>0.961542</td>\n",
       "      <td>0.961821</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.586301</td>\n",
       "      <td>1.740519</td>\n",
       "      <td>0.018879</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.943647</td>\n",
       "      <td>0.944533</td>\n",
       "      <td>0.943250</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>31</td>\n",
       "      <td>0.982863</td>\n",
       "      <td>0.981386</td>\n",
       "      <td>0.979174</td>\n",
       "      <td>0.981141</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>14</td>\n",
       "      <td>0.961774</td>\n",
       "      <td>0.962147</td>\n",
       "      <td>0.961542</td>\n",
       "      <td>0.961821</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.126039</td>\n",
       "      <td>2.323870</td>\n",
       "      <td>0.017482</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.945664</td>\n",
       "      <td>0.948167</td>\n",
       "      <td>0.947711</td>\n",
       "      <td>0.947181</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>18</td>\n",
       "      <td>0.968491</td>\n",
       "      <td>0.967564</td>\n",
       "      <td>0.965352</td>\n",
       "      <td>0.967135</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>19</td>\n",
       "      <td>0.956941</td>\n",
       "      <td>0.957767</td>\n",
       "      <td>0.956450</td>\n",
       "      <td>0.957053</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.223404</td>\n",
       "      <td>1.389719</td>\n",
       "      <td>0.007323</td>\n",
       "      <td>0.005914</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.945664</td>\n",
       "      <td>0.948167</td>\n",
       "      <td>0.947711</td>\n",
       "      <td>0.947181</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>18</td>\n",
       "      <td>0.968491</td>\n",
       "      <td>0.967564</td>\n",
       "      <td>0.965352</td>\n",
       "      <td>0.967135</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>19</td>\n",
       "      <td>0.956941</td>\n",
       "      <td>0.957767</td>\n",
       "      <td>0.956450</td>\n",
       "      <td>0.957053</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.748952</td>\n",
       "      <td>0.248892</td>\n",
       "      <td>0.011757</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.945664</td>\n",
       "      <td>0.948158</td>\n",
       "      <td>0.947711</td>\n",
       "      <td>0.947178</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>20</td>\n",
       "      <td>0.968491</td>\n",
       "      <td>0.967379</td>\n",
       "      <td>0.965352</td>\n",
       "      <td>0.967074</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>21</td>\n",
       "      <td>0.956941</td>\n",
       "      <td>0.957672</td>\n",
       "      <td>0.956450</td>\n",
       "      <td>0.957021</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.551216</td>\n",
       "      <td>0.057811</td>\n",
       "      <td>0.016627</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.953493</td>\n",
       "      <td>0.958418</td>\n",
       "      <td>0.956835</td>\n",
       "      <td>0.956249</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>7</td>\n",
       "      <td>0.955777</td>\n",
       "      <td>0.951530</td>\n",
       "      <td>0.955953</td>\n",
       "      <td>0.954420</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>29</td>\n",
       "      <td>0.954633</td>\n",
       "      <td>0.954962</td>\n",
       "      <td>0.956393</td>\n",
       "      <td>0.955329</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.546649</td>\n",
       "      <td>0.073448</td>\n",
       "      <td>0.014245</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.953493</td>\n",
       "      <td>0.958418</td>\n",
       "      <td>0.956835</td>\n",
       "      <td>0.956249</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>7</td>\n",
       "      <td>0.955777</td>\n",
       "      <td>0.951530</td>\n",
       "      <td>0.955953</td>\n",
       "      <td>0.954420</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>29</td>\n",
       "      <td>0.954633</td>\n",
       "      <td>0.954962</td>\n",
       "      <td>0.956393</td>\n",
       "      <td>0.955329</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.488067</td>\n",
       "      <td>0.083658</td>\n",
       "      <td>0.005207</td>\n",
       "      <td>0.007364</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.953493</td>\n",
       "      <td>0.958418</td>\n",
       "      <td>0.956835</td>\n",
       "      <td>0.956249</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>7</td>\n",
       "      <td>0.955777</td>\n",
       "      <td>0.951530</td>\n",
       "      <td>0.955953</td>\n",
       "      <td>0.954420</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>29</td>\n",
       "      <td>0.954633</td>\n",
       "      <td>0.954962</td>\n",
       "      <td>0.956393</td>\n",
       "      <td>0.955329</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.816210</td>\n",
       "      <td>0.314993</td>\n",
       "      <td>0.041116</td>\n",
       "      <td>0.009887</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.944283</td>\n",
       "      <td>0.947235</td>\n",
       "      <td>0.948135</td>\n",
       "      <td>0.946551</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>24</td>\n",
       "      <td>0.955592</td>\n",
       "      <td>0.962772</td>\n",
       "      <td>0.960192</td>\n",
       "      <td>0.959519</td>\n",
       "      <td>0.002969</td>\n",
       "      <td>22</td>\n",
       "      <td>0.949904</td>\n",
       "      <td>0.954940</td>\n",
       "      <td>0.954125</td>\n",
       "      <td>0.952990</td>\n",
       "      <td>0.002207</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8.065831</td>\n",
       "      <td>0.617059</td>\n",
       "      <td>0.042478</td>\n",
       "      <td>0.006967</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.944394</td>\n",
       "      <td>0.946882</td>\n",
       "      <td>0.947953</td>\n",
       "      <td>0.946409</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>26</td>\n",
       "      <td>0.954487</td>\n",
       "      <td>0.962588</td>\n",
       "      <td>0.960007</td>\n",
       "      <td>0.959027</td>\n",
       "      <td>0.003379</td>\n",
       "      <td>23</td>\n",
       "      <td>0.949413</td>\n",
       "      <td>0.954670</td>\n",
       "      <td>0.953942</td>\n",
       "      <td>0.952675</td>\n",
       "      <td>0.002325</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.015996</td>\n",
       "      <td>1.835013</td>\n",
       "      <td>0.015327</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 5.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.944384</td>\n",
       "      <td>0.946834</td>\n",
       "      <td>0.947972</td>\n",
       "      <td>0.946396</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>27</td>\n",
       "      <td>0.954303</td>\n",
       "      <td>0.961666</td>\n",
       "      <td>0.960376</td>\n",
       "      <td>0.958782</td>\n",
       "      <td>0.003211</td>\n",
       "      <td>24</td>\n",
       "      <td>0.949317</td>\n",
       "      <td>0.954192</td>\n",
       "      <td>0.954133</td>\n",
       "      <td>0.952548</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7.945403</td>\n",
       "      <td>4.162896</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.003422</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.947867</td>\n",
       "      <td>0.948099</td>\n",
       "      <td>0.949321</td>\n",
       "      <td>0.948429</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>14</td>\n",
       "      <td>0.958172</td>\n",
       "      <td>0.956137</td>\n",
       "      <td>0.952820</td>\n",
       "      <td>0.955710</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>26</td>\n",
       "      <td>0.952992</td>\n",
       "      <td>0.952101</td>\n",
       "      <td>0.951067</td>\n",
       "      <td>0.952053</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.331585</td>\n",
       "      <td>0.342770</td>\n",
       "      <td>0.016970</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.947858</td>\n",
       "      <td>0.948118</td>\n",
       "      <td>0.949311</td>\n",
       "      <td>0.948429</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>13</td>\n",
       "      <td>0.957988</td>\n",
       "      <td>0.956506</td>\n",
       "      <td>0.952635</td>\n",
       "      <td>0.955710</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>25</td>\n",
       "      <td>0.952896</td>\n",
       "      <td>0.952294</td>\n",
       "      <td>0.950970</td>\n",
       "      <td>0.952053</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6.703099</td>\n",
       "      <td>1.860404</td>\n",
       "      <td>0.014036</td>\n",
       "      <td>0.012951</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.947867</td>\n",
       "      <td>0.948099</td>\n",
       "      <td>0.949146</td>\n",
       "      <td>0.948371</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>15</td>\n",
       "      <td>0.958172</td>\n",
       "      <td>0.956137</td>\n",
       "      <td>0.952820</td>\n",
       "      <td>0.955710</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>26</td>\n",
       "      <td>0.952992</td>\n",
       "      <td>0.952101</td>\n",
       "      <td>0.950979</td>\n",
       "      <td>0.952024</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.358846</td>\n",
       "      <td>1.193102</td>\n",
       "      <td>0.011092</td>\n",
       "      <td>0.006406</td>\n",
       "      <td>1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 1, 'class_weight': 'balanced', 'max_iter': 2500}</td>\n",
       "      <td>0.949432</td>\n",
       "      <td>0.951085</td>\n",
       "      <td>0.951985</td>\n",
       "      <td>0.950834</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>10</td>\n",
       "      <td>0.954855</td>\n",
       "      <td>0.953188</td>\n",
       "      <td>0.950055</td>\n",
       "      <td>0.952700</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>35</td>\n",
       "      <td>0.952136</td>\n",
       "      <td>0.952135</td>\n",
       "      <td>0.951019</td>\n",
       "      <td>0.951764</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.892004</td>\n",
       "      <td>2.403354</td>\n",
       "      <td>0.011030</td>\n",
       "      <td>0.006567</td>\n",
       "      <td>1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 1, 'class_weight': 'balanced', 'max_iter': 5000}</td>\n",
       "      <td>0.949432</td>\n",
       "      <td>0.951085</td>\n",
       "      <td>0.951985</td>\n",
       "      <td>0.950834</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>10</td>\n",
       "      <td>0.954855</td>\n",
       "      <td>0.953188</td>\n",
       "      <td>0.950055</td>\n",
       "      <td>0.952700</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>35</td>\n",
       "      <td>0.952136</td>\n",
       "      <td>0.952135</td>\n",
       "      <td>0.951019</td>\n",
       "      <td>0.951764</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.365329</td>\n",
       "      <td>0.157749</td>\n",
       "      <td>0.015892</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 1, 'class_weight': 'balanced', 'max_iter': 1000}</td>\n",
       "      <td>0.949432</td>\n",
       "      <td>0.951085</td>\n",
       "      <td>0.951976</td>\n",
       "      <td>0.950831</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>12</td>\n",
       "      <td>0.954855</td>\n",
       "      <td>0.953188</td>\n",
       "      <td>0.949871</td>\n",
       "      <td>0.952638</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>37</td>\n",
       "      <td>0.952136</td>\n",
       "      <td>0.952135</td>\n",
       "      <td>0.950923</td>\n",
       "      <td>0.951731</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.614857</td>\n",
       "      <td>0.159371</td>\n",
       "      <td>0.019639</td>\n",
       "      <td>0.004708</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': 'balanced', 'max_iter': 1000}</td>\n",
       "      <td>0.945318</td>\n",
       "      <td>0.947119</td>\n",
       "      <td>0.949762</td>\n",
       "      <td>0.947400</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>17</td>\n",
       "      <td>0.952460</td>\n",
       "      <td>0.957243</td>\n",
       "      <td>0.954663</td>\n",
       "      <td>0.954789</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>28</td>\n",
       "      <td>0.948876</td>\n",
       "      <td>0.952154</td>\n",
       "      <td>0.952206</td>\n",
       "      <td>0.951079</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4.391443</td>\n",
       "      <td>0.459902</td>\n",
       "      <td>0.026330</td>\n",
       "      <td>0.017397</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 10.0, 'class_weight': 'balanced', 'max_iter': 2500}</td>\n",
       "      <td>0.945545</td>\n",
       "      <td>0.946217</td>\n",
       "      <td>0.949258</td>\n",
       "      <td>0.947007</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>21</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.956506</td>\n",
       "      <td>0.955031</td>\n",
       "      <td>0.953929</td>\n",
       "      <td>0.002671</td>\n",
       "      <td>32</td>\n",
       "      <td>0.947891</td>\n",
       "      <td>0.951334</td>\n",
       "      <td>0.952136</td>\n",
       "      <td>0.950453</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>8.511529</td>\n",
       "      <td>3.642228</td>\n",
       "      <td>0.025706</td>\n",
       "      <td>0.007725</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': 'balanced', 'max_iter': 5000}</td>\n",
       "      <td>0.945525</td>\n",
       "      <td>0.946207</td>\n",
       "      <td>0.948554</td>\n",
       "      <td>0.946762</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>22</td>\n",
       "      <td>0.949880</td>\n",
       "      <td>0.956321</td>\n",
       "      <td>0.954847</td>\n",
       "      <td>0.953683</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>33</td>\n",
       "      <td>0.947697</td>\n",
       "      <td>0.951237</td>\n",
       "      <td>0.951690</td>\n",
       "      <td>0.950208</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.533417</td>\n",
       "      <td>0.019591</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.945117</td>\n",
       "      <td>0.946630</td>\n",
       "      <td>0.950506</td>\n",
       "      <td>0.947418</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>16</td>\n",
       "      <td>0.948775</td>\n",
       "      <td>0.957796</td>\n",
       "      <td>0.952083</td>\n",
       "      <td>0.952884</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>34</td>\n",
       "      <td>0.946943</td>\n",
       "      <td>0.952180</td>\n",
       "      <td>0.951294</td>\n",
       "      <td>0.950139</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3.430758</td>\n",
       "      <td>0.284270</td>\n",
       "      <td>0.008533</td>\n",
       "      <td>0.006460</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.945039</td>\n",
       "      <td>0.945325</td>\n",
       "      <td>0.949118</td>\n",
       "      <td>0.946494</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>25</td>\n",
       "      <td>0.944168</td>\n",
       "      <td>0.955953</td>\n",
       "      <td>0.952267</td>\n",
       "      <td>0.950796</td>\n",
       "      <td>0.004922</td>\n",
       "      <td>38</td>\n",
       "      <td>0.944603</td>\n",
       "      <td>0.950609</td>\n",
       "      <td>0.950690</td>\n",
       "      <td>0.948634</td>\n",
       "      <td>0.002851</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6.086598</td>\n",
       "      <td>0.291956</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.945142</td>\n",
       "      <td>0.945478</td>\n",
       "      <td>0.949449</td>\n",
       "      <td>0.946690</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>23</td>\n",
       "      <td>0.942878</td>\n",
       "      <td>0.955584</td>\n",
       "      <td>0.951898</td>\n",
       "      <td>0.950120</td>\n",
       "      <td>0.005337</td>\n",
       "      <td>39</td>\n",
       "      <td>0.944009</td>\n",
       "      <td>0.950504</td>\n",
       "      <td>0.950672</td>\n",
       "      <td>0.948395</td>\n",
       "      <td>0.003102</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.161251</td>\n",
       "      <td>0.177501</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.008049</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2500}</td>\n",
       "      <td>0.962841</td>\n",
       "      <td>0.967904</td>\n",
       "      <td>0.966511</td>\n",
       "      <td>0.965752</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>4</td>\n",
       "      <td>0.916713</td>\n",
       "      <td>0.889237</td>\n",
       "      <td>0.914854</td>\n",
       "      <td>0.906935</td>\n",
       "      <td>0.012537</td>\n",
       "      <td>40</td>\n",
       "      <td>0.939211</td>\n",
       "      <td>0.926904</td>\n",
       "      <td>0.939973</td>\n",
       "      <td>0.935363</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.193446</td>\n",
       "      <td>0.089689</td>\n",
       "      <td>0.012278</td>\n",
       "      <td>0.004347</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "      <td>0.962841</td>\n",
       "      <td>0.967904</td>\n",
       "      <td>0.966511</td>\n",
       "      <td>0.965752</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>4</td>\n",
       "      <td>0.916713</td>\n",
       "      <td>0.889237</td>\n",
       "      <td>0.914854</td>\n",
       "      <td>0.906935</td>\n",
       "      <td>0.012537</td>\n",
       "      <td>40</td>\n",
       "      <td>0.939211</td>\n",
       "      <td>0.926904</td>\n",
       "      <td>0.939973</td>\n",
       "      <td>0.935363</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.076494</td>\n",
       "      <td>0.194046</td>\n",
       "      <td>0.017514</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{0: 10.0, 1: 1.0}</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 5000}</td>\n",
       "      <td>0.962841</td>\n",
       "      <td>0.967904</td>\n",
       "      <td>0.966511</td>\n",
       "      <td>0.965752</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>4</td>\n",
       "      <td>0.916713</td>\n",
       "      <td>0.889237</td>\n",
       "      <td>0.914854</td>\n",
       "      <td>0.906935</td>\n",
       "      <td>0.012537</td>\n",
       "      <td>40</td>\n",
       "      <td>0.939211</td>\n",
       "      <td>0.926904</td>\n",
       "      <td>0.939973</td>\n",
       "      <td>0.935363</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.806781</td>\n",
       "      <td>0.053997</td>\n",
       "      <td>0.015712</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': 'balanced', 'max_iter': 5000}</td>\n",
       "      <td>0.973281</td>\n",
       "      <td>0.976564</td>\n",
       "      <td>0.974882</td>\n",
       "      <td>0.974909</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>1</td>\n",
       "      <td>0.865856</td>\n",
       "      <td>0.837081</td>\n",
       "      <td>0.872650</td>\n",
       "      <td>0.858529</td>\n",
       "      <td>0.015418</td>\n",
       "      <td>43</td>\n",
       "      <td>0.916431</td>\n",
       "      <td>0.901459</td>\n",
       "      <td>0.920937</td>\n",
       "      <td>0.912942</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.570927</td>\n",
       "      <td>0.075147</td>\n",
       "      <td>0.005207</td>\n",
       "      <td>0.007364</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'C': 0.1, 'class_weight': 'balanced', 'max_iter': 2500}</td>\n",
       "      <td>0.973281</td>\n",
       "      <td>0.976564</td>\n",
       "      <td>0.974882</td>\n",
       "      <td>0.974909</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>1</td>\n",
       "      <td>0.865856</td>\n",
       "      <td>0.837081</td>\n",
       "      <td>0.872650</td>\n",
       "      <td>0.858529</td>\n",
       "      <td>0.015418</td>\n",
       "      <td>43</td>\n",
       "      <td>0.916431</td>\n",
       "      <td>0.901459</td>\n",
       "      <td>0.920937</td>\n",
       "      <td>0.912942</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.720846</td>\n",
       "      <td>0.211269</td>\n",
       "      <td>0.020071</td>\n",
       "      <td>0.006318</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 0.1, 'class_weight': 'balanced', 'max_iter': 1000}</td>\n",
       "      <td>0.973281</td>\n",
       "      <td>0.976564</td>\n",
       "      <td>0.974882</td>\n",
       "      <td>0.974909</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>1</td>\n",
       "      <td>0.865856</td>\n",
       "      <td>0.837081</td>\n",
       "      <td>0.872650</td>\n",
       "      <td>0.858529</td>\n",
       "      <td>0.015418</td>\n",
       "      <td>43</td>\n",
       "      <td>0.916431</td>\n",
       "      <td>0.901459</td>\n",
       "      <td>0.920937</td>\n",
       "      <td>0.912942</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0        2.017925      0.359134         0.016367        0.007963       1   \n",
       "1        1.045039      0.058741         0.012817        0.001324       1   \n",
       "2        0.906880      0.150823         0.015623        0.000002       1   \n",
       "3        0.877124      0.215718         0.002019        0.002855       1   \n",
       "4        1.185640      0.398609         0.014978        0.000741       1   \n",
       "5        1.876595      0.375678         0.016896        0.004938       1   \n",
       "6        0.141493      0.014667         0.010427        0.007373     0.1   \n",
       "7        0.121098      0.008291         0.015624        0.000002     0.1   \n",
       "8        0.174960      0.036263         0.013825        0.014432     0.1   \n",
       "9        0.167871      0.022609         0.015038        0.003149     0.1   \n",
       "10       0.169322      0.023033         0.008848        0.006801     0.1   \n",
       "11       0.181996      0.018836         0.013139        0.002583     0.1   \n",
       "12       2.527870      0.029245         0.018417        0.003954    10.0   \n",
       "13       2.027452      0.577999         0.020859        0.015172    10.0   \n",
       "14       3.871116      1.296488         0.014544        0.002272    10.0   \n",
       "15       5.169486      0.265915         0.023109        0.003995    10.0   \n",
       "16       4.863868      0.848621         0.015561        0.001125    10.0   \n",
       "17       8.586301      1.740519         0.018879        0.002194    10.0   \n",
       "18       5.126039      2.323870         0.017482        0.002572       1   \n",
       "19       3.223404      1.389719         0.007323        0.005914       1   \n",
       "20       1.748952      0.248892         0.011757        0.000369       1   \n",
       "21       0.551216      0.057811         0.016627        0.000844     0.1   \n",
       "22       0.546649      0.073448         0.014245        0.001922     0.1   \n",
       "23       0.488067      0.083658         0.005207        0.007364     0.1   \n",
       "24       2.816210      0.314993         0.041116        0.009887    10.0   \n",
       "25       8.065831      0.617059         0.042478        0.006967    10.0   \n",
       "26       8.015996      1.835013         0.015327        0.002699    10.0   \n",
       "27       7.945403      4.162896         0.020408        0.003422       1   \n",
       "28       2.331585      0.342770         0.016970        0.001928       1   \n",
       "29       6.703099      1.860404         0.014036        0.012951       1   \n",
       "30       4.358846      1.193102         0.011092        0.006406       1   \n",
       "31       4.892004      2.403354         0.011030        0.006567       1   \n",
       "32       2.365329      0.157749         0.015892        0.000383       1   \n",
       "33       2.614857      0.159371         0.019639        0.004708    10.0   \n",
       "34       4.391443      0.459902         0.026330        0.017397    10.0   \n",
       "35       8.511529      3.642228         0.025706        0.007725    10.0   \n",
       "36       1.533417      0.019591         0.008929        0.003383    10.0   \n",
       "37       3.430758      0.284270         0.008533        0.006460    10.0   \n",
       "38       6.086598      0.291956         0.007215        0.005102    10.0   \n",
       "39       1.161251      0.177501         0.011355        0.008049     0.1   \n",
       "40       1.193446      0.089689         0.012278        0.004347     0.1   \n",
       "41       1.076494      0.194046         0.017514        0.005738     0.1   \n",
       "42       0.806781      0.053997         0.015712        0.000151     0.1   \n",
       "43       0.570927      0.075147         0.005207        0.007364     0.1   \n",
       "44       0.720846      0.211269         0.020071        0.006318     0.1   \n",
       "\n",
       "   param_class_weight param_max_iter  \\\n",
       "0    {0: 1.0, 1: 1.0}           1000   \n",
       "1    {0: 1.0, 1: 1.0}           1000   \n",
       "2    {0: 1.0, 1: 1.0}           2500   \n",
       "3    {0: 1.0, 1: 1.0}           5000   \n",
       "4    {0: 1.0, 1: 1.0}           5000   \n",
       "5    {0: 1.0, 1: 1.0}           2500   \n",
       "6    {0: 1.0, 1: 1.0}           2500   \n",
       "7    {0: 1.0, 1: 1.0}           1000   \n",
       "8    {0: 1.0, 1: 1.0}           2500   \n",
       "9    {0: 1.0, 1: 1.0}           5000   \n",
       "10   {0: 1.0, 1: 1.0}           5000   \n",
       "11   {0: 1.0, 1: 1.0}           1000   \n",
       "12   {0: 1.0, 1: 1.0}           1000   \n",
       "13   {0: 1.0, 1: 1.0}           1000   \n",
       "14   {0: 1.0, 1: 1.0}           2500   \n",
       "15   {0: 1.0, 1: 1.0}           2500   \n",
       "16   {0: 1.0, 1: 1.0}           5000   \n",
       "17   {0: 1.0, 1: 1.0}           5000   \n",
       "18   {0: 5.0, 1: 1.0}           5000   \n",
       "19   {0: 5.0, 1: 1.0}           2500   \n",
       "20   {0: 5.0, 1: 1.0}           1000   \n",
       "21   {0: 5.0, 1: 1.0}           1000   \n",
       "22   {0: 5.0, 1: 1.0}           2500   \n",
       "23   {0: 5.0, 1: 1.0}           5000   \n",
       "24   {0: 5.0, 1: 1.0}           1000   \n",
       "25   {0: 5.0, 1: 1.0}           2500   \n",
       "26   {0: 5.0, 1: 1.0}           5000   \n",
       "27  {0: 10.0, 1: 1.0}           5000   \n",
       "28  {0: 10.0, 1: 1.0}           1000   \n",
       "29  {0: 10.0, 1: 1.0}           2500   \n",
       "30           balanced           2500   \n",
       "31           balanced           5000   \n",
       "32           balanced           1000   \n",
       "33           balanced           1000   \n",
       "34           balanced           2500   \n",
       "35           balanced           5000   \n",
       "36  {0: 10.0, 1: 1.0}           1000   \n",
       "37  {0: 10.0, 1: 1.0}           2500   \n",
       "38  {0: 10.0, 1: 1.0}           5000   \n",
       "39  {0: 10.0, 1: 1.0}           2500   \n",
       "40  {0: 10.0, 1: 1.0}           1000   \n",
       "41  {0: 10.0, 1: 1.0}           5000   \n",
       "42           balanced           5000   \n",
       "43           balanced           2500   \n",
       "44           balanced           1000   \n",
       "\n",
       "                                                              params  \\\n",
       "0       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "1       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "2       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "3       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "4       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "5       {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "6     {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "7     {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "8     {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "9     {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "10    {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "11    {'C': 0.1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "12   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "13   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "14   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "15   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "16   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "17   {'C': 10.0, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "18      {'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "19      {'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "20      {'C': 1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "21    {'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "22    {'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "23    {'C': 0.1, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "24   {'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "25   {'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "26   {'C': 10.0, 'class_weight': {0: 5.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "27     {'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "28     {'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "29     {'C': 1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "30            {'C': 1, 'class_weight': 'balanced', 'max_iter': 2500}   \n",
       "31            {'C': 1, 'class_weight': 'balanced', 'max_iter': 5000}   \n",
       "32            {'C': 1, 'class_weight': 'balanced', 'max_iter': 1000}   \n",
       "33         {'C': 10.0, 'class_weight': 'balanced', 'max_iter': 1000}   \n",
       "34         {'C': 10.0, 'class_weight': 'balanced', 'max_iter': 2500}   \n",
       "35         {'C': 10.0, 'class_weight': 'balanced', 'max_iter': 5000}   \n",
       "36  {'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "37  {'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "38  {'C': 10.0, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "39   {'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 2500}   \n",
       "40   {'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 1000}   \n",
       "41   {'C': 0.1, 'class_weight': {0: 10.0, 1: 1.0}, 'max_iter': 5000}   \n",
       "42          {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 5000}   \n",
       "43          {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 2500}   \n",
       "44          {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 1000}   \n",
       "\n",
       "    split0_test_precision  split1_test_precision  split2_test_precision  \\\n",
       "0                0.941043               0.941557               0.940507   \n",
       "1                0.941043               0.941557               0.940507   \n",
       "2                0.941043               0.941557               0.940507   \n",
       "3                0.941043               0.941557               0.940507   \n",
       "4                0.941043               0.941557               0.940507   \n",
       "5                0.941043               0.941557               0.940507   \n",
       "6                0.935211               0.932358               0.933287   \n",
       "7                0.935211               0.932358               0.933287   \n",
       "8                0.935211               0.932358               0.933287   \n",
       "9                0.935211               0.932358               0.933287   \n",
       "10               0.935211               0.932358               0.933287   \n",
       "11               0.935211               0.932358               0.933287   \n",
       "12               0.941571               0.943814               0.944543   \n",
       "13               0.941571               0.943814               0.944533   \n",
       "14               0.941571               0.943814               0.944533   \n",
       "15               0.941571               0.943647               0.944533   \n",
       "16               0.941571               0.943647               0.944533   \n",
       "17               0.941571               0.943647               0.944533   \n",
       "18               0.945664               0.948167               0.947711   \n",
       "19               0.945664               0.948167               0.947711   \n",
       "20               0.945664               0.948158               0.947711   \n",
       "21               0.953493               0.958418               0.956835   \n",
       "22               0.953493               0.958418               0.956835   \n",
       "23               0.953493               0.958418               0.956835   \n",
       "24               0.944283               0.947235               0.948135   \n",
       "25               0.944394               0.946882               0.947953   \n",
       "26               0.944384               0.946834               0.947972   \n",
       "27               0.947867               0.948099               0.949321   \n",
       "28               0.947858               0.948118               0.949311   \n",
       "29               0.947867               0.948099               0.949146   \n",
       "30               0.949432               0.951085               0.951985   \n",
       "31               0.949432               0.951085               0.951985   \n",
       "32               0.949432               0.951085               0.951976   \n",
       "33               0.945318               0.947119               0.949762   \n",
       "34               0.945545               0.946217               0.949258   \n",
       "35               0.945525               0.946207               0.948554   \n",
       "36               0.945117               0.946630               0.950506   \n",
       "37               0.945039               0.945325               0.949118   \n",
       "38               0.945142               0.945478               0.949449   \n",
       "39               0.962841               0.967904               0.966511   \n",
       "40               0.962841               0.967904               0.966511   \n",
       "41               0.962841               0.967904               0.966511   \n",
       "42               0.973281               0.976564               0.974882   \n",
       "43               0.973281               0.976564               0.974882   \n",
       "44               0.973281               0.976564               0.974882   \n",
       "\n",
       "    mean_test_precision  std_test_precision  rank_test_precision  \\\n",
       "0              0.941036            0.000429                   34   \n",
       "1              0.941036            0.000429                   34   \n",
       "2              0.941036            0.000429                   34   \n",
       "3              0.941036            0.000429                   34   \n",
       "4              0.941036            0.000429                   34   \n",
       "5              0.941036            0.000429                   34   \n",
       "6              0.933619            0.001188                   40   \n",
       "7              0.933619            0.001188                   40   \n",
       "8              0.933619            0.001188                   40   \n",
       "9              0.933619            0.001188                   40   \n",
       "10             0.933619            0.001188                   40   \n",
       "11             0.933619            0.001188                   40   \n",
       "12             0.943309            0.001265                   28   \n",
       "13             0.943306            0.001262                   29   \n",
       "14             0.943306            0.001262                   29   \n",
       "15             0.943250            0.001241                   31   \n",
       "16             0.943250            0.001241                   31   \n",
       "17             0.943250            0.001241                   31   \n",
       "18             0.947181            0.001089                   18   \n",
       "19             0.947181            0.001089                   18   \n",
       "20             0.947178            0.001086                   20   \n",
       "21             0.956249            0.002053                    7   \n",
       "22             0.956249            0.002053                    7   \n",
       "23             0.956249            0.002053                    7   \n",
       "24             0.946551            0.001645                   24   \n",
       "25             0.946409            0.001491                   26   \n",
       "26             0.946396            0.001497                   27   \n",
       "27             0.948429            0.000637                   14   \n",
       "28             0.948429            0.000633                   13   \n",
       "29             0.948371            0.000556                   15   \n",
       "30             0.950834            0.001057                   10   \n",
       "31             0.950834            0.001057                   10   \n",
       "32             0.950831            0.001054                   12   \n",
       "33             0.947400            0.001825                   17   \n",
       "34             0.947007            0.001616                   21   \n",
       "35             0.946762            0.001297                   22   \n",
       "36             0.947418            0.002269                   16   \n",
       "37             0.946494            0.001859                   25   \n",
       "38             0.946690            0.001956                   23   \n",
       "39             0.965752            0.002135                    4   \n",
       "40             0.965752            0.002135                    4   \n",
       "41             0.965752            0.002135                    4   \n",
       "42             0.974909            0.001341                    1   \n",
       "43             0.974909            0.001341                    1   \n",
       "44             0.974909            0.001341                    1   \n",
       "\n",
       "    split0_test_recall  split1_test_recall  split2_test_recall  \\\n",
       "0             0.991155            0.991707            0.990601   \n",
       "1             0.991155            0.991707            0.990601   \n",
       "2             0.991155            0.991707            0.990601   \n",
       "3             0.991155            0.991707            0.990601   \n",
       "4             0.991155            0.991707            0.990601   \n",
       "5             0.991155            0.991707            0.990601   \n",
       "6             0.997420            0.998341            0.997788   \n",
       "7             0.997420            0.998341            0.997788   \n",
       "8             0.997420            0.998341            0.997788   \n",
       "9             0.997420            0.998341            0.997788   \n",
       "10            0.997420            0.998341            0.997788   \n",
       "11            0.997420            0.998341            0.997788   \n",
       "12            0.982863            0.981386            0.979359   \n",
       "13            0.982863            0.981386            0.979174   \n",
       "14            0.982863            0.981386            0.979174   \n",
       "15            0.982863            0.981386            0.979174   \n",
       "16            0.982863            0.981386            0.979174   \n",
       "17            0.982863            0.981386            0.979174   \n",
       "18            0.968491            0.967564            0.965352   \n",
       "19            0.968491            0.967564            0.965352   \n",
       "20            0.968491            0.967379            0.965352   \n",
       "21            0.955777            0.951530            0.955953   \n",
       "22            0.955777            0.951530            0.955953   \n",
       "23            0.955777            0.951530            0.955953   \n",
       "24            0.955592            0.962772            0.960192   \n",
       "25            0.954487            0.962588            0.960007   \n",
       "26            0.954303            0.961666            0.960376   \n",
       "27            0.958172            0.956137            0.952820   \n",
       "28            0.957988            0.956506            0.952635   \n",
       "29            0.958172            0.956137            0.952820   \n",
       "30            0.954855            0.953188            0.950055   \n",
       "31            0.954855            0.953188            0.950055   \n",
       "32            0.954855            0.953188            0.949871   \n",
       "33            0.952460            0.957243            0.954663   \n",
       "34            0.950249            0.956506            0.955031   \n",
       "35            0.949880            0.956321            0.954847   \n",
       "36            0.948775            0.957796            0.952083   \n",
       "37            0.944168            0.955953            0.952267   \n",
       "38            0.942878            0.955584            0.951898   \n",
       "39            0.916713            0.889237            0.914854   \n",
       "40            0.916713            0.889237            0.914854   \n",
       "41            0.916713            0.889237            0.914854   \n",
       "42            0.865856            0.837081            0.872650   \n",
       "43            0.865856            0.837081            0.872650   \n",
       "44            0.865856            0.837081            0.872650   \n",
       "\n",
       "    mean_test_recall  std_test_recall  rank_test_recall  split0_test_f1  \\\n",
       "0           0.991154         0.000451                 7        0.965449   \n",
       "1           0.991154         0.000451                 7        0.965449   \n",
       "2           0.991154         0.000451                 7        0.965449   \n",
       "3           0.991154         0.000451                 7        0.965449   \n",
       "4           0.991154         0.000451                 7        0.965449   \n",
       "5           0.991154         0.000451                 7        0.965449   \n",
       "6           0.997850         0.000379                 1        0.965314   \n",
       "7           0.997850         0.000379                 1        0.965314   \n",
       "8           0.997850         0.000379                 1        0.965314   \n",
       "9           0.997850         0.000379                 1        0.965314   \n",
       "10          0.997850         0.000379                 1        0.965314   \n",
       "11          0.997850         0.000379                 1        0.965314   \n",
       "12          0.981203         0.001437                13        0.961774   \n",
       "13          0.981141         0.001516                14        0.961774   \n",
       "14          0.981141         0.001516                14        0.961774   \n",
       "15          0.981141         0.001516                14        0.961774   \n",
       "16          0.981141         0.001516                14        0.961774   \n",
       "17          0.981141         0.001516                14        0.961774   \n",
       "18          0.967135         0.001317                19        0.956941   \n",
       "19          0.967135         0.001317                19        0.956941   \n",
       "20          0.967074         0.001299                21        0.956941   \n",
       "21          0.954420         0.002045                29        0.954633   \n",
       "22          0.954420         0.002045                29        0.954633   \n",
       "23          0.954420         0.002045                29        0.954633   \n",
       "24          0.959519         0.002969                22        0.949904   \n",
       "25          0.959027         0.003379                23        0.949413   \n",
       "26          0.958782         0.003211                24        0.949317   \n",
       "27          0.955710         0.002206                26        0.952992   \n",
       "28          0.955710         0.002256                25        0.952896   \n",
       "29          0.955710         0.002206                26        0.952992   \n",
       "30          0.952700         0.001990                35        0.952136   \n",
       "31          0.952700         0.001990                35        0.952136   \n",
       "32          0.952638         0.002072                37        0.952136   \n",
       "33          0.954789         0.001955                28        0.948876   \n",
       "34          0.953929         0.002671                32        0.947891   \n",
       "35          0.953683         0.002755                33        0.947697   \n",
       "36          0.952884         0.003726                34        0.946943   \n",
       "37          0.950796         0.004922                38        0.944603   \n",
       "38          0.950120         0.005337                39        0.944009   \n",
       "39          0.906935         0.012537                40        0.939211   \n",
       "40          0.906935         0.012537                40        0.939211   \n",
       "41          0.906935         0.012537                40        0.939211   \n",
       "42          0.858529         0.015418                43        0.916431   \n",
       "43          0.858529         0.015418                43        0.916431   \n",
       "44          0.858529         0.015418                43        0.916431   \n",
       "\n",
       "    split1_test_f1  split2_test_f1  mean_test_f1  std_test_f1  rank_test_f1  \n",
       "0         0.965982        0.964904      0.965445     0.000440             1  \n",
       "1         0.965982        0.964904      0.965445     0.000440             1  \n",
       "2         0.965982        0.964904      0.965445     0.000440             1  \n",
       "3         0.965982        0.964904      0.965445     0.000440             1  \n",
       "4         0.965982        0.964904      0.965445     0.000440             1  \n",
       "5         0.965982        0.964904      0.965445     0.000440             1  \n",
       "6         0.964222        0.964461      0.964666     0.000469             7  \n",
       "7         0.964222        0.964461      0.964666     0.000469             7  \n",
       "8         0.964222        0.964461      0.964666     0.000469             7  \n",
       "9         0.964222        0.964461      0.964666     0.000469             7  \n",
       "10        0.964222        0.964461      0.964666     0.000469             7  \n",
       "11        0.964222        0.964461      0.964666     0.000469             7  \n",
       "12        0.962233        0.961636      0.961881     0.000255            13  \n",
       "13        0.962233        0.961542      0.961850     0.000287            14  \n",
       "14        0.962233        0.961542      0.961850     0.000287            14  \n",
       "15        0.962147        0.961542      0.961821     0.000249            16  \n",
       "16        0.962147        0.961542      0.961821     0.000249            16  \n",
       "17        0.962147        0.961542      0.961821     0.000249            16  \n",
       "18        0.957767        0.956450      0.957053     0.000543            19  \n",
       "19        0.957767        0.956450      0.957053     0.000543            19  \n",
       "20        0.957672        0.956450      0.957021     0.000502            21  \n",
       "21        0.954962        0.956393      0.955329     0.000764            22  \n",
       "22        0.954962        0.956393      0.955329     0.000764            22  \n",
       "23        0.954962        0.956393      0.955329     0.000764            22  \n",
       "24        0.954940        0.954125      0.952990     0.002207            25  \n",
       "25        0.954670        0.953942      0.952675     0.002325            26  \n",
       "26        0.954192        0.954133      0.952548     0.002284            27  \n",
       "27        0.952101        0.951067      0.952053     0.000787            28  \n",
       "28        0.952294        0.950970      0.952053     0.000804            29  \n",
       "29        0.952101        0.950979      0.952024     0.000823            30  \n",
       "30        0.952135        0.951019      0.951764     0.000526            31  \n",
       "31        0.952135        0.951019      0.951764     0.000526            31  \n",
       "32        0.952135        0.950923      0.951731     0.000572            33  \n",
       "33        0.952154        0.952206      0.951079     0.001558            34  \n",
       "34        0.951334        0.952136      0.950453     0.001841            35  \n",
       "35        0.951237        0.951690      0.950208     0.001785            36  \n",
       "36        0.952180        0.951294      0.950139     0.002289            37  \n",
       "37        0.950609        0.950690      0.948634     0.002851            38  \n",
       "38        0.950504        0.950672      0.948395     0.003102            39  \n",
       "39        0.926904        0.939973      0.935363     0.005989            40  \n",
       "40        0.926904        0.939973      0.935363     0.005989            40  \n",
       "41        0.926904        0.939973      0.935363     0.005989            40  \n",
       "42        0.901459        0.920937      0.912942     0.008326            43  \n",
       "43        0.901459        0.920937      0.912942     0.008326            43  \n",
       "44        0.901459        0.920937      0.912942     0.008326            43  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>2.017925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.359134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.016367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.007963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_C</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_class_weight</th>\n",
       "      <td>{0: 1.0, 1: 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_max_iter</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.941043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.941557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.940507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.941036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.000429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.991155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.991707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.990601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.991154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.000451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.965449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.965982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.964904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.965445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.00044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  0\n",
       "mean_fit_time                                                              2.017925\n",
       "std_fit_time                                                               0.359134\n",
       "mean_score_time                                                            0.016367\n",
       "std_score_time                                                             0.007963\n",
       "param_C                                                                           1\n",
       "param_class_weight                                                 {0: 1.0, 1: 1.0}\n",
       "param_max_iter                                                                 1000\n",
       "params                 {'C': 1, 'class_weight': {0: 1.0, 1: 1.0}, 'max_iter': 1000}\n",
       "split0_test_precision                                                      0.941043\n",
       "split1_test_precision                                                      0.941557\n",
       "split2_test_precision                                                      0.940507\n",
       "mean_test_precision                                                        0.941036\n",
       "std_test_precision                                                         0.000429\n",
       "rank_test_precision                                                              34\n",
       "split0_test_recall                                                         0.991155\n",
       "split1_test_recall                                                         0.991707\n",
       "split2_test_recall                                                         0.990601\n",
       "mean_test_recall                                                           0.991154\n",
       "std_test_recall                                                            0.000451\n",
       "rank_test_recall                                                                  7\n",
       "split0_test_f1                                                             0.965449\n",
       "split1_test_f1                                                             0.965982\n",
       "split2_test_f1                                                             0.964904\n",
       "mean_test_f1                                                               0.965445\n",
       "std_test_f1                                                                 0.00044\n",
       "rank_test_f1                                                                      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svc_cvdf = pd.DataFrame(svc_cv.cv_results_).sort_values(by='rank_test_f1').reset_index(drop=True)\n",
    "svc_best = pd.DataFrame(svc_cvdf.iloc[0,:])\n",
    "display(svc_cvdf)\n",
    "display(svc_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d13b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
